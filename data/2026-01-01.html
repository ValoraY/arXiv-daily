<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2026-01-01.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 40]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 6]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 4]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-scaling-remote-sensing-foundation-models-data-domain-tradeoffs-at-the-peta-scale">[1] <a href="https://arxiv.org/abs/2512.23903">Scaling Remote Sensing Foundation Models: Data Domain Tradeoffs at the Peta-Scale</a></h3>
<p><em>Charith Wickrema, Eliza Mace, Hunter Brown, Heidys Cabrera, Nick Krall, Matthew O'Neill, Shivangi Sarkar, Lowell Weissman, Eric Hughes, Guido Zarrella</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æ¢ç´¢äº†é«˜åˆ†è¾¨ç‡å…‰ç”µé¥æ„Ÿæ•°æ®ä¸ŠåŸºç¡€æ¨¡å‹çš„ç¼©æ”¾è§„å¾‹ï¼Œé€šè¿‡è®­ç»ƒè§„æ¨¡è¿œè¶…ç°æœ‰æ°´å¹³çš„è§†è§‰Transformeréª¨å¹²ç½‘ç»œï¼Œæ­ç¤ºäº†åœ¨é¥æ„Ÿé¢†åŸŸå³ä½¿è¾¾åˆ°åƒä¸‡äº¿åƒç´ æ•°æ®è§„æ¨¡ï¼Œæ€§èƒ½ä»å—æ•°æ®è€Œéæ¨¡å‹å‚æ•°é™åˆ¶çš„è§„å¾‹ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€æœºå™¨å­¦ä¹ åº”ç”¨ä¾èµ–äºé’ˆå¯¹éæ–‡æœ¬æ¨¡æ€çš„é²æ£’ã€é¢†åŸŸä¸“ç”¨ç¼–ç å™¨ï¼Œåœ¨è‡ªç„¶å›¾åƒé¢†åŸŸå·²æœ‰æˆç†Ÿçš„ç¼©æ”¾è§„å¾‹æŒ‡å¯¼æ¨¡å‹å®¹é‡ã€è®­ç»ƒè®¡ç®—å’Œæ•°æ®é›†è§„æ¨¡çš„è”åˆä¼˜åŒ–ï¼Œä½†åœ¨é«˜ä»·å€¼é¥æ„Ÿé¢†åŸŸè¿™äº›å…³ç³»å°šæœªè¢«å……åˆ†ç†è§£ï¼Œé™åˆ¶äº†å‰æ²¿è§„æ¨¡é¥æ„ŸåŸºç¡€æ¨¡å‹çš„å¼€å‘ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶ä½¿ç”¨è¶…è¿‡åƒä¸‡äº¿åƒç´ çš„å•†ä¸šå«æ˜Ÿå…‰ç”µæ•°æ®ï¼Œåœ¨MITREè”é‚¦AIæ²™ç®±ä¸­è®­ç»ƒäº†æ¸è¿›å¢å¤§çš„è§†è§‰Transformeréª¨å¹²ç½‘ç»œï¼Œåˆ†æäº†åœ¨åƒä¸‡äº¿è§„æ¨¡è®­ç»ƒä¸­è§‚å¯Ÿåˆ°çš„æˆåŠŸå’Œå¤±è´¥æ¨¡å¼ï¼Œå¹¶ç ”ç©¶äº†è·¨é¢å¤–é¥æ„Ÿæ¨¡æ€çš„é¢†åŸŸå·®è·æ¡¥æ¥é—®é¢˜ã€‚</p>
<p><strong>Result:</strong> å®éªŒå‘ç°å³ä½¿è¾¾åˆ°åƒä¸‡äº¿åƒç´ æ•°æ®è§„æ¨¡ï¼Œæ€§èƒ½ä»ä¸æ•°æ®å—é™è€Œéæ¨¡å‹å‚æ•°å—é™çš„æœºåˆ¶ä¸€è‡´ï¼Œè¿™è¡¨æ˜é¥æ„Ÿé¢†åŸŸçš„ç¼©æ”¾è§„å¾‹ä¸è‡ªç„¶å›¾åƒé¢†åŸŸå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œéœ€è¦ä¸åŒçš„ä¼˜åŒ–ç­–ç•¥ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æä¾›äº†å…³äºæ•°æ®æ”¶é›†ç­–ç•¥ã€è®¡ç®—é¢„ç®—å’Œä¼˜åŒ–è°ƒåº¦çš„å®ç”¨è§è§£ï¼Œæ—¨åœ¨æŒ‡å¯¼å‰æ²¿è§„æ¨¡é¥æ„ŸåŸºç¡€æ¨¡å‹çš„æœªæ¥å‘å±•ï¼Œå¼ºè°ƒäº†é¥æ„Ÿé¢†åŸŸéœ€è¦ä¸åŒäºè‡ªç„¶å›¾åƒé¢†åŸŸçš„ç¼©æ”¾è§„å¾‹å’Œè®­ç»ƒæ–¹æ³•ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>We explore the scaling behaviors of artificial intelligence to establish practical techniques for training foundation models on high-resolution electro-optical (EO) datasets that exceed the current state-of-the-art scale by orders of magnitude. Modern multimodal machine learning (ML) applications, such as generative artificial intelligence (GenAI) systems for image captioning, search, and reasoning, depend on robust, domain-specialized encoders for non-text modalities. In natural-image domains where internet-scale data is plentiful, well-established scaling laws help optimize the joint scaling of model capacity, training compute, and dataset size. Unfortunately, these relationships are much less well-understood in high-value domains like remote sensing (RS). Using over a quadrillion pixels of commercial satellite EO data and the MITRE Federal AI Sandbox, we train progressively larger vision transformer (ViT) backbones, report success and failure modes observed at petascale, and analyze implications for bridging domain gaps across additional RS modalities. We observe that even at this scale, performance is consistent with a data limited regime rather than a model parameter-limited one. These practical insights are intended to inform data-collection strategies, compute budgets, and optimization schedules that advance the future development of frontier-scale RS foundation models.</p>
<h3 id="2-mgml-a-plug-and-play-meta-guided-multi-modal-learning-framework-for-incomplete-multimodal-brain-tumor-segmentation">[2] <a href="https://arxiv.org/abs/2512.23936">MGML: A Plug-and-Play Meta-Guided Multi-Modal Learning Framework for Incomplete Multimodal Brain Tumor Segmentation</a></h3>
<p><em>Yulong Zou, Bo Liu, Cun-Jing Zheng, Yuan-ming Geng, Siyue Li, Qiankun Zuo, Shuihua Wang, Yudong Zhang, Jin Hong</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å…ƒå¼•å¯¼çš„å¤šæ¨¡æ€å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å…ƒå‚æ•°åŒ–è‡ªé€‚åº”æ¨¡æ€èåˆå’Œä¸€è‡´æ€§æ­£åˆ™åŒ–æ¨¡å—ï¼Œæœ‰æ•ˆè§£å†³äº†ä¸´åºŠå®è·µä¸­å¤šæ¨¡æ€MRIæ•°æ®ä¸å®Œæ•´æƒ…å†µä¸‹çš„è„‘è‚¿ç˜¤åˆ†å‰²é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åœ¨ä¸´åºŠå®è·µä¸­ï¼Œå¤šæ¨¡æ€ç£å…±æŒ¯æˆåƒæ•°æ®å¸¸å¸¸ä¸å®Œæ•´ï¼Œè¿™ç»™å……åˆ†åˆ©ç”¨å¯ç”¨ä¿¡æ¯è¿›è¡Œç—…ç¶åˆ†å‰²å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è„‘è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡ä¸­ï¼Œå¦‚ä½•æœ€å¤§åŒ–åˆ©ç”¨ä¸å®Œæ•´çš„å¤šæ¨¡æ€ä¿¡æ¯æˆä¸ºä¸€ä¸ªå…³é”®çš„ç ”ç©¶é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„å…ƒå¼•å¯¼å¤šæ¨¡æ€å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå…ƒå‚æ•°åŒ–è‡ªé€‚åº”æ¨¡æ€èåˆæ¨¡å—å’Œä¸€è‡´æ€§æ­£åˆ™åŒ–æ¨¡å—ã€‚å…ƒå‚æ•°åŒ–è‡ªé€‚åº”æ¨¡æ€èåˆèƒ½å¤Ÿæ ¹æ®å¯ç”¨æ¨¡æ€ç”Ÿæˆè‡ªé€‚åº”è½¯æ ‡ç­¾ç›‘ç£ä¿¡å·ï¼Œåœ¨ä¸åŒè¾“å…¥æ¡ä»¶ä¸‹æœ‰æ•ˆæ•´åˆå¤šæ¨¡æ€ä¿¡æ¯ï¼›ä¸€è‡´æ€§æ­£åˆ™åŒ–æ¨¡å—åˆ™å¢å¼ºåˆ†å‰²æ€§èƒ½å¹¶éšå¼æå‡æ¡†æ¶çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä¸”è¯¥æ–¹æ³•ä¸æ”¹å˜åŸå§‹æ¨¡å‹æ¶æ„ï¼Œå¯æ–¹ä¾¿é›†æˆåˆ°ç«¯åˆ°ç«¯è®­ç»ƒæµç¨‹ä¸­ã€‚</p>
<p><strong>Result:</strong> åœ¨å…¬å¼€æ•°æ®é›†BraTS2020å’ŒBraTS2023ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åäº”ç§ç¼ºå¤±æ¨¡æ€ç»„åˆçš„å¹³å‡Diceåˆ†æ•°ä¸Šä¼˜äºå¤šç§æœ€å…ˆè¿›æ–¹æ³•ã€‚åœ¨BraTS2020ä¸Šï¼ŒåŸºäºåŸºçº¿æ¨¡å‹ï¼Œè¯¥æ–¹æ³•åœ¨å…¨è‚¿ç˜¤ã€è‚¿ç˜¤æ ¸å¿ƒå’Œå¢å¼ºè‚¿ç˜¤åŒºåŸŸåˆ†åˆ«è·å¾—äº†87.55ã€79.36å’Œ62.67çš„Diceåˆ†æ•°ï¼Œå±•ç°äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºè§£å†³ä¸å®Œæ•´å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²é—®é¢˜æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡å…ƒå¼•å¯¼çš„èåˆæœºåˆ¶å’Œä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œä¸ä»…æå‡äº†åˆ†å‰²ç²¾åº¦ï¼Œè¿˜å¢å¼ºäº†æ¨¡å‹å¯¹ç¼ºå¤±æ¨¡æ€çš„é²æ£’æ€§ï¼Œä¸ºä¸´åºŠå®é™…åº”ç”¨ä¸­çš„å¤šæ¨¡æ€æ•°æ®å¤„ç†æä¾›äº†å®ç”¨æ¡†æ¶ï¼Œä¸”å…¶æ¨¡å—åŒ–è®¾è®¡ä¾¿äºé›†æˆåˆ°ç°æœ‰è®­ç»ƒæµç¨‹ä¸­ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Leveraging multimodal information from Magnetic Resonance Imaging (MRI) plays a vital role in lesion segmentation, especially for brain tumors. However, in clinical practice, multimodal MRI data are often incomplete, making it challenging to fully utilize the available information. Therefore, maximizing the utilization of this incomplete multimodal information presents a crucial research challenge. We present a novel meta-guided multi-modal learning (MGML) framework that comprises two components: meta-parameterized adaptive modality fusion and consistency regularization module. The meta-parameterized adaptive modality fusion (Meta-AMF) enables the model to effectively integrate information from multiple modalities under varying input conditions. By generating adaptive soft-label supervision signals based on the available modalities, Meta-AMF explicitly promotes more coherent multimodal fusion. In addition, the consistency regularization module enhances segmentation performance and implicitly reinforces the robustness and generalization of the overall framework. Notably, our approach does not alter the original model architecture and can be conveniently integrated into the training pipeline for end-to-end model optimization. We conducted extensive experiments on the public BraTS2020 and BraTS2023 datasets. Compared to multiple state-of-the-art methods from previous years, our method achieved superior performance. On BraTS2020, for the average Dice scores across fifteen missing modality combinations, building upon the baseline, our method obtained scores of 87.55, 79.36, and 62.67 for the whole tumor (WT), the tumor core (TC), and the enhancing tumor (ET), respectively. We have made our source code publicly available at https://github.com/worldlikerr/MGML.</p>
<h3 id="3-t2vattack-adversarial-attack-on-text-to-video-diffusion-models">[3] <a href="https://arxiv.org/abs/2512.23953">T2VAttack: Adversarial Attack on Text-to-Video Diffusion Models</a></h3>
<p><em>Changzhen Li, Yuecong Min, Jie Zhang, Zheng Yuan, Shiguang Shan, Xilin Chen</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†T2VAttackï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹å¯¹æŠ—æ”»å‡»çš„å…¨é¢ç ”ç©¶ï¼Œé€šè¿‡è¯­ä¹‰å’Œæ—¶é—´ä¸¤ä¸ªç»´åº¦è¯„ä¼°æ¨¡å‹çš„è„†å¼±æ€§ï¼Œæ­ç¤ºäº†å³ä½¿å¾®å°çš„æç¤ºä¿®æ”¹ä¹Ÿèƒ½å¯¼è‡´è§†é¢‘ç”Ÿæˆè´¨é‡çš„æ˜¾è‘—ä¸‹é™ã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡ã€æ—¶é—´è¿è´¯çš„è§†é¢‘æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶å¯¹æŠ—æ”»å‡»çš„è„†å¼±æ€§å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œè§†é¢‘æ•°æ®çš„åŠ¨æ€ç‰¹æ€§è¦æ±‚ä»è¯­ä¹‰å’Œæ—¶é—´ä¸¤ä¸ªç»´åº¦è¯„ä¼°æ¨¡å‹çš„é²æ£’æ€§ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†ä¸¤ç§æ”»å‡»ç›®æ ‡ï¼šè¯­ä¹‰ç›®æ ‡è¯„ä¼°è§†é¢‘-æ–‡æœ¬å¯¹é½ï¼Œæ—¶é—´ç›®æ ‡è¯„ä¼°æ—¶é—´åŠ¨æ€ï¼›å¹¶å¼€å‘äº†ä¸¤ç§æ”»å‡»æ–¹æ³•ï¼šT2VAttack-Sé€šè¿‡è´ªå©ªæœç´¢è¯†åˆ«å…³é”®è¯å¹¶ç”¨åŒä¹‰è¯æ›¿æ¢ï¼ŒT2VAttack-Ié€šè¿‡è¿­ä»£æ’å…¥ä¼˜åŒ–è¯å®ç°æœ€å°æç¤ºæ‰°åŠ¨ã€‚</p>
<p><strong>Result:</strong> å®éªŒåœ¨å¤šä¸ªæœ€å…ˆè¿›çš„T2Væ¨¡å‹ï¼ˆåŒ…æ‹¬ModelScopeã€CogVideoXã€Open-Soraå’ŒHunyuanVideoï¼‰ä¸Šè¿›è¡Œï¼Œç»“æœè¡¨æ˜å³ä½¿å•ä¸ªè¯çš„æ›¿æ¢æˆ–æ’å…¥ä¹Ÿä¼šå¯¼è‡´è¯­ä¹‰ä¿çœŸåº¦å’Œæ—¶é—´åŠ¨æ€çš„æ˜¾è‘—é€€åŒ–ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹çš„ä¸¥é‡è„†å¼±æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨å¯¹æŠ—æ”»å‡»ä¸‹çš„å…³é”®æ¼æ´ï¼Œå¼ºè°ƒäº†åœ¨æ¨¡å‹å¼€å‘ä¸­è€ƒè™‘é²æ£’æ€§çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥æ›´å®‰å…¨çš„è§†é¢‘ç”Ÿæˆç³»ç»Ÿæä¾›äº†é‡è¦çš„å®‰å…¨è¯„ä¼°æ¡†æ¶å’ŒåŸºå‡†ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>The rapid evolution of Text-to-Video (T2V) diffusion models has driven remarkable advancements in generating high-quality, temporally coherent videos from natural language descriptions. Despite these achievements, their vulnerability to adversarial attacks remains largely unexplored. In this paper, we introduce T2VAttack, a comprehensive study of adversarial attacks on T2V diffusion models from both semantic and temporal perspectives. Considering the inherently dynamic nature of video data, we propose two distinct attack objectives: a semantic objective to evaluate video-text alignment and a temporal objective to assess the temporal dynamics. To achieve an effective and efficient attack process, we propose two adversarial attack methods: (i) T2VAttack-S, which identifies semantically or temporally critical words in prompts and replaces them with synonyms via greedy search, and (ii) T2VAttack-I, which iteratively inserts optimized words with minimal perturbation to the prompt. By combining these objectives and strategies, we conduct a comprehensive evaluation on the adversarial robustness of several state-of-the-art T2V models, including ModelScope, CogVideoX, Open-Sora, and HunyuanVideo. Our experiments reveal that even minor prompt modifications, such as the substitution or insertion of a single word, can cause substantial degradation in semantic fidelity and temporal dynamics, highlighting critical vulnerabilities in current T2V diffusion models.</p>
<h3 id="4-bridging-structure-and-appearance-topological-features-for-robust-self-supervised-segmentation">[4] <a href="https://arxiv.org/abs/2512.23997">Bridging Structure and Appearance: Topological Features for Robust Self-Supervised Segmentation</a></h3>
<p><em>Haotang Li, Zhenyu Qi, Hao Qin, Huanrui Yang, Sen He, Kebin Peng</em></p>
<h4 id="tldr_3">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºGASegæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥å¯å¾®ç›’è®¡æ•°æ¨¡å—å’Œæ‹“æ‰‘å¢å¼ºç­–ç•¥ï¼Œå°†å‡ ä½•æ‹“æ‰‘ä¿¡æ¯ä¸å¤–è§‚ç‰¹å¾ç›¸ç»“åˆï¼Œä»¥è§£å†³è‡ªç›‘ç£è¯­ä¹‰åˆ†å‰²ä¸­å› å¤–è§‚æ¨¡ç³Šæ€§å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_3">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è‡ªç›‘ç£è¯­ä¹‰åˆ†å‰²æ–¹æ³•åœ¨é¢å¯¹å¤–è§‚æ¨¡ç³Šæ€§æ—¶ç»å¸¸å¤±è´¥ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºè¿‡åº¦ä¾èµ–ä¸ç¨³å®šçš„å¤–è§‚ç‰¹å¾ï¼Œå¦‚é˜´å½±ã€çœ©å…‰å’Œå±€éƒ¨çº¹ç†ï¼Œè€Œå¿½ç•¥äº†æ›´ç¨³å®šçš„å‡ ä½•æ‹“æ‰‘ä¿¡æ¯ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯å¯å¾®ç›’è®¡æ•°æ¨¡å—ï¼Œç”¨äºä»å‡ ä½•ç‰¹å¾æµå’Œå¤–è§‚ç‰¹å¾æµä¸­é‡åŒ–å¤šå°ºåº¦æ‹“æ‰‘ç»Ÿè®¡ä¿¡æ¯ï¼›åŒæ—¶å¼•å…¥æ‹“æ‰‘å¢å¼ºç­–ç•¥ï¼Œé€šè¿‡å½¢æ€å­¦æ“ä½œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„æ¨¡ç³Šæ€§ï¼›æœ€åä½¿ç”¨GALosså¤šç›®æ ‡æŸå¤±å‡½æ•°æ˜¾å¼åœ°å¼ºåˆ¶å‡ ä½•ç‰¹å¾ä¸å¤–è§‚ç‰¹å¾ä¹‹é—´çš„è·¨æ¨¡æ€å¯¹é½ã€‚</p>
<p><strong>Result:</strong> åœ¨COCO-Stuffã€Cityscapeså’ŒPASCALç­‰å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒGASegå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒéªŒè¯äº†é€šè¿‡æ‹“æ‰‘ä¿¡æ¯æ¡¥æ¥å‡ ä½•ä¸å¤–è§‚æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡æ•´åˆç¨³å®šçš„å‡ ä½•æ‹“æ‰‘ä¿¡æ¯å¯ä»¥æœ‰æ•ˆç¼“è§£è‡ªç›‘ç£è¯­ä¹‰åˆ†å‰²ä¸­çš„å¤–è§‚æ¨¡ç³Šæ€§é—®é¢˜ï¼Œä¸ºè·¨æ¨¡æ€ç‰¹å¾å¯¹é½æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†æ‹“æ‰‘å¢å¼ºåœ¨æ¨¡æ‹ŸçœŸå®ä¸–ç•Œæ¨¡ç³Šæ€§æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_3">ğŸ“„ Abstract</h4>
<p>Self-supervised semantic segmentation methods often fail when faced with appearance ambiguities. We argue that this is due to an over-reliance on unstable, appearance-based features such as shadows, glare, and local textures. We propose \textbf{GASeg}, a novel framework that bridges appearance and geometry by leveraging stable topological information. The core of our method is Differentiable Box-Counting (\textbf{DBC}) module, which quantifies multi-scale topological statistics from two parallel streams: geometric-based features and appearance-based features. To force the model to learn these stable structural representations, we introduce Topological Augmentation (\textbf{TopoAug}), an adversarial strategy that simulates real-world ambiguities by applying morphological operators to the input images. A multi-objective loss, \textbf{GALoss}, then explicitly enforces cross-modal alignment between geometric-based and appearance-based features. Extensive experiments demonstrate that GASeg achieves state-of-the-art performance on four benchmarks, including COCO-Stuff, Cityscapes, and PASCAL, validating our approach of bridging geometry and appearance via topological information.</p>
<h3 id="5-bridging-the-perception-cognition-gapre-engineering-sam2-with-hilbert-mamba-for-robust-vlm-based-medical-diagnosis">[5] <a href="https://arxiv.org/abs/2512.24013">Bridging the Perception-Cognition Gap:Re-engineering SAM2 with Hilbert-Mamba for Robust VLM-based Medical Diagnosis</a></h3>
<p><em>Hao Wu, Hui Li, Yiyun Su</em></p>
<h4 id="tldr_4">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºHilbert-VLMï¼Œä¸€ç§ç”¨äºä¸‰ç»´å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†æçš„ä¸¤é˜¶æ®µèåˆæ¡†æ¶ï¼Œé€šè¿‡å°†å¸Œå°”ä¼¯ç‰¹ç©ºé—´å¡«å……æ›²çº¿é›†æˆåˆ°MambaçŠ¶æ€ç©ºé—´æ¨¡å‹ä¸­ï¼Œæœ‰æ•ˆä¿ç•™ç©ºé—´å±€éƒ¨æ€§ï¼Œå¹¶ç»“åˆå¢å¼ºæç¤ºå¼•å¯¼è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå‡†ç¡®ç–¾ç—…åˆ†ç±»ã€‚</p>
<hr />
<h4 id="detailed-summary_4">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚ä¸‰ç»´å¤šæ¨¡æ€åŒ»å­¦å›¾åƒæ—¶é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šäº’è¡¥ä¿¡æ¯çš„æœ‰æ•ˆæ•´åˆä»¥åŠç»†å¾®ä½†å…³é”®ç—…ç†ç‰¹å¾çš„å¶å°”é—æ¼ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨è‡ªåŠ¨åŒ–åŒ»ç–—è¯Šæ–­ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</p>
<p><strong>Method:</strong> æå‡ºHilbert-VLMä¸¤é˜¶æ®µèåˆæ¡†æ¶ï¼ŒåŒ…å«HilbertMed-SAMæ¨¡å—ç”¨äºç²¾ç¡®ç—…ç¶åˆ†å‰²å’Œæç¤ºå¢å¼ºæ¨¡å—ã€‚æ ¸å¿ƒåˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°é‡æ–°è®¾è®¡SAM2æ¶æ„ï¼šå°†å¸Œå°”ä¼¯ç‰¹ç©ºé—´å¡«å……æ›²çº¿é›†æˆåˆ°MambaçŠ¶æ€ç©ºé—´æ¨¡å‹çš„æ‰«ææœºåˆ¶ä¸­ä»¥æœ€å¤§åŒ–ä¿ç•™ä¸‰ç»´æ•°æ®çš„ç©ºé—´å±€éƒ¨æ€§ï¼ŒåŒæ—¶å¼•å…¥å¸Œå°”ä¼¯ç‰¹-Mambaäº¤å‰æ³¨æ„åŠ›æœºåˆ¶å’Œå°ºåº¦æ„ŸçŸ¥è§£ç å™¨ä»¥æ•è·ç»†ç²’åº¦ç»†èŠ‚ï¼Œåˆ†å‰²æ©ç ä¸å¯¹åº”æ–‡æœ¬å±æ€§è¢«ç»Ÿä¸€ä¸ºä¿¡æ¯å¯†é›†æç¤ºä»¥æ”¯æŒè§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†ã€‚</p>
<p><strong>Result:</strong> åœ¨BraTS2021åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¨¡å‹è¾¾åˆ°82.35%çš„Diceåˆ†æ•°ï¼Œç–¾ç—…åˆ†ç±»å‡†ç¡®ç‡è¾¾åˆ°78.85%ï¼Œå®éªŒç»“æœè¡¨æ˜æ‰€ææ¨¡å‹åœ¨åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹åˆ†ææ–¹é¢å…·æœ‰æ˜¾è‘—æ”¹è¿›å‡†ç¡®æ€§å’Œå¯é æ€§çš„æ½œåŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡ç³»ç»Ÿæ€§åœ°é‡æ–°è®¾è®¡æ¶æ„ä»¥ä¿ç•™ä¸‰ç»´åŒ»å­¦å›¾åƒç©ºé—´å±€éƒ¨æ€§çš„é‡è¦æ€§ï¼ŒHilbert-VLMæ¡†æ¶ä¸ºå¤„ç†å¤æ‚å¤šæ¨¡æ€åŒ»å­¦å›¾åƒæä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†åœ¨è‡ªåŠ¨åŒ–åŒ»ç–—è¯Šæ–­ä¸­æé«˜è§†è§‰è¯­è¨€æ¨¡å‹å‡†ç¡®æ€§å’Œå¯é æ€§çš„å®è´¨æ€§æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_4">ğŸ“„ Abstract</h4>
<p>Recent studies suggest that Visual Language Models (VLMs) hold great potential for tasks such as automated medical diagnosis. However, processing complex three-dimensional (3D) multimodal medical images poses significant challenges - specifically, the effective integration of complementary information and the occasional oversight of subtle yet critical pathological features. To address these issues, we present a novel two-stage fusion framework termed Hilbert-VLM. This framework leverages the HilbertMed-SAM module for precise lesion segmentation, with the generated multimodal enhanced prompts then guiding the VLM toward accurate disease classification. Our key innovation lies in the systematic redesign of the Segment Anything Model 2 (SAM2) architecture: we incorporate Hilbert space-filling curves into the scanning mechanism of the Mamba State Space Model (SSM) to maximize the preservation of spatial locality in 3D data, a property critical for medical image analysis. We also introduce a novel Hilbert-Mamba Cross-Attention (HMCA) mechanism and a scale-aware decoder to capture fine-grained details. Meanwhile, the prompt enhancement module unifies segmentation masks and their corresponding textual attributes into an information-dense prompt to support VLM inference. Extensive experiments were conducted to validate the effectiveness of the Hilbert-VLM model. On the BraTS2021 segmentation benchmark, it achieves a Dice score of 82.35 percent, with a diagnostic classification accuracy (ACC) of 78.85 percent. These results demonstrate that the proposed model offers substantial potential to improve the accuracy and reliability of medical VLM-based analysis.</p>
<h3 id="6-fuse-rsvlm-feature-fusion-vision-language-model-for-remote-sensing">[6] <a href="https://arxiv.org/abs/2512.24022">FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing</a></h3>
<p><em>Yunkai Dang, Donghao Wang, Jiacheng Yang, Yifan Jiang, Meiyi Zhu, Yuekun Yang, Cong Wang, Qi Fan, Wenbin Li, Yang Gao</em></p>
<h4 id="tldr_5">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºMF-RSVLMï¼Œä¸€ç§å¤šç‰¹å¾èåˆçš„é¥æ„Ÿè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å¤šå°ºåº¦è§†è§‰è¡¨å¾å­¦ä¹ å’Œå¾ªç¯è§†è§‰ç‰¹å¾æ³¨å…¥æœºåˆ¶ï¼Œæœ‰æ•ˆè§£å†³äº†é¥æ„Ÿå›¾åƒç†è§£ä¸­çš„ç»†ç²’åº¦ç‰¹å¾æå–å’Œè§†è§‰é—å¿˜é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_5">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é€šç”¨é¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨é¥æ„Ÿé¢†åŸŸé¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œä¸»è¦æºäºé¥æ„Ÿå›¾åƒä¸è‡ªç„¶å›¾åƒçš„å›ºæœ‰å·®å¼‚ã€‚ç°æœ‰é¥æ„ŸVLMéš¾ä»¥æå–ç»†ç²’åº¦è§†è§‰ç‰¹å¾ï¼Œå¹¶åœ¨æ·±åº¦è¯­è¨€å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°è§†è§‰é—å¿˜ç°è±¡ï¼Œé™åˆ¶äº†å…¶åœ¨é¥æ„Ÿåœºæ™¯ç†è§£ä¸­çš„åº”ç”¨æ•ˆæœã€‚</p>
<p><strong>Method:</strong> MF-RSVLMé‡‡ç”¨å¤šç‰¹å¾èåˆæ¶æ„ï¼Œå­¦ä¹ å¤šå°ºåº¦è§†è§‰è¡¨å¾ï¼Œå°†å…¨å±€ä¸Šä¸‹æ–‡ä¸å±€éƒ¨ç»†èŠ‚ç›¸ç»“åˆï¼Œä»¥æ›´å¥½åœ°æ•æ‰é¥æ„Ÿåœºæ™¯ä¸­çš„å°å‹å¤æ‚ç»“æ„ã€‚æ¨¡å‹å¼•å…¥å¾ªç¯è§†è§‰ç‰¹å¾æ³¨å…¥æ–¹æ¡ˆï¼Œç¡®ä¿è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æŒç»­åŸºäºè§†è§‰è¯æ®ï¼Œæœ‰æ•ˆå‡å°‘è§†è§‰é—å¿˜é—®é¢˜ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªé¥æ„ŸåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMF-RSVLMåœ¨é¥æ„Ÿåˆ†ç±»ã€å›¾åƒæè¿°å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸­å‡å–å¾—äº†æœ€å…ˆè¿›æˆ–æå…·ç«äº‰åŠ›çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹åœ¨ç»†ç²’åº¦ç‰¹å¾æå–å’Œè§†è§‰ä¿¡æ¯ä¿æŒæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºé¥æ„Ÿé¢†åŸŸçš„è§†è§‰è¯­è¨€å»ºæ¨¡æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡å¤šå°ºåº¦ç‰¹å¾èåˆå’Œå¾ªç¯è§†è§‰æ³¨å…¥æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹é¥æ„Ÿå›¾åƒçš„ç†è§£èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ä¸ºè§£å†³é¥æ„Ÿå›¾åƒä¸è‡ªç„¶å›¾åƒå·®å¼‚å¸¦æ¥çš„æŒ‘æˆ˜æä¾›äº†æ–°æ€è·¯ï¼Œå¹¶ä¸ºé¥æ„Ÿå¤šæ¨¡æ€ä»»åŠ¡çš„å‘å±•å¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_5">ğŸ“„ Abstract</h4>
<p>Large vision-language models (VLMs) exhibit strong performance across various tasks. However, these VLMs encounter significant challenges when applied to the remote sensing domain due to the inherent differences between remote sensing images and natural images. Existing remote sensing VLMs often fail to extract fine-grained visual features and suffer from visual forgetting during deep language processing. To address this, we introduce MF-RSVLM, a Multi-Feature Fusion Remote Sensing Vision--Language Model that effectively extracts and fuses visual features for RS understanding. MF-RSVLM learns multi-scale visual representations and combines global context with local details, improving the capture of small and complex structures in RS scenes. A recurrent visual feature injection scheme ensures the language model remains grounded in visual evidence and reduces visual forgetting during generation. Extensive experiments on diverse RS benchmarks show that MF-RSVLM achieves state-of-the-art or highly competitive performance across remote sensing classification, image captioning, and VQA tasks. Our code is publicly available at https://github.com/Yunkaidang/RSVLM.</p>
<h3 id="7-rsagent-learning-to-reason-and-act-for-text-guided-segmentation-via-multi-turn-tool-invocations">[7] <a href="https://arxiv.org/abs/2512.24023">RSAgent: Learning to Reason and Act for Text-Guided Segmentation via Multi-Turn Tool Invocations</a></h3>
<p><em>Xingqi He, Yujie Zhang, Shuyong Gao, Wenjie Li, Lingyi Hong, Mingxi Chen, Kaixun Jiang, Jiyuan Fu, Wenqiang Zhang</em></p>
<h4 id="tldr_6">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºRSAgentï¼Œä¸€ç§åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé€šè¿‡å¤šè½®å·¥å…·è°ƒç”¨å®ç°æ–‡æœ¬å¼•å¯¼çš„å¯¹è±¡åˆ†å‰²ï¼Œå°†æ¨ç†ä¸è¡ŒåŠ¨äº¤ç»‡ä»¥è§£å†³ä¼ ç»Ÿå•æ¬¡å®šä½æ–¹æ³•çš„å±€é™æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_6">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æ–‡æœ¬å¼•å¯¼åˆ†å‰²æ–¹æ³•é€šå¸¸å°†ä»»åŠ¡è§†ä¸ºå•æ¬¡å®šä½é—®é¢˜ï¼Œæ¨¡å‹é€šè¿‡å•æ¬¡å‰å‘ä¼ æ’­é¢„æµ‹åƒç´ æç¤ºæ¥é©±åŠ¨å¤–éƒ¨åˆ†å‰²å™¨ï¼Œè¿™ç§æ–¹æ³•åœ¨åˆå§‹å®šä½é”™è¯¯æ—¶ç¼ºä¹éªŒè¯ã€é‡æ–°èšç„¦å’Œç»†åŒ–çš„èƒ½åŠ›ï¼Œé™åˆ¶äº†åˆ†å‰²æ€§èƒ½çš„è¿›ä¸€æ­¥æå‡ã€‚</p>
<p><strong>Method:</strong> RSAgenté‡‡ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡å¤šè½®å·¥å…·è°ƒç”¨å®ç°æ¨ç†ä¸è¡ŒåŠ¨çš„äº¤ç»‡ï¼Œå…·ä½“åŒ…æ‹¬æŸ¥è¯¢åˆ†å‰²å·¥å…·ç®±ã€è§‚å¯Ÿè§†è§‰åé¦ˆã€åˆ©ç”¨å†å²è§‚å¯Ÿä¿®æ­£ç©ºé—´å‡è®¾ä»¥é‡æ–°å®šä½ç›®æ ‡å¹¶è¿­ä»£ä¼˜åŒ–æ©ç ã€‚ç ”ç©¶è¿˜æ„å»ºäº†åˆæˆå¤šè½®æ¨ç†åˆ†å‰²è½¨è¿¹çš„æ•°æ®ç®¡é“ï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼šå†·å¯åŠ¨ç›‘ç£å¾®è°ƒåæ¥åŸºäºç»†ç²’åº¦ä»»åŠ¡ç‰¹å®šå¥–åŠ±çš„æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒRSAgentåœ¨ReasonSegæµ‹è¯•é›†ä¸Šè¾¾åˆ°66.5%çš„é›¶æ ·æœ¬å¹¿ä¹‰äº¤å¹¶æ¯”ï¼Œç›¸æ¯”Seg-Zero-7Bæå‡9%ï¼Œåœ¨RefCOCOgæ•°æ®é›†ä¸Šè¾¾åˆ°81.5%çš„ç±»åˆ«äº¤å¹¶æ¯”ï¼Œåœ¨é¢†åŸŸå†…å’Œé¢†åŸŸå¤–åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºæœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†å°†æ™ºèƒ½ä½“èŒƒå¼å¼•å…¥æ–‡æœ¬å¼•å¯¼åˆ†å‰²ä»»åŠ¡çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡å¤šè½®äº¤äº’å¼æ¨ç†æ˜¾è‘—æå‡äº†åˆ†å‰²ç²¾åº¦å’Œé²æ£’æ€§ï¼Œä¸ºå¤æ‚è§†è§‰è¯­è¨€ä»»åŠ¡æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆæ¡†æ¶ï¼Œå¹¶éªŒè¯äº†å¼ºåŒ–å­¦ä¹ åœ¨ç»†ç²’åº¦è§†è§‰ä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ã€‚</p>
<hr />
<h4 id="abstract_6">ğŸ“„ Abstract</h4>
<p>Text-guided object segmentation requires both cross-modal reasoning and pixel grounding abilities. Most recent methods treat text-guided segmentation as one-shot grounding, where the model predicts pixel prompts in a single forward pass to drive an external segmentor, which limits verification, refocusing and refinement when initial localization is wrong. To address this limitation, we propose RSAgent, an agentic Multimodal Large Language Model (MLLM) which interleaves reasoning and action for segmentation via multi-turn tool invocations. RSAgent queries a segmentation toolbox, observes visual feedback, and revises its spatial hypothesis using historical observations to re-localize targets and iteratively refine masks. We further build a data pipeline to synthesize multi-turn reasoning segmentation trajectories, and train RSAgent with a two-stage framework: cold-start supervised fine-tuning followed by agentic reinforcement learning with fine-grained, task-specific rewards. Extensive experiments show that RSAgent achieves a zero-shot performance of 66.5% gIoU on ReasonSeg test, improving over Seg-Zero-7B by 9%, and reaches 81.5% cIoU on RefCOCOg, demonstrating state-of-the-art performance on both in-domain and out-of-domain benchmarks.</p>
<h3 id="8-neighbor-aware-instance-refining-with-noisy-labels-for-cross-modal-retrieval">[8] <a href="https://arxiv.org/abs/2512.24064">Neighbor-aware Instance Refining with Noisy Labels for Cross-Modal Retrieval</a></h3>
<p><em>Yizhi Liu, Ruitao Pu, Shilin Xu, Yingke Chen, Quan-Hui Liu, Yuan Sun</em></p>
<h4 id="tldr_7">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºNIRNLçš„é²æ£’è·¨æ¨¡æ€å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡è·¨æ¨¡æ€è¾¹ç•Œä¿æŒå’Œé‚»å±…æ„ŸçŸ¥å®ä¾‹ç²¾ç‚¼æŠ€æœ¯ï¼Œæœ‰æ•ˆå¤„ç†å¤šæ¨¡æ€æ•°æ®ä¸­çš„å™ªå£°æ ‡ç­¾é—®é¢˜ï¼Œåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ£€ç´¢æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_7">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è·¨æ¨¡æ€æ£€ç´¢é¢†åŸŸé¢ä¸´å¤§è§„æ¨¡é«˜è´¨é‡æ ‡æ³¨æ•°æ®æ”¶é›†å›°éš¾çš„é—®é¢˜ï¼Œå¤šæ¨¡æ€æ•°æ®æ ‡æ³¨ä¸­ä¸å¯é¿å…åœ°å­˜åœ¨å™ªå£°æ ‡ç­¾ï¼Œè¿™ä¼šæ˜¾è‘—é™ä½æ¨¡å‹çš„æ£€ç´¢æ€§èƒ½ã€‚ç°æœ‰é²æ£’å­¦ä¹ æ–¹æ³•åœ¨æ¨¡å‹æ€§èƒ½ä¸Šé™ã€æ ¡å‡†å¯é æ€§å’Œæ•°æ®åˆ©ç”¨ç‡æ–¹é¢éš¾ä»¥åŒæ—¶æ»¡è¶³è¦æ±‚ï¼Œéœ€è¦ä¸€ç§æ›´æœ‰æ•ˆçš„å™ªå£°æ ‡ç­¾å¤„ç†æ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> æå‡ºçš„NIRNLæ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæŠ€æœ¯ï¼šè·¨æ¨¡æ€è¾¹ç•Œä¿æŒç”¨äºè°ƒæ•´æ­£è´Ÿæ ·æœ¬å¯¹çš„ç›¸å¯¹è·ç¦»ä»¥å¢å¼ºæ ·æœ¬å¯¹åŒºåˆ†åº¦ï¼›é‚»å±…æ„ŸçŸ¥å®ä¾‹ç²¾ç‚¼é€šè¿‡è·¨æ¨¡æ€é‚»å±…å…±è¯†è¯†åˆ«çº¯å­é›†ã€å›°éš¾å­é›†å’Œå™ªå£°å­é›†ï¼Œå¹¶ä¸ºè¿™ç§ç»†ç²’åº¦åˆ’åˆ†æ„å»ºå®šåˆ¶åŒ–ä¼˜åŒ–ç­–ç•¥ï¼Œæœ€å¤§åŒ–æ•°æ®åˆ©ç”¨ç‡åŒæ—¶å‡å°‘è¯¯å·®ä¼ æ’­ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒNIRNLå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„é²æ£’æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜å™ªå£°ç‡æ¡ä»¶ä¸‹ã€‚è¯¥æ–¹æ³•åœ¨æ¨¡å‹æ€§èƒ½ã€æ ¡å‡†å¯é æ€§å’Œæ•°æ®åˆ©ç”¨ç‡æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡ç»†ç²’åº¦çš„å®ä¾‹åˆ’åˆ†å’Œå®šåˆ¶åŒ–ä¼˜åŒ–ç­–ç•¥ï¼Œæœ‰æ•ˆè§£å†³äº†è·¨æ¨¡æ€æ£€ç´¢ä¸­çš„å™ªå£°æ ‡ç­¾é—®é¢˜ï¼Œä¸ºå¤šæ¨¡æ€å­¦ä¹ ä¸­çš„å™ªå£°é²æ£’æ€§æä¾›äº†æ–°æ€è·¯ã€‚æ¡†æ¶çš„è®¾è®¡å¹³è¡¡äº†æ•°æ®åˆ©ç”¨æ•ˆç‡å’Œè¯¯å·®æ§åˆ¶ï¼Œå…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_7">ğŸ“„ Abstract</h4>
<p>In recent years, Cross-Modal Retrieval (CMR) has made significant progress in the field of multi-modal analysis. However, since it is time-consuming and labor-intensive to collect large-scale and well-annotated data, the annotation of multi-modal data inevitably contains some noise. This will degrade the retrieval performance of the model. To tackle the problem, numerous robust CMR methods have been developed, including robust learning paradigms, label calibration strategies, and instance selection mechanisms. Unfortunately, they often fail to simultaneously satisfy model performance ceilings, calibration reliability, and data utilization rate. To overcome the limitations, we propose a novel robust cross-modal learning framework, namely Neighbor-aware Instance Refining with Noisy Labels (NIRNL). Specifically, we first propose Cross-modal Margin Preserving (CMP) to adjust the relative distance between positive and negative pairs, thereby enhancing the discrimination between sample pairs. Then, we propose Neighbor-aware Instance Refining (NIR) to identify pure subset, hard subset, and noisy subset through cross-modal neighborhood consensus. Afterward, we construct different tailored optimization strategies for this fine-grained partitioning, thereby maximizing the utilization of all available data while mitigating error propagation. Extensive experiments on three benchmark datasets demonstrate that NIRNL achieves state-of-the-art performance, exhibiting remarkable robustness, especially under high noise rates.</p>
<h3 id="9-balanced-hierarchical-contrastive-learning-with-decoupled-queries-for-fine-grained-object-detection-in-remote-sensing-images">[9] <a href="https://arxiv.org/abs/2512.24074">Balanced Hierarchical Contrastive Learning with Decoupled Queries for Fine-grained Object Detection in Remote Sensing Images</a></h3>
<p><em>Jingzhou Chen, Dexin Chen, Fengchao Xiong, Yuntao Qian, Liang Xiao</em></p>
<h4 id="tldr_8">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¹³è¡¡å±‚æ¬¡å¯¹æ¯”æŸå¤±ä¸è§£è€¦å­¦ä¹ ç­–ç•¥ï¼Œä»¥è§£å†³ç»†ç²’åº¦é¥æ„Ÿç›®æ ‡æ£€æµ‹ä¸­å±‚æ¬¡æ ‡ç­¾ç»“æ„åµŒå…¥çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•åœ¨DETRæ¡†æ¶å†…é€šè¿‡å¯å­¦ä¹ çš„ç±»åˆ«åŸå‹å’Œæ¢¯åº¦å‡è¡¡æœºåˆ¶ï¼Œæœ‰æ•ˆç¼“è§£äº†æ•°æ®åˆ†å¸ƒä¸å¹³è¡¡å¯¹å±‚æ¬¡è¯­ä¹‰å­¦ä¹ çš„å½±å“ã€‚</p>
<hr />
<h4 id="detailed-summary_8">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç»†ç²’åº¦é¥æ„Ÿæ•°æ®é›†é€šå¸¸é‡‡ç”¨å±‚æ¬¡æ ‡ç­¾ç»“æ„æ¥åŒºåˆ†å¯¹è±¡ï¼Œä½†å°†è¿™ç§è¯­ä¹‰å±‚æ¬¡åµŒå…¥è¡¨ç¤ºå­¦ä¹ ç©ºé—´ä»¥æå‡æ£€æµ‹æ€§èƒ½ä»å…·æŒ‘æˆ˜æ€§ã€‚å…ˆå‰ç ”ç©¶åœ¨åº”ç”¨ç›‘ç£å¯¹æ¯”å­¦ä¹ æ—¶å¿½è§†äº†ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šæ ‡ç­¾å±‚æ¬¡ä¸­æ•°æ®åˆ†å¸ƒä¸å¹³è¡¡å¯¼è‡´é«˜é¢‘ç±»åˆ«ä¸»å¯¼å­¦ä¹ è¿‡ç¨‹ï¼Œä»¥åŠç±»åˆ«é—´è¯­ä¹‰å…³ç³»å­¦ä¹ å¹²æ‰°äº†ç±»åˆ«æ— å…³çš„å®šä½ä»»åŠ¡ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§å¹³è¡¡å±‚æ¬¡å¯¹æ¯”æŸå¤±ä¸è§£è€¦å­¦ä¹ ç­–ç•¥ï¼Œç»“åˆåˆ°æ£€æµ‹å˜æ¢å™¨ï¼ˆDETRï¼‰æ¡†æ¶ä¸­ã€‚è¯¥æŸå¤±å¼•å…¥äº†å¯å­¦ä¹ çš„ç±»åˆ«åŸå‹ï¼Œå¹¶åœ¨æ¯ä¸ªå±‚æ¬¡çº§åˆ«å‡è¡¡ä¸åŒç±»åˆ«è´¡çŒ®çš„æ¢¯åº¦ï¼Œç¡®ä¿æ¯ä¸ªå±‚æ¬¡ç±»åˆ«åœ¨æ¯ä¸ªå°æ‰¹é‡ä¸­å¯¹æŸå¤±è®¡ç®—è´¡çŒ®ç›¸ç­‰ã€‚è§£è€¦ç­–ç•¥å°†DETRçš„å¯¹è±¡æŸ¥è¯¢åˆ†ä¸ºåˆ†ç±»å’Œå®šä½ä¸¤ä¸ªé›†åˆï¼Œå®ç°ä»»åŠ¡ç‰¹å®šçš„ç‰¹å¾æå–å’Œä¼˜åŒ–ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸‰ä¸ªå…·æœ‰å±‚æ¬¡æ ‡æ³¨çš„ç»†ç²’åº¦æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼Œå¹³è¡¡å±‚æ¬¡å¯¹æ¯”æŸå¤±æœ‰æ•ˆç¼“è§£äº†æ•°æ®ä¸å¹³è¡¡é—®é¢˜ï¼Œè€Œè§£è€¦å­¦ä¹ ç­–ç•¥æ˜¾è‘—æå‡äº†åˆ†ç±»å’Œå®šä½ä»»åŠ¡çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶è¯æ˜äº†åœ¨ç»†ç²’åº¦ç›®æ ‡æ£€æµ‹ä¸­æœ‰æ•ˆåˆ©ç”¨å±‚æ¬¡è¯­ä¹‰ä¿¡æ¯çš„é‡è¦æ€§ï¼Œæå‡ºçš„å¹³è¡¡å±‚æ¬¡å¯¹æ¯”æŸå¤±å’Œè§£è€¦å­¦ä¹ ç­–ç•¥ä¸ºè§£å†³æ•°æ®ä¸å¹³è¡¡å’Œä»»åŠ¡å¹²æ‰°é—®é¢˜æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚è¯¥æ–¹æ³•ä¸ºå±‚æ¬¡æ ‡æ³¨æ•°æ®çš„è¡¨ç¤ºå­¦ä¹ å¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œå…·æœ‰æ‰©å±•åˆ°å…¶ä»–ç»†ç²’åº¦è§†è§‰ä»»åŠ¡çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_8">ğŸ“„ Abstract</h4>
<p>Fine-grained remote sensing datasets often use hierarchical label structures to differentiate objects in a coarse-to-fine manner, with each object annotated across multiple levels. However, embedding this semantic hierarchy into the representation learning space to improve fine-grained detection performance remains challenging. Previous studies have applied supervised contrastive learning at different hierarchical levels to group objects under the same parent class while distinguishing sibling subcategories. Nevertheless, they overlook two critical issues: (1) imbalanced data distribution across the label hierarchy causes high-frequency classes to dominate the learning process, and (2) learning semantic relationships among categories interferes with class-agnostic localization. To address these issues, we propose a balanced hierarchical contrastive loss combined with a decoupled learning strategy within the detection transformer (DETR) framework. The proposed loss introduces learnable class prototypes and equilibrates gradients contributed by different classes at each hierarchical level, ensuring that each hierarchical class contributes equally to the loss computation in every mini-batch. The decoupled strategy separates DETR's object queries into classification and localization sets, enabling task-specific feature extraction and optimization. Experiments on three fine-grained datasets with hierarchical annotations demonstrate that our method outperforms state-of-the-art approaches.</p>
<h3 id="10-factorized-learning-for-temporally-grounded-video-language-models">[10] <a href="https://arxiv.org/abs/2512.24097">Factorized Learning for Temporally Grounded Video-Language Models</a></h3>
<p><em>Wenzheng Zeng, Difei Gao, Mike Zheng Shou, Hwee Tou Ng</em></p>
<h4 id="tldr_9">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºDÂ²VLMæ¡†æ¶å’Œå› å­åŒ–åå¥½ä¼˜åŒ–ç®—æ³•ï¼Œé€šè¿‡è§£è€¦æ—¶é—´å®šä½ä¸æ–‡æœ¬å“åº”çš„å­¦ä¹ æ¥è§£å†³è§†é¢‘è¯­è¨€æ¨¡å‹ä¸­äº‹ä»¶çº§æ„ŸçŸ¥çš„å‡†ç¡®æ€§é—®é¢˜ï¼Œå¼ºè°ƒä¸¤è€…ä¹‹é—´çš„é€»è¾‘å±‚æ¬¡ä¾èµ–å…³ç³»ã€‚</p>
<hr />
<h4 id="detailed-summary_9">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†é¢‘è¯­è¨€æ¨¡å‹åœ¨äº‹ä»¶çº§æ„ŸçŸ¥æ–¹é¢å­˜åœ¨æ—¶é—´å®šä½ä¸å‡†ç¡®çš„é—®é¢˜ï¼Œè€Œæ—¶é—´å®šä½ä¸æ–‡æœ¬å“åº”è¿™ä¸¤ä¸ªæ ¸å¿ƒä»»åŠ¡é€šå¸¸ä»¥è€¦åˆæ–¹å¼å¤„ç†ï¼Œç¼ºä¹æ¸…æ™°çš„é€»è¾‘å±‚æ¬¡ç»“æ„ï¼Œå¯¼è‡´ä¼˜åŒ–ç›®æ ‡æ¬¡ä¼˜ã€‚ç ”ç©¶è§‚å¯Ÿåˆ°å‡†ç¡®çš„æ—¶é—´è¯æ®å®šä½æ˜¯å¯é æ–‡æœ¬å“åº”çš„åŸºç¡€ï¼Œéœ€è¦ä»å› å­åŒ–å­¦ä¹ çš„è§’åº¦è§£å†³è¿™ä¸€é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºDÂ²VLMæ¡†æ¶ï¼Œé‡‡ç”¨"å…ˆå®šä½åå›ç­”å¹¶å¼•ç”¨è¯æ®"çš„èŒƒå¼ï¼Œè§£è€¦æ—¶é—´å®šä½ä¸æ–‡æœ¬å“åº”çš„å­¦ä¹ åŒæ—¶å¼ºè°ƒå…¶å†…åœ¨ä¾èµ–å…³ç³»ã€‚å¼•å…¥è¯æ®ä»¤ç‰Œè¿›è¡Œè¯æ®å®šä½ï¼Œä¸“æ³¨äºäº‹ä»¶çº§è§†è§‰è¯­ä¹‰æ•è·è€Œéå•çº¯çš„æ—¶é—´æˆ³è¡¨ç¤ºã€‚è¿›ä¸€æ­¥æå‡ºå› å­åŒ–åå¥½ä¼˜åŒ–ç®—æ³•ï¼Œå°†æ¦‚ç‡æ—¶é—´å®šä½å»ºæ¨¡æ˜¾å¼çº³å…¥ä¼˜åŒ–ç›®æ ‡ï¼Œå®ç°æ—¶é—´å®šä½å’Œæ–‡æœ¬å“åº”çš„åå¥½å­¦ä¹ ã€‚ä¸ºæ”¯æŒå› å­åŒ–åå¥½å­¦ä¹ ï¼Œæ„å»ºäº†åŒ…å«æ˜¾å¼æ—¶é—´å®šä½æ ‡æ³¨çš„åˆæˆæ•°æ®é›†ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šç§ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚DÂ²VLMæ¡†æ¶ç»“åˆFPOç®—æ³•åœ¨æ—¶é—´å®šä½å‡†ç¡®æ€§å’Œæ–‡æœ¬å“åº”è´¨é‡æ–¹é¢å‡å–å¾—æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†å› å­åŒ–å­¦ä¹ æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚è¯æ®ä»¤ç‰Œæœºåˆ¶æˆåŠŸæ•è·äº†è¶…è¶Šæ—¶é—´æˆ³è¡¨ç¤ºçš„äº‹ä»¶çº§è§†è§‰è¯­ä¹‰ä¿¡æ¯ï¼Œæé«˜äº†äº‹ä»¶æ„ŸçŸ¥çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¯å®äº†æ—¶é—´å®šä½ä¸æ–‡æœ¬å“åº”ä¹‹é—´çš„é€»è¾‘å±‚æ¬¡å…³ç³»ï¼Œå› å­åŒ–å­¦ä¹ æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæå‡è§†é¢‘è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚è¯æ®ä»¤ç‰Œå’ŒFPOç®—æ³•ä¸ºè§†é¢‘ç†è§£ä»»åŠ¡æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå¼ºè°ƒäº‹ä»¶çº§è¯­ä¹‰æ•è·çš„é‡è¦æ€§ã€‚è¯¥æ–¹æ³•ä¸ºè§†é¢‘è¯­è¨€æ¨¡å‹çš„è®¾è®¡æä¾›äº†æ–°çš„èŒƒå¼ï¼Œæœªæ¥å¯æ‰©å±•åˆ°æ›´å¤æ‚çš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­ã€‚</p>
<hr />
<h4 id="abstract_9">ğŸ“„ Abstract</h4>
<p>Recent video-language models have shown great potential for video understanding, but still struggle with accurate temporal grounding for event-level perception. We observe that two main factors in video understanding (i.e., temporal grounding and textual response) form a logical hierarchy: accurate temporal evidence grounding lays the foundation for reliable textual response. However, existing works typically handle these two tasks in a coupled manner without a clear logical structure, leading to sub-optimal objectives. We address this from a factorized learning perspective. We first propose D$^2$VLM, a framework that decouples the learning of these two tasks while also emphasizing their inherent dependency. We adopt a "grounding then answering with evidence referencing" paradigm and introduce evidence tokens for evidence grounding, which emphasize event-level visual semantic capture beyond the focus on timestamp representation in existing works. To further facilitate the learning of these two tasks, we introduce a novel factorized preference optimization (FPO) algorithm. Unlike standard preference optimization, FPO explicitly incorporates probabilistic temporal grounding modeling into the optimization objective, enabling preference learning for both temporal grounding and textual response. We also construct a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding. Experiments on various tasks demonstrate the clear advantage of our approach. Our source code is available at https://github.com/nusnlp/d2vlm.</p>
<h3 id="11-geobench-rethinking-multimodal-geometric-problem-solving-via-hierarchical-evaluation">[11] <a href="https://arxiv.org/abs/2512.24119">GeoBench: Rethinking Multimodal Geometric Problem-Solving via Hierarchical Evaluation</a></h3>
<p><em>Yuan Feng, Yue Yang, Xiaohan He, Jiatong Zhao, Jianlong Chen, Zijun Chen, Daocheng Fu, Qi Liu, Renqiu Xia, Bo Zhang, Junchi Yan</em></p>
<h4 id="tldr_10">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†GeoBenchï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹å‡ ä½•æ¨ç†èƒ½åŠ›çš„å±‚æ¬¡åŒ–åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡å››ä¸ªæ¨ç†çº§åˆ«å’Œå…­ä¸ªæ­£å¼éªŒè¯çš„ä»»åŠ¡æ¥ç³»ç»Ÿåˆ†ææ¨¡å‹åœ¨å‡ ä½•é—®é¢˜è§£å†³ä¸­çš„èƒ½åŠ›è¡¨ç°ã€‚</p>
<hr />
<h4 id="detailed-summary_10">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å‡ ä½•æ¨ç†è¯„ä¼°æ–¹é¢å­˜åœ¨ä¸‰ä¸ªä¸»è¦å±€é™ï¼šæ•™ç§‘ä¹¦åŸºå‡†å¯èƒ½å¯¼è‡´æµ‹è¯•æ•°æ®æ±¡æŸ“ã€è¿‡åº¦å¼ºè°ƒæœ€ç»ˆç­”æ¡ˆè€Œå¿½è§†æ¨ç†è¿‡ç¨‹ã€ä»¥åŠè¯Šæ–­ç²’åº¦ä¸è¶³ï¼Œè¿™äº›é—®é¢˜é˜»ç¢äº†å¯¹æ¨¡å‹å‡ ä½•æ¨ç†èƒ½åŠ›çš„å‡†ç¡®è¯„ä¼°ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†GeoBenchå±‚æ¬¡åŒ–åŸºå‡†ï¼ŒåŒ…å«è§†è§‰æ„ŸçŸ¥ã€ç›®æ ‡å¯¼å‘è§„åˆ’ã€ä¸¥æ ¼å®šç†åº”ç”¨å’Œè‡ªæˆ‘åæ€å›æº¯å››ä¸ªæ¨ç†çº§åˆ«ï¼Œé€šè¿‡TrustGeoGenç”Ÿæˆçš„å…­ä¸ªæ­£å¼éªŒè¯ä»»åŠ¡æ¥ç³»ç»Ÿè¯„ä¼°ä»å±æ€§æå–åˆ°é€»è¾‘é”™è¯¯æ ¡æ­£çš„èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶æ¨ç†æ¨¡å‹å¦‚OpenAI-o3ä¼˜äºé€šç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œä½†éšç€ä»»åŠ¡å¤æ‚åº¦å¢åŠ ï¼Œæ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼›å­ç›®æ ‡åˆ†è§£å’Œæ— å…³å‰æè¿‡æ»¤å¯¹æœ€ç»ˆé—®é¢˜è§£å†³å‡†ç¡®æ€§æœ‰é‡è¦å½±å“ï¼Œè€Œæ€ç»´é“¾æç¤ºåœ¨æŸäº›ä»»åŠ¡ä¸­æ„å¤–åœ°é™ä½äº†æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> GeoBenchä¸ºå‡ ä½•é—®é¢˜è§£å†³æä¾›äº†å…¨é¢çš„è¯„ä¼°åŸºå‡†ï¼ŒåŒæ—¶æ­ç¤ºäº†å­ç›®æ ‡åˆ†è§£å’Œå‰æè¿‡æ»¤ç­‰å…³é”®å› ç´ å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œä¸ºå¼€å‘å‡ ä½•é—®é¢˜è§£å†³ç³»ç»Ÿæä¾›äº†å¯æ“ä½œçš„æŒ‡å¯¼åŸåˆ™ã€‚</p>
<hr />
<h4 id="abstract_10">ğŸ“„ Abstract</h4>
<p>Geometric problem solving constitutes a critical branch of mathematical reasoning, requiring precise analysis of shapes and spatial relationships. Current evaluations of geometric reasoning in vision-language models (VLMs) face limitations, including the risk of test data contamination from textbook-based benchmarks, overemphasis on final answers over reasoning processes, and insufficient diagnostic granularity. To address these issues, we present GeoBench, a hierarchical benchmark featuring four reasoning levels in geometric problem-solving: Visual Perception, Goal-Oriented Planning, Rigorous Theorem Application, and Self-Reflective Backtracking. Through six formally verified tasks generated via TrustGeoGen, we systematically assess capabilities ranging from attribute extraction to logical error correction. Experiments reveal that while reasoning models like OpenAI-o3 outperform general MLLMs, performance declines significantly with increasing task complexity. Key findings demonstrate that sub-goal decomposition and irrelevant premise filtering critically influence final problem-solving accuracy, whereas Chain-of-Thought prompting unexpectedly degrades performance in some tasks. These findings establish GeoBench as a comprehensive benchmark while offering actionable guidelines for developing geometric problem-solving systems.</p>
<h3 id="12-dermavqa-das-dermatology-assessment-schema-das-datasets-for-closed-ended-question-answering-segmentation-in-patient-generated-dermatology-images">[12] <a href="https://arxiv.org/abs/2512.24340">DermaVQA-DAS: Dermatology Assessment Schema (DAS) &amp; Datasets for Closed-Ended Question Answering &amp; Segmentation in Patient-Generated Dermatology Images</a></h3>
<p><em>Wen-wai Yim, Yujuan Fu, Asma Ben Abacha, Meliha Yetisgen, Noel Codella, Roberto Andres Novoa, Josep Malvehy</em></p>
<h4 id="tldr_11">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†DermaVQA-DASæ•°æ®é›†å’Œçš®è‚¤ç—…è¯„ä¼°æ¨¡å¼(DAS)ï¼Œæ‰©å±•äº†ç°æœ‰çš®è‚¤ç—…å›¾åƒåˆ†æåŸºå‡†ï¼Œé€šè¿‡æ”¯æŒå°é—­å¼é—®ç­”å’Œçš®è‚¤ç—…å˜åˆ†å‰²ä»»åŠ¡ï¼Œä¸ºä»¥æ‚£è€…ä¸ºä¸­å¿ƒçš„çš®è‚¤ç—…è§†è§‰è¯­è¨€å»ºæ¨¡ç ”ç©¶æä¾›äº†æ ‡å‡†åŒ–è¯„ä¼°æ¡†æ¶ã€‚</p>
<hr />
<h4 id="detailed-summary_11">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰çš®è‚¤ç—…å›¾åƒåˆ†æåŸºå‡†ä¸»è¦å…³æ³¨çš®è‚¤é•œå›¾åƒï¼Œç¼ºä¹æ‚£è€…è‡ªè¿°çš„ä¸´åºŠæŸ¥è¯¢å’Œä¸´åºŠä¸Šä¸‹æ–‡ï¼Œé™åˆ¶äº†å…¶åœ¨ä»¥æ‚£è€…ä¸ºä¸­å¿ƒçš„åŒ»ç–—æŠ¤ç†ä¸­çš„åº”ç”¨ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæä¾›æ”¯æŒæ‚£è€…ä¸­å¿ƒåŒ–æŠ¤ç†çš„æ ‡å‡†åŒ–è¯„ä¼°æ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶å¼•å…¥äº†çš®è‚¤ç—…è¯„ä¼°æ¨¡å¼(DAS)ï¼Œè¿™æ˜¯ä¸€ä¸ªç”±ä¸“å®¶å¼€å‘çš„ç³»ç»Ÿæ€§æ¡†æ¶ï¼ŒåŒ…å«36ä¸ªé«˜çº§å’Œ27ä¸ªç»†ç²’åº¦è¯„ä¼°é—®é¢˜ï¼Œæä¾›ä¸­è‹±æ–‡å¤šé€‰é¡¹ã€‚åŸºäºDASæ„å»ºäº†DermaVQA-DASæ•°æ®é›†ï¼Œæ”¯æŒå°é—­å¼é—®ç­”å’Œçš®è‚¤ç—…å˜åˆ†å‰²ä¸¤ä¸ªäº’è¡¥ä»»åŠ¡ï¼Œå¹¶è¯„ä¼°äº†å¤šç§æœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ¨¡å‹å’Œæç¤ºç­–ç•¥ã€‚</p>
<p><strong>Result:</strong> åœ¨åˆ†å‰²ä»»åŠ¡ä¸­ï¼Œæç¤ºè®¾è®¡æ˜¾è‘—å½±å“æ€§èƒ½ï¼šé»˜è®¤æç¤ºåœ¨Mean-of-Maxå’ŒMean-of-Meanè¯„ä¼°æ–¹æ¡ˆä¸‹è¡¨ç°æœ€ä½³ï¼Œè€Œç»“åˆæ‚£è€…æŸ¥è¯¢æ ‡é¢˜å’Œå†…å®¹çš„å¢å¼ºæç¤ºåœ¨å¤šæ•°æŠ•ç¥¨å¾®è¯„åˆ†è¯„ä¼°ä¸‹è¾¾åˆ°æœ€é«˜æ€§èƒ½ï¼Œä½¿ç”¨BiomedParseè·å¾—JaccardæŒ‡æ•°0.395å’ŒDiceåˆ†æ•°0.566ã€‚åœ¨å°é—­å¼é—®ç­”ä¸­ï¼Œo3æ¨¡å‹è·å¾—æœ€ä½³æ€»ä½“å‡†ç¡®ç‡0.798ï¼ŒGPT-4.1ä»¥0.796ç´§éšå…¶åï¼ŒGemini-1.5-Proåœ¨Geminiç³»åˆ—ä¸­è¡¨ç°ç«äº‰æ€§(0.783)ã€‚</p>
<p><strong>Conclusion:</strong> DermaVQA-DASå’ŒDASæ¡†æ¶ä¸ºä»¥æ‚£è€…ä¸ºä¸­å¿ƒçš„çš®è‚¤ç—…è§†è§‰è¯­è¨€å»ºæ¨¡ç ”ç©¶æä¾›äº†æ ‡å‡†åŒ–è¯„ä¼°åŸºç¡€ï¼Œæ­ç¤ºäº†æç¤ºè®¾è®¡å¯¹åˆ†å‰²æ€§èƒ½çš„é‡è¦å½±å“ï¼Œå¹¶å±•ç¤ºäº†å½“å‰å¤šæ¨¡æ€æ¨¡å‹åœ¨çš®è‚¤ç—…é—®ç­”ä»»åŠ¡ä¸­çš„å¼ºå¥è¡¨ç°ã€‚å…¬å¼€çš„æ•°æ®é›†å’Œè¯„ä¼°åè®®å°†åŠ é€Ÿè¯¥é¢†åŸŸæœªæ¥ç ”ç©¶çš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_11">ğŸ“„ Abstract</h4>
<p>Recent advances in dermatological image analysis have been driven by large-scale annotated datasets; however, most existing benchmarks focus on dermatoscopic images and lack patient-authored queries and clinical context, limiting their applicability to patient-centered care. To address this gap, we introduce DermaVQA-DAS, an extension of the DermaVQA dataset that supports two complementary tasks: closed-ended question answering (QA) and dermatological lesion segmentation. Central to this work is the Dermatology Assessment Schema (DAS), a novel expert-developed framework that systematically captures clinically meaningful dermatological features in a structured and standardized form. DAS comprises 36 high-level and 27 fine-grained assessment questions, with multiple-choice options in English and Chinese. Leveraging DAS, we provide expert-annotated datasets for both closed QA and segmentation and benchmark state-of-the-art multimodal models. For segmentation, we evaluate multiple prompting strategies and show that prompt design impacts performance: the default prompt achieves the best results under Mean-of-Max and Mean-of-Mean evaluation aggregation schemes, while an augmented prompt incorporating both patient query title and content yields the highest performance under majority-vote-based microscore evaluation, achieving a Jaccard index of 0.395 and a Dice score of 0.566 with BiomedParse. For closed-ended QA, overall performance is strong across models, with average accuracies ranging from 0.729 to 0.798; o3 achieves the best overall accuracy (0.798), closely followed by GPT-4.1 (0.796), while Gemini-1.5-Pro shows competitive performance within the Gemini family (0.783). We publicly release DermaVQA-DAS, the DAS schema, and evaluation protocols to support and accelerate future research in patient-centered dermatological vision-language modeling (https://osf.io/72rp3).</p>
<h3 id="13-taming-preference-mode-collapse-via-directional-decoupling-alignment-in-diffusion-reinforcement-learning">[13] <a href="https://arxiv.org/abs/2512.24146">Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning</a></h3>
<p><em>Chubin Chen, Sujie Hu, Jiashu Zhu, Meiqi Wu, Jintao Chen, Yanxun Li, Nisha Huang, Chengyu Fang, Jiahong Wu, Xiangxiang Chu, Xiu Li</em></p>
<h4 id="tldr_12">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ–¹å‘è§£è€¦å¯¹é½ï¼ˆDÂ²-Alignï¼‰çš„æ–°æ¡†æ¶ï¼Œç”¨äºç¼“è§£æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨äººç±»åå¥½å¯¹é½ä¸­å‡ºç°çš„åå¥½æ¨¡å¼å´©æºƒé—®é¢˜ï¼Œè¯¥æ–¹æ³•é€šè¿‡æ–¹å‘æ€§ä¿®æ­£å¥–åŠ±ä¿¡å·æ¥ç»´æŒç”Ÿæˆå¤šæ ·æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_12">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¯¹é½æ–¹æ³•è™½ç„¶èƒ½åœ¨è‡ªåŠ¨åŒ–å¥–åŠ±æŒ‡æ ‡ä¸Šè·å¾—é«˜åˆ†ï¼Œä½†ç»å¸¸å¯¼è‡´åå¥½æ¨¡å¼å´©æºƒâ€”â€”ä¸€ç§ç‰¹å®šçš„å¥–åŠ±æ”»å‡»å½¢å¼ï¼Œæ¨¡å‹æ”¶æ•›äºç‹­çª„çš„é«˜åˆ†è¾“å‡ºæ¨¡å¼ï¼Œä¸¥é‡æŸå®³äº†ç”Ÿæˆå¤šæ ·æ€§ï¼Œæœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€å…³é”®é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡é¦–å…ˆæå‡ºå¹¶é‡åŒ–äº†åå¥½æ¨¡å¼å´©æºƒç°è±¡ï¼Œå¹¶è®¾è®¡äº†DivGenBenchåŸºå‡†æ¥è¡¡é‡å…¶ç¨‹åº¦ï¼›åŸºäºåˆ†ææå‡ºäº†æ–¹å‘è§£è€¦å¯¹é½æ¡†æ¶ï¼Œè¯¥æ–¹æ³•åœ¨å†»ç»“æ¨¡å‹çš„æƒ…å†µä¸‹å­¦ä¹ å¥–åŠ±æ¨¡å‹åµŒå…¥ç©ºé—´ä¸­çš„æ–¹å‘æ€§ä¿®æ­£ï¼Œç„¶ååœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­å°†æ­¤ä¿®æ­£åº”ç”¨äºå¥–åŠ±ä¿¡å·ï¼Œé˜²æ­¢æ¨¡å‹å¡Œç¼©åˆ°ç‰¹å®šæ¨¡å¼ã€‚</p>
<p><strong>Result:</strong> ç»¼åˆè¯„ä¼°ç»“åˆäº†è´¨é‡ä¸å¤šæ ·æ€§çš„å®šæ€§å’Œå®šé‡æŒ‡æ ‡ï¼Œç»“æœæ˜¾ç¤ºDÂ²-Alignåœ¨ç»´æŒç”Ÿæˆå¤šæ ·æ€§çš„åŒæ—¶å®ç°äº†ä¸äººç±»åå¥½çš„æ›´å¥½å¯¹é½ï¼Œæœ‰æ•ˆç¼“è§£äº†åå¥½æ¨¡å¼å´©æºƒé—®é¢˜ï¼Œåœ¨DivGenBenchåŸºå‡†ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜åå¥½æ¨¡å¼å´©æºƒæ˜¯ç”±å¥–åŠ±æ¨¡å‹å›ºæœ‰åè§çš„è¿‡åº¦ä¼˜åŒ–é©±åŠ¨çš„ï¼Œæ–¹å‘è§£è€¦å¯¹é½é€šè¿‡æ–¹å‘æ€§ä¿®æ­£å¥–åŠ±ä¿¡å·æœ‰æ•ˆè§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œä¸ºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¯¹é½æä¾›äº†ä¿æŒå¤šæ ·æ€§çš„æ–°æ–¹æ³•ï¼Œå¯¹ç”Ÿæˆæ¨¡å‹çš„åå¥½å¯¹é½ç ”ç©¶å…·æœ‰é‡è¦å¯ç¤ºã€‚</p>
<hr />
<h4 id="abstract_12">ğŸ“„ Abstract</h4>
<p>Recent studies have demonstrated significant progress in aligning text-to-image diffusion models with human preference via Reinforcement Learning from Human Feedback. However, while existing methods achieve high scores on automated reward metrics, they often lead to Preference Mode Collapse (PMC)-a specific form of reward hacking where models converge on narrow, high-scoring outputs (e.g., images with monolithic styles or pervasive overexposure), severely degrading generative diversity. In this work, we introduce and quantify this phenomenon, proposing DivGenBench, a novel benchmark designed to measure the extent of PMC. We posit that this collapse is driven by over-optimization along the reward model's inherent biases. Building on this analysis, we propose Directional Decoupling Alignment (D$^2$-Align), a novel framework that mitigates PMC by directionally correcting the reward signal. Specifically, our method first learns a directional correction within the reward model's embedding space while keeping the model frozen. This correction is then applied to the reward signal during the optimization process, preventing the model from collapsing into specific modes and thereby maintaining diversity. Our comprehensive evaluation, combining qualitative analysis with quantitative metrics for both quality and diversity, reveals that D$^2$-Align achieves superior alignment with human preference.</p>
<h3 id="14-cpj-explainable-agricultural-pest-diagnosis-via-caption-prompt-judge-with-llm-judged-refinement">[14] <a href="https://arxiv.org/abs/2512.24947">CPJ: Explainable Agricultural Pest Diagnosis via Caption-Prompt-Judge with LLM-Judged Refinement</a></h3>
<p><em>Wentao Zhang, Tao Fang, Lina Lu, Lifei Wang, Weihe Zhong</em></p>
<h4 id="tldr_13">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºCaption-Prompt-Judge (CPJ)æ¡†æ¶ï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„å°æ ·æœ¬æ–¹æ³•ï¼Œé€šè¿‡ç»“æ„åŒ–å¯è§£é‡Šçš„å›¾åƒæè¿°å¢å¼ºå†œä¸šç—…è™«å®³è§†è§‰é—®ç­”æ€§èƒ½ï¼Œæ˜¾è‘—æå‡è·¨åŸŸè¯Šæ–­çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_13">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å†œä¸šç—…è™«å®³è¯Šæ–­æ–¹æ³•é€šå¸¸ä¾èµ–æ˜‚è´µçš„ç›‘ç£å¾®è°ƒï¼Œä¸”åœ¨é¢†åŸŸåç§»ä¸‹è¡¨ç°ä¸ä½³ï¼ŒåŒæ—¶ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œè¿™é™åˆ¶äº†å®é™…å†œä¸šå†³ç­–ä¸­çš„åº”ç”¨ã€‚</p>
<p><strong>Method:</strong> CPJæ¡†æ¶é‡‡ç”¨æ— éœ€è®­ç»ƒçš„å°æ ·æœ¬æ–¹æ³•ï¼Œé¦–å…ˆåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šè§’åº¦å›¾åƒæè¿°ï¼Œç„¶åé€šè¿‡LLM-as-Judgeæ¨¡å—è¿­ä»£ä¼˜åŒ–æè¿°ï¼Œæœ€ååŸºäºä¼˜åŒ–åçš„æè¿°è¿›è¡ŒåŒç­”æ¡ˆè§†è§‰é—®ç­”ï¼ŒåŒæ—¶è¾“å‡ºè¯†åˆ«å’Œç®¡ç†å»ºè®®ã€‚</p>
<p><strong>Result:</strong> åœ¨CDDMBenchåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCPJæ¡†æ¶æ˜¾è‘—æå‡æ€§èƒ½ï¼šä½¿ç”¨GPT-5-miniç”Ÿæˆçš„æè¿°ï¼ŒGPT-5-Nanoåœ¨ç—…å®³åˆ†ç±»ä»»åŠ¡ä¸Šç›¸å¯¹æ— æè¿°åŸºçº¿æå‡22.7ä¸ªç™¾åˆ†ç‚¹ï¼Œåœ¨é—®ç­”å¾—åˆ†ä¸Šæå‡19.5åˆ†ï¼ŒåŒæ—¶æä¾›é€æ˜ã€åŸºäºè¯æ®çš„æ¨ç†è¿‡ç¨‹ã€‚</p>
<p><strong>Conclusion:</strong> CPJæ¡†æ¶å±•ç¤ºäº†æ— éœ€å¾®è°ƒå³å¯å®ç°é²æ£’ã€å¯è§£é‡Šå†œä¸šè¯Šæ–­çš„å¯è¡Œæ€§ï¼Œé€šè¿‡ç»“æ„åŒ–æè¿°å’Œè¿­ä»£ä¼˜åŒ–æœºåˆ¶æé«˜äº†è·¨åŸŸé€‚åº”æ€§ï¼Œä¸ºå†œä¸šå†³ç­–æ”¯æŒç³»ç»Ÿæä¾›äº†é€æ˜ã€è¯æ®é©±åŠ¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_13">ğŸ“„ Abstract</h4>
<p>Accurate and interpretable crop disease diagnosis is essential for agricultural decision-making, yet existing methods often rely on costly supervised fine-tuning and perform poorly under domain shifts. We propose Caption--Prompt--Judge (CPJ), a training-free few-shot framework that enhances Agri-Pest VQA through structured, interpretable image captions. CPJ employs large vision-language models to generate multi-angle captions, refined iteratively via an LLM-as-Judge module, which then inform a dual-answer VQA process for both recognition and management responses. Evaluated on CDDMBench, CPJ significantly improves performance: using GPT-5-mini captions, GPT-5-Nano achieves \textbf{+22.7} pp in disease classification and \textbf{+19.5} points in QA score over no-caption baselines. The framework provides transparent, evidence-based reasoning, advancing robust and explainable agricultural diagnosis without fine-tuning. Our code and data are publicly available at: https://github.com/CPJ-Agricultural/CPJ-Agricultural-Diagnosis.</p>
<h3 id="15-taming-hallucinations-boosting-mllms-video-understanding-via-counterfactual-video-generation">[15] <a href="https://arxiv.org/abs/2512.24271">Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation</a></h3>
<p><em>Zhe Huang, Hao Wen, Aiming Hao, Bingze Song, Meiqi Wu, Jiahong Wu, Xiangxiang Chu, Sheng Lu, Haoqian Wang</em></p>
<h4 id="tldr_14">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†DualityForgeæ¡†æ¶ï¼Œé€šè¿‡å¯æ§æ‰©æ•£è§†é¢‘ç¼–è¾‘åˆæˆåäº‹å®è§†é¢‘æ•°æ®ï¼Œå¹¶å¼€å‘DNA-Trainè®­ç»ƒæ–¹æ³•ï¼Œæ˜¾è‘—å‡å°‘äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ç†è§£ä¸­çš„å¹»è§‰é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_14">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ç†è§£ä¸­å­˜åœ¨ä¸¥é‡ä¾èµ–è¯­è¨€å…ˆéªŒçš„é—®é¢˜ï¼Œå¯¼è‡´åœ¨å¤„ç†è¿èƒŒå¸¸è¯†çš„åäº‹å®è§†é¢‘æ—¶äº§ç”Ÿè§†è§‰æœªæ¥åœ°å¹»è§‰ï¼Œè€Œæ”¶é›†å’Œæ ‡æ³¨åäº‹å®æ•°æ®æˆæœ¬é«˜æ˜‚ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆè§£å†³è¿™ä¸€æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†DualityForgeåäº‹å®æ•°æ®åˆæˆæ¡†æ¶ï¼Œåˆ©ç”¨å¯æ§æ‰©æ•£è§†é¢‘ç¼–è¾‘å°†çœŸå®è§†é¢‘è½¬æ¢ä¸ºåäº‹å®åœºæ™¯ï¼Œå¹¶åµŒå…¥ç»“æ„åŒ–ä¸Šä¸‹æ–‡ä¿¡æ¯è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡QAå¯¹ï¼›å¼€å‘äº†DualityVidQAå¤§è§„æ¨¡è§†é¢‘æ•°æ®é›†ï¼Œå¹¶æå‡ºDuality-Normalized Advantage Trainingä¸¤é˜¶æ®µè®­ç»ƒæœºåˆ¶ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ é˜¶æ®µåº”ç”¨æˆå¯¹â„“1ä¼˜åŠ¿å½’ä¸€åŒ–ä»¥å®ç°æ›´ç¨³å®šçš„ç­–ç•¥ä¼˜åŒ–ã€‚</p>
<p><strong>Result:</strong> åœ¨DualityVidQA-Testæµ‹è¯•é›†ä¸Šï¼Œè¯¥æ–¹æ³•å°†æ¨¡å‹åœ¨åäº‹å®è§†é¢‘ä¸Šçš„å¹»è§‰ç›¸å¯¹å‡å°‘äº†24.0%ï¼Œæ˜¾è‘—ä¼˜äºQwen2.5-VL-7BåŸºçº¿ï¼›åœ¨å¹»è§‰åŸºå‡†å’Œé€šç”¨åŸºå‡†ä¸Šå‡å–å¾—æ˜¾è‘—æå‡ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡åˆæˆåäº‹å®æ•°æ®å’Œå¯¹æ¯”è®­ç»ƒæœºåˆ¶æœ‰æ•ˆç¼“è§£äº†MLLMçš„å¹»è§‰é—®é¢˜ï¼Œä¸ºè§†é¢‘ç†è§£ä¸­çš„è§†è§‰æ¥åœ°æ€§æä¾›äº†ç³»ç»Ÿè§£å†³æ–¹æ¡ˆï¼Œå…¶æ•°æ®åˆæˆæ¡†æ¶å’Œè®­ç»ƒæ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_14">ğŸ“„ Abstract</h4>
<p>Multimodal Large Language Models (MLLMs) have made remarkable progress in video understanding. However, they suffer from a critical vulnerability: an over-reliance on language priors, which can lead to visual ungrounded hallucinations, especially when processing counterfactual videos that defy common sense. This limitation, stemming from the intrinsic data imbalance between text and video, is challenging to address due to the substantial cost of collecting and annotating counterfactual data. To address this, we introduce DualityForge, a novel counterfactual data synthesis framework that employs controllable, diffusion-based video editing to transform real-world videos into counterfactual scenarios. By embedding structured contextual information into the video editing and QA generation processes, the framework automatically produces high-quality QA pairs together with original-edited video pairs for contrastive training. Based on this, we build DualityVidQA, a large-scale video dataset designed to reduce MLLM hallucinations. In addition, to fully exploit the contrastive nature of our paired data, we propose Duality-Normalized Advantage Training (DNA-Train), a two-stage SFT-RL training regime where the RL phase applies pair-wise $\ell_1$ advantage normalization, thereby enabling a more stable and efficient policy optimization. Experiments on DualityVidQA-Test demonstrate that our method substantially reduces model hallucinations on counterfactual videos, yielding a relative improvement of 24.0% over the Qwen2.5-VL-7B baseline. Moreover, our approach achieves significant gains across both hallucination and general-purpose benchmarks, indicating strong generalization capability. We will open-source our dataset and code.</p>
<h3 id="16-towards-open-vocabulary-industrial-defect-understanding-with-a-large-scale-multimodal-dataset">[16] <a href="https://arxiv.org/abs/2512.24160">Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset</a></h3>
<p><em>TsaiChing Ni, ZhenQi Chen, YuanFu Yang</em></p>
<h4 id="tldr_15">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†IMDD-1Mï¼Œè¿™æ˜¯é¦–ä¸ªåŒ…å«100ä¸‡å¯¹é½å›¾åƒ-æ–‡æœ¬å¯¹çš„å¤§è§„æ¨¡å·¥ä¸šå¤šæ¨¡æ€ç¼ºé™·æ•°æ®é›†ï¼Œå¹¶åŸºäºè¯¥æ•°æ®é›†ä»å¤´è®­ç»ƒäº†ä¸€ä¸ªä¸“é—¨é’ˆå¯¹å·¥ä¸šåœºæ™¯çš„æ‰©æ•£å¼è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡è½»é‡çº§å¾®è°ƒå³å¯é«˜æ•ˆé€‚åº”ä¸“ä¸šé¢†åŸŸã€‚</p>
<hr />
<h4 id="detailed-summary_15">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å·¥ä¸šç¼ºé™·æ£€æµ‹é¢†åŸŸç¼ºä¹å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œè¿™é™åˆ¶äº†å¤šæ¨¡æ€å­¦ä¹ åœ¨åˆ¶é€ ä¸šå’Œè´¨é‡æ£€æµ‹ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯å¯¹äºéœ€è¦ç»†ç²’åº¦ç†è§£å’Œç”Ÿæˆèƒ½åŠ›çš„å·¥ä¸šåœºæ™¯ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é¦–å…ˆæ„å»ºäº†IMDD-1Mæ•°æ®é›†ï¼ŒåŒ…å«100ä¸‡å¯¹é½çš„å›¾åƒ-æ–‡æœ¬å¯¹ï¼Œæ¶µç›–60å¤šç§ææ–™ç±»åˆ«å’Œ400å¤šç§ç¼ºé™·ç±»å‹ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½é…æœ‰ä¸“å®¶éªŒè¯çš„æ³¨é‡Šå’Œç»†ç²’åº¦æ–‡æœ¬æè¿°ï¼›åŸºäºè¯¥æ•°æ®é›†ï¼Œä»å¤´è®­ç»ƒäº†ä¸€ä¸ªæ‰©æ•£å¼è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œä¸“é—¨é’ˆå¯¹å·¥ä¸šåœºæ™¯è¿›è¡Œä¼˜åŒ–ã€‚</p>
<p><strong>Result:</strong> è¯¥åŸºç¡€æ¨¡å‹åœ¨ä»…éœ€ä¸åˆ°5%ä»»åŠ¡ç‰¹å®šæ•°æ®çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡è½»é‡çº§å¾®è°ƒå³å¯è¾¾åˆ°ä¸ä¸“ç”¨ä¸“å®¶æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œæ”¯æŒåˆ†ç±»ã€åˆ†å‰²ã€æ£€ç´¢ã€æè¿°å’Œç”Ÿæˆç­‰å¤šç§åº”ç”¨ï¼Œå±•ç¤ºäº†æ•°æ®é«˜æ•ˆçš„åŸºç¡€æ¨¡å‹é€‚åº”èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¿™é¡¹ç ”ç©¶ä¸ºå·¥ä¸šæ£€æµ‹å’Œç”Ÿæˆä»»åŠ¡æä¾›äº†å¯æ‰©å±•ã€é¢†åŸŸè‡ªé€‚åº”å’ŒçŸ¥è¯†é©±åŠ¨çš„è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†åŸºç¡€æ¨¡å‹åœ¨å·¥ä¸šåœºæ™¯ä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºåˆ¶é€ ä¸šæ™ºèƒ½åŒ–å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡æ•°æ®é«˜æ•ˆé€‚åº”ç­–ç•¥é™ä½äº†ä¸“ä¸šé¢†åŸŸåº”ç”¨çš„é—¨æ§›ã€‚</p>
<hr />
<h4 id="abstract_15">ğŸ“„ Abstract</h4>
<p>We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.</p>
<h3 id="17-diffthinker-towards-generative-multimodal-reasoning-with-diffusion-models">[17] <a href="https://arxiv.org/abs/2512.24165">DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models</a></h3>
<p><em>Zefeng He, Xiaoye Qu, Yafu Li, Tong Zhu, Siyuan Huang, Yu Cheng</em></p>
<h4 id="tldr_16">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºDiffThinkerï¼Œä¸€ç§åŸºäºæ‰©æ•£çš„ç”Ÿæˆå¼å¤šæ¨¡æ€æ¨ç†æ¡†æ¶ï¼Œå°†å¤šæ¨¡æ€æ¨ç†é‡æ–°å®šä¹‰ä¸ºåŸç”Ÿå›¾åƒåˆ°å›¾åƒçš„ç”Ÿæˆä»»åŠ¡ï¼Œåœ¨è§†è§‰ä¸­å¿ƒä»»åŠ¡ä¸­å®ç°äº†å“è¶Šçš„é€»è¾‘ä¸€è‡´æ€§å’Œç©ºé—´ç²¾åº¦ã€‚</p>
<hr />
<h4 id="detailed-summary_16">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä»ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒï¼Œå¯¼è‡´åœ¨å¤æ‚é•¿è§†é‡çš„è§†è§‰ä¸­å¿ƒä»»åŠ¡ä¸­è¡¨ç°æ¬ ä½³ï¼Œéœ€è¦ä¸€ç§æ›´æœ‰æ•ˆçš„å¤šæ¨¡æ€æ¨ç†èŒƒå¼æ¥è§£å†³è¿™ä¸€å±€é™æ€§ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡å»ºç«‹äº†ç”Ÿæˆå¼å¤šæ¨¡æ€æ¨ç†æ–°èŒƒå¼ï¼Œå¼•å…¥DiffThinkerè¿™ä¸€åŸºäºæ‰©æ•£çš„æ¨ç†æ¡†æ¶ï¼Œå°†å¤šæ¨¡æ€æ¨ç†é‡æ–°å®šä¹‰ä¸ºåŸç”Ÿå›¾åƒåˆ°å›¾åƒçš„ç”Ÿæˆä»»åŠ¡ï¼Œå¹¶ç³»ç»Ÿæ¯”è¾ƒäº†è¯¥èŒƒå¼ä¸MLLMsçš„å†…åœ¨ç‰¹æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨å››ä¸ªé¢†åŸŸï¼ˆé¡ºåºè§„åˆ’ã€ç»„åˆä¼˜åŒ–ã€çº¦æŸæ»¡è¶³å’Œç©ºé—´é…ç½®ï¼‰çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDiffThinkeræ˜¾è‘—ä¼˜äºé¢†å…ˆçš„é—­æºæ¨¡å‹ï¼ŒåŒ…æ‹¬GPT-5ï¼ˆ+314.2%ï¼‰ã€Gemini-3-Flashï¼ˆ+111.6%ï¼‰ä»¥åŠå¾®è°ƒçš„Qwen3-VL-32BåŸºçº¿ï¼ˆ+39.0%ï¼‰ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†ç”Ÿæˆå¼å¤šæ¨¡æ€æ¨ç†èŒƒå¼çš„å››ä¸ªæ ¸å¿ƒç‰¹æ€§ï¼šæ•ˆç‡ã€å¯æ§æ€§ã€åŸç”Ÿå¹¶è¡Œæ€§å’Œåä½œæ€§ï¼Œè¡¨æ˜ç”Ÿæˆå¼å¤šæ¨¡æ€æ¨ç†æ˜¯è§†è§‰ä¸­å¿ƒæ¨ç†çš„æœ‰å‰æ™¯æ–¹æ³•ï¼Œä¸ºå¤šæ¨¡æ€æ¨ç†ç ”ç©¶å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_16">ğŸ“„ Abstract</h4>
<p>While recent Multimodal Large Language Models (MLLMs) have attained significant strides in multimodal reasoning, their reasoning processes remain predominantly text-centric, leading to suboptimal performance in complex long-horizon, vision-centric tasks. In this paper, we establish a novel Generative Multimodal Reasoning paradigm and introduce DiffThinker, a diffusion-based reasoning framework. Conceptually, DiffThinker reformulates multimodal reasoning as a native generative image-to-image task, achieving superior logical consistency and spatial precision in vision-centric tasks. We perform a systematic comparison between DiffThinker and MLLMs, providing the first in-depth investigation into the intrinsic characteristics of this paradigm, revealing four core properties: efficiency, controllability, native parallelism, and collaboration. Extensive experiments across four domains (sequential planning, combinatorial optimization, constraint satisfaction, and spatial configuration) demonstrate that DiffThinker significantly outperforms leading closed source models including GPT-5 (+314.2\%) and Gemini-3-Flash (+111.6\%), as well as the fine-tuned Qwen3-VL-32B baseline (+39.0\%), highlighting generative multimodal reasoning as a promising approach for vision-centric reasoning.</p>
<h3 id="18-corgi-contribution-guided-block-wise-interval-caching-for-training-free-acceleration-of-diffusion-transformers">[18] <a href="https://arxiv.org/abs/2512.24195">CorGi: Contribution-Guided Block-Wise Interval Caching for Training-Free Acceleration of Diffusion Transformers</a></h3>
<p><em>Yonglak Son, Suhyeok Kim, Seungryong Kim, Young Geun Kim</em></p>
<h4 id="tldr_17">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†CorGiå’ŒCorGi+ï¼Œä¸€ç§æ— éœ€è®­ç»ƒå³å¯åŠ é€Ÿæ‰©æ•£Transformeræ¨ç†çš„æ¡†æ¶ï¼Œé€šè¿‡é€‰æ‹©æ€§é‡ç”¨Transformerå—è¾“å‡ºæ¥å‡å°‘å»å™ªæ­¥éª¤é—´çš„å†—ä½™è®¡ç®—ï¼Œåœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶å®ç°é«˜è¾¾2.0å€çš„åŠ é€Ÿã€‚</p>
<hr />
<h4 id="detailed-summary_17">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ‰©æ•£Transformeråœ¨è§†è§‰ç”Ÿæˆä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶è¿­ä»£å»å™ªè¿‡ç¨‹ç»“åˆå¤§æ¨¡å‹å®¹é‡å¯¼è‡´é«˜æ¨ç†æˆæœ¬ï¼Œç°æœ‰ç ”ç©¶è¡¨æ˜DiTæ¨¡å‹çš„è¿­ä»£å»å™ªè¿‡ç¨‹åœ¨æ­¥éª¤é—´å­˜åœ¨å¤§é‡å†—ä½™è®¡ç®—ï¼Œéœ€è¦æœ‰æ•ˆå‡å°‘è¿™äº›å†—ä½™è®¡ç®—ä»¥æé«˜æ¨ç†æ•ˆç‡ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†CorGiæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„DiTæ¨ç†åŠ é€Ÿæ–¹æ³•ï¼Œé€šè¿‡è´¡çŒ®åº¦å¼•å¯¼çš„å—çº§é—´éš”ç¼“å­˜æœºåˆ¶ï¼Œé€‰æ‹©æ€§é‡ç”¨DiTä¸­Transformerå—çš„è¾“å‡ºï¼›å¯¹äºæ–‡æœ¬åˆ°å›¾åƒä»»åŠ¡ï¼Œè¿›ä¸€æ­¥æå‡ºCorGi+ï¼Œåˆ©ç”¨æ¯å—äº¤å‰æ³¨æ„åŠ›å›¾è¯†åˆ«æ˜¾è‘—æ ‡è®°å¹¶åº”ç”¨éƒ¨åˆ†æ³¨æ„åŠ›æ›´æ–°ä»¥ä¿æŠ¤é‡è¦å¯¹è±¡ç»†èŠ‚ã€‚</p>
<p><strong>Result:</strong> åœ¨æœ€å…ˆè¿›çš„DiTæ¨¡å‹ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒCorGiå’ŒCorGi+å¹³å‡å®ç°äº†é«˜è¾¾2.0å€çš„åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒäº†é«˜ç”Ÿæˆè´¨é‡ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨å‡å°‘å†—ä½™è®¡ç®—æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†é€šè¿‡é€‰æ‹©æ€§ç¼“å­˜å’Œé‡ç”¨ä½è´¡çŒ®åº¦Transformerå—å¯ä»¥æ˜¾è‘—åŠ é€ŸDiTæ¨ç†è€Œä¸æŸå®³ç”Ÿæˆè´¨é‡ï¼Œä¸ºæ‰©æ•£æ¨¡å‹çš„æ•ˆç‡ä¼˜åŒ–æä¾›äº†æ–°æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼Œç»“åˆæ³¨æ„åŠ›æœºåˆ¶çš„ä¿æŠ¤ç­–ç•¥è¿›ä¸€æ­¥æå‡äº†ç»†èŠ‚ä¿ç•™èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="abstract_17">ğŸ“„ Abstract</h4>
<p>Diffusion transformer (DiT) achieves remarkable performance in visual generation, but its iterative denoising process combined with larger capacity leads to a high inference cost. Recent works have demonstrated that the iterative denoising process of DiT models involves substantial redundant computation across steps. To effectively reduce the redundant computation in DiT, we propose CorGi (Contribution-Guided Block-Wise Interval Caching), training-free DiT inference acceleration framework that selectively reuses the outputs of transformer blocks in DiT across denoising steps. CorGi caches low-contribution blocks and reuses them in later steps within each interval to reduce redundant computation while preserving generation quality. For text-to-image tasks, we further propose CorGi+, which leverages per-block cross-attention maps to identify salient tokens and applies partial attention updates to protect important object details. Evaluation on the state-of-the-art DiT models demonstrates that CorGi and CorGi+ achieve up to 2.0x speedup on average, while preserving high generation quality.</p>
<h3 id="19-arm-a-learnable-plug-and-play-module-for-clip-based-open-vocabulary-semantic-segmentation">[19] <a href="https://arxiv.org/abs/2512.24224">ARM: A Learnable, Plug-and-Play Module for CLIP-based Open-vocabulary Semantic Segmentation</a></h3>
<p><em>Ziquan Liu, Zhewei Zhu, Xuyang Shi</em></p>
<h4 id="tldr_18">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºæ³¨æ„åŠ›ç²¾ç‚¼æ¨¡å—ï¼ˆARMï¼‰ï¼Œä¸€ç§è½»é‡çº§å¯å­¦ä¹ æ¨¡å—ï¼Œé€šè¿‡è‡ªé€‚åº”èåˆCLIPçš„å±‚æ¬¡ç‰¹å¾æ¥æå‡å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ€§èƒ½ï¼Œå®ç°"ä¸€æ¬¡è®­ç»ƒï¼Œéšå¤„ä½¿ç”¨"çš„èŒƒå¼ï¼Œæ˜¾è‘—æå‡å¤šç§å…è®­ç»ƒåŸºçº¿æ–¹æ³•çš„æ•ˆæœã€‚</p>
<hr />
<h4 id="detailed-summary_18">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰é¢ä¸´CLIPå›¾åƒçº§è¡¨ç¤ºç¼ºä¹åƒç´ çº§ç»†èŠ‚çš„æ ¹æœ¬é™åˆ¶ï¼Œç°æœ‰å…è®­ç»ƒæ–¹æ³•è¦ä¹ˆä¾èµ–æ˜‚è´µçš„å¤–éƒ¨åŸºç¡€æ¨¡å‹ï¼ˆå¦‚SAMã€DINOï¼‰ï¼Œè¦ä¹ˆå¯¹CLIPå†…éƒ¨ç‰¹å¾åº”ç”¨é™æ€å¯å‘å¼æ–¹æ³•ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜æˆ–æ•ˆæœæ¬¡ä¼˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºæ³¨æ„åŠ›ç²¾ç‚¼æ¨¡å—ï¼ˆARMï¼‰ï¼ŒåŒ…å«è¯­ä¹‰å¼•å¯¼çš„äº¤å‰æ³¨æ„åŠ›å—å’Œè‡ªæ³¨æ„åŠ›å—ï¼Œä½¿ç”¨é²æ£’çš„æ·±å±‚ç‰¹å¾ï¼ˆKã€Vï¼‰é€‰æ‹©å’Œç²¾ç‚¼ç»†èŠ‚ä¸°å¯Œçš„æµ…å±‚ç‰¹å¾ï¼ˆQï¼‰ï¼Œå®ç°è‡ªé€‚åº”å±‚æ¬¡ç‰¹å¾èåˆï¼Œé‡‡ç”¨"ä¸€æ¬¡è®­ç»ƒï¼Œéšå¤„ä½¿ç”¨"èŒƒå¼ï¼Œåœ¨é€šç”¨æ•°æ®é›†ï¼ˆå¦‚COCO-Stuffï¼‰è®­ç»ƒåå¯ä½œä¸ºé€šç”¨å³æ’å³ç”¨åå¤„ç†å™¨ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ARMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æŒç»­æå‡åŸºçº¿æ€§èƒ½ï¼Œæ¨ç†å¼€é”€å¯å¿½ç•¥ä¸è®¡ï¼Œä¸ºå…è®­ç»ƒOVSSå»ºç«‹äº†é«˜æ•ˆæœ‰æ•ˆçš„èŒƒå¼ï¼Œæ˜¾è‘—ä¼˜äºé™æ€èåˆæ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> ARMé€šè¿‡è§£é”å’Œç²¾ç‚¼CLIPå†…éƒ¨æ½œåŠ›ï¼Œè§£å†³äº†ç°æœ‰å…è®­ç»ƒæ–¹æ³•çš„è®¡ç®—æˆæœ¬ä¸æ•ˆæœæƒè¡¡é—®é¢˜ï¼Œå…¶é€šç”¨å³æ’å³ç”¨ç‰¹æ€§ä¸ºå¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†è‡ªé€‚åº”ç‰¹å¾èåˆç›¸å¯¹äºé™æ€æ–¹æ³•çš„ä¼˜åŠ¿ã€‚</p>
<hr />
<h4 id="abstract_18">ğŸ“„ Abstract</h4>
<p>Open-vocabulary semantic segmentation (OVSS) is fundamentally hampered by the coarse, image-level representations of CLIP, which lack precise pixel-level details. Existing training-free methods attempt to resolve this by either importing priors from costly external foundation models (e.g., SAM, DINO) or by applying static, hand-crafted heuristics to CLIP's internal features. These approaches are either computationally expensive or sub-optimal. We propose the Attention Refinement Module (ARM), a lightweight, learnable module that effectively unlocks and refines CLIP's internal potential. Unlike static-fusion methods, ARM learns to adaptively fuse hierarchical features. It employs a semantically-guided cross-attention block, using robust deep features (K, V) to select and refine detail-rich shallow features (Q), followed by a self-attention block. The key innovation lies in a ``train once, use anywhere" paradigm. Trained once on a general-purpose dataset (e.g., COCO-Stuff), ARM acts as a universal plug-and-play post-processor for diverse training-free frameworks. Extensive experiments show that ARM consistently boosts baseline performance on multiple benchmarks with negligible inference overhead, establishing an efficient and effective paradigm for training-free OVSS.</p>
<h3 id="20-evolving-not-training-zero-shot-reasoning-segmentation-via-evolutionary-prompting">[20] <a href="https://arxiv.org/abs/2512.24702">Evolving, Not Training: Zero-Shot Reasoning Segmentation via Evolutionary Prompting</a></h3>
<p><em>Kai Ye, Xiaotong You, Jianghang Lin, Jiayi Ji, Pingyang Dai, Liujuan Cao</em></p>
<h4 id="tldr_19">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºEVOL-SAM3ï¼Œä¸€ç§æ–°é¢–çš„é›¶æ ·æœ¬æ¨ç†åˆ†å‰²æ¡†æ¶ï¼Œå°†ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºæ¨ç†æ—¶è¿›åŒ–æœç´¢è¿‡ç¨‹ï¼Œé€šè¿‡"ç”Ÿæˆ-è¯„ä¼°-è¿›åŒ–"å¾ªç¯è¿­ä»£ä¼˜åŒ–æç¤ºå‡è®¾ï¼Œåœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹æ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_19">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æ¨ç†åˆ†å‰²æ–¹æ³•å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼šç›‘ç£å¾®è°ƒæ–¹æ³•é­å—ç¾éš¾æ€§é—å¿˜å’Œé¢†åŸŸä¾èµ–é—®é¢˜ï¼Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•é¢ä¸´è®­ç»ƒä¸ç¨³å®šæ€§å’Œå¯¹é¢„å®šä¹‰å¥–åŠ±å‡½æ•°çš„åˆšæ€§ä¾èµ–ï¼Œè€Œå…è®­ç»ƒæ–¹æ³•è™½ç„¶é¿å…äº†è®­ç»ƒè´Ÿæ‹…ï¼Œä½†å…¶é™æ€æ¨ç†èŒƒå¼å­˜åœ¨æ¨ç†æ·±åº¦ä¸è¶³ã€ç¼ºä¹è‡ªæˆ‘çº æ­£è¯­è¨€å¹»è§‰æˆ–ç©ºé—´è¯¯è§£èƒ½åŠ›çš„é—®é¢˜ï¼Œé€šå¸¸ä¾èµ–äºå•æ¬¡"ç”Ÿæˆ-ç„¶å-åˆ†å‰²"é“¾å¼æµç¨‹ã€‚</p>
<p><strong>Method:</strong> EVOL-SAM3å°†æ¨ç†åˆ†å‰²é‡æ–°å®šä¹‰ä¸ºæ¨ç†æ—¶è¿›åŒ–æœç´¢è¿‡ç¨‹ï¼Œé‡‡ç”¨"ç”Ÿæˆ-è¯„ä¼°-è¿›åŒ–"å¾ªç¯è¿­ä»£ä¼˜åŒ–æç¤ºå‡è®¾ã€‚æ¡†æ¶ç»´æŠ¤ä¸€ä¸ªæç¤ºå‡è®¾ç§ç¾¤ï¼Œé€šè¿‡è§†è§‰ç«æŠ€åœºè¿›è¡Œæ— å‚è€ƒæˆå¯¹ç«èµ›è¯„ä¼°æç¤ºé€‚åº”æ€§ï¼Œå¼•å…¥è¯­ä¹‰å˜å¼‚ç®—å­æ³¨å…¥å¤šæ ·æ€§å¹¶çº æ­£è¯­ä¹‰é”™è¯¯ï¼ŒåŒæ—¶é‡‡ç”¨å¼‚æ„ç«æŠ€åœºæ¨¡å—æ•´åˆå‡ ä½•å…ˆéªŒä¸è¯­ä¹‰æ¨ç†ä»¥ç¡®ä¿æœ€ç»ˆé€‰æ‹©çš„é²æ£’æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ReasonSegåŸºå‡†æµ‹è¯•ä¸­ï¼ŒEVOL-SAM3ä¸ä»…æ˜¾è‘—è¶…è¶Šé™æ€åŸºçº¿æ–¹æ³•ï¼Œè¿˜åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹å¤§å¹…è¶…è¿‡å®Œå…¨ç›‘ç£çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„æ¨ç†åˆ†å‰²èƒ½åŠ›ã€‚å¹¿æ³›çš„å®éªŒéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†å°†è¿›åŒ–æœç´¢èŒƒå¼å¼•å…¥æ¨ç†åˆ†å‰²ä»»åŠ¡çš„æ½œåŠ›ï¼Œæä¾›äº†ä¸€ç§æ— éœ€è®­ç»ƒå³å¯å®ç°é«˜æ€§èƒ½æ¨ç†åˆ†å‰²çš„åˆ›æ–°æ–¹æ³•ã€‚æ¡†æ¶çš„è¿›åŒ–æœºåˆ¶èƒ½å¤Ÿè‡ªæˆ‘çº æ­£è¯­è¨€å¹»è§‰å’Œç©ºé—´è¯¯è§£ï¼Œä¸ºå¤æ‚è§†è§‰è¯­è¨€ä»»åŠ¡æä¾›äº†æ–°çš„è§£å†³æ€è·¯ï¼Œå¹¶å¯èƒ½å¯å‘å…¶ä»–éœ€è¦æ·±åº¦æ¨ç†å’Œå¤šè½®äº¤äº’çš„è§†è§‰ä»»åŠ¡ç ”ç©¶ã€‚</p>
<hr />
<h4 id="abstract_19">ğŸ“„ Abstract</h4>
<p>Reasoning Segmentation requires models to interpret complex, context-dependent linguistic queries to achieve pixel-level localization. Current dominant approaches rely heavily on Supervised Fine-Tuning (SFT) or Reinforcement Learning (RL). However, SFT suffers from catastrophic forgetting and domain dependency, while RL is often hindered by training instability and rigid reliance on predefined reward functions. Although recent training-free methods circumvent these training burdens, they are fundamentally limited by a static inference paradigm. These methods typically rely on a single-pass "generate-then-segment" chain, which suffers from insufficient reasoning depth and lacks the capability to self-correct linguistic hallucinations or spatial misinterpretations. In this paper, we challenge these limitations and propose EVOL-SAM3, a novel zero-shot framework that reformulates reasoning segmentation as an inference-time evolutionary search process. Instead of relying on a fixed prompt, EVOL-SAM3 maintains a population of prompt hypotheses and iteratively refines them through a "Generate-Evaluate-Evolve" loop. We introduce a Visual Arena to assess prompt fitness via reference-free pairwise tournaments, and a Semantic Mutation operator to inject diversity and correct semantic errors. Furthermore, a Heterogeneous Arena module integrates geometric priors with semantic reasoning to ensure robust final selection. Extensive experiments demonstrate that EVOL-SAM3 not only substantially outperforms static baselines but also significantly surpasses fully supervised state-of-the-art methods on the challenging ReasonSeg benchmark in a zero-shot setting. The code is available at https://github.com/AHideoKuzeA/Evol-SAM3.</p>
<h3 id="21-mambaseg-harnessing-mamba-for-accurate-and-efficient-image-event-semantic-segmentation">[21] <a href="https://arxiv.org/abs/2512.24243">MambaSeg: Harnessing Mamba for Accurate and Efficient Image-Event Semantic Segmentation</a></h3>
<p><em>Fuqiang Gu, Yuanke Li, Xianlei Long, Kangping Ji, Chao Chen, Qingyi Gu, Zhenliang Ni</em></p>
<h4 id="tldr_20">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºMambaSegï¼Œä¸€ç§æ–°é¢–çš„åŒåˆ†æ”¯è¯­ä¹‰åˆ†å‰²æ¡†æ¶ï¼Œé‡‡ç”¨å¹¶è¡ŒMambaç¼–ç å™¨é«˜æ•ˆå»ºæ¨¡RGBå›¾åƒå’Œäº‹ä»¶æµï¼Œå¹¶é€šè¿‡åŒç»´äº¤äº’æ¨¡å—å®ç°æ—¶ç©ºç»´åº¦çš„ç»†ç²’åº¦èåˆï¼Œåœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶è¾¾åˆ°æœ€å…ˆè¿›çš„åˆ†å‰²æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_20">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> RGBç›¸æœºåœ¨å¿«é€Ÿè¿åŠ¨ã€ä½å…‰ç…§æˆ–é«˜åŠ¨æ€èŒƒå›´æ¡ä»¶ä¸‹æ€§èƒ½ä¸‹é™ï¼Œè€Œäº‹ä»¶ç›¸æœºè™½ç„¶å…·æœ‰é«˜æ—¶é—´åˆ†è¾¨ç‡å’Œä½å»¶è¿Ÿä¼˜åŠ¿ï¼Œä½†ç¼ºä¹é¢œè‰²å’Œçº¹ç†ä¿¡æ¯ã€‚ç°æœ‰RGBä¸äº‹ä»¶æ•°æ®èåˆæ–¹æ³•è®¡ç®—æˆæœ¬é«˜ä¸”ä¸»è¦å…³æ³¨ç©ºé—´èåˆï¼Œå¿½ç•¥äº†äº‹ä»¶æµå›ºæœ‰çš„æ—¶é—´åŠ¨æ€ç‰¹æ€§ï¼Œéœ€è¦æ›´é«˜æ•ˆã€å…¨é¢çš„è·¨æ¨¡æ€èåˆæ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> æå‡ºMambaSegåŒåˆ†æ”¯è¯­ä¹‰åˆ†å‰²æ¡†æ¶ï¼Œé‡‡ç”¨å¹¶è¡ŒMambaç¼–ç å™¨åˆ†åˆ«å¤„ç†RGBå›¾åƒå’Œäº‹ä»¶æµã€‚å¼•å…¥åŒç»´äº¤äº’æ¨¡å—ï¼ˆDDIMï¼‰ï¼ŒåŒ…å«è·¨ç©ºé—´äº¤äº’æ¨¡å—ï¼ˆCSIMï¼‰å’Œè·¨æ—¶é—´äº¤äº’æ¨¡å—ï¼ˆCTIMï¼‰ï¼Œåœ¨ç©ºé—´å’Œæ—¶é—´ç»´åº¦ä¸Šè”åˆæ‰§è¡Œç»†ç²’åº¦èåˆï¼Œæ”¹å–„è·¨æ¨¡æ€å¯¹é½å¹¶å‡å°‘æ¨¡ç³Šæ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨DDD17å’ŒDSECæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMambaSegå®ç°äº†æœ€å…ˆè¿›çš„è¯­ä¹‰åˆ†å‰²æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå±•ç¤ºäº†å…¶åœ¨é«˜æ•ˆã€å¯æ‰©å±•å’Œé²æ£’çš„å¤šæ¨¡æ€æ„ŸçŸ¥æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†Mambaæ¶æ„åœ¨é«˜æ•ˆå¤„ç†å¤šæ¨¡æ€è§†è§‰æ•°æ®æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼ŒåŒç»´äº¤äº’æœºåˆ¶èƒ½å¤Ÿå……åˆ†åˆ©ç”¨RGBå’Œäº‹ä»¶æ•°æ®çš„äº’è¡¥ç‰¹æ€§ï¼Œä¸ºå®æ—¶ã€é²æ£’çš„è¯­ä¹‰åˆ†å‰²ç³»ç»Ÿæä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨æ€å’ŒæŒ‘æˆ˜æ€§ç¯å¢ƒä¸­ã€‚</p>
<hr />
<h4 id="abstract_20">ğŸ“„ Abstract</h4>
<p>Semantic segmentation is a fundamental task in computer vision with wide-ranging applications, including autonomous driving and robotics. While RGB-based methods have achieved strong performance with CNNs and Transformers, their effectiveness degrades under fast motion, low-light, or high dynamic range conditions due to limitations of frame cameras. Event cameras offer complementary advantages such as high temporal resolution and low latency, yet lack color and texture, making them insufficient on their own. To address this, recent research has explored multimodal fusion of RGB and event data; however, many existing approaches are computationally expensive and focus primarily on spatial fusion, neglecting the temporal dynamics inherent in event streams. In this work, we propose MambaSeg, a novel dual-branch semantic segmentation framework that employs parallel Mamba encoders to efficiently model RGB images and event streams. To reduce cross-modal ambiguity, we introduce the Dual-Dimensional Interaction Module (DDIM), comprising a Cross-Spatial Interaction Module (CSIM) and a Cross-Temporal Interaction Module (CTIM), which jointly perform fine-grained fusion along both spatial and temporal dimensions. This design improves cross-modal alignment, reduces ambiguity, and leverages the complementary properties of each modality. Extensive experiments on the DDD17 and DSEC datasets demonstrate that MambaSeg achieves state-of-the-art segmentation performance while significantly reducing computational cost, showcasing its promise for efficient, scalable, and robust multimodal perception.</p>
<h3 id="22-video-and-language-alignment-in-2d-systems-for-3d-multi-object-scenes-with-multi-information-derivative-free-control">[22] <a href="https://arxiv.org/abs/2512.24826">Video and Language Alignment in 2D Systems for 3D Multi-object Scenes with Multi-Information Derivative-Free Control</a></h3>
<p><em>Jason Armitage, Rico Sennnrich</em></p>
<h4 id="tldr_21">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡åŸºäºé—æ†¾æœ€å°åŒ–çš„æ— å¯¼æ•°ä¼˜åŒ–æ¥æ”¹è¿›å¤šå…ƒäº’ä¿¡æ¯ä¼°è®¡çš„æ–°æ–¹æ³•ï¼Œä½¿ç°æˆçš„è·¨æ¨¡æ€ç³»ç»Ÿèƒ½å¤Ÿåœ¨æ— éœ€é¢„è®­ç»ƒæˆ–å¾®è°ƒçš„æƒ…å†µä¸‹é€‚åº”3Dåœºæ™¯ä¸­çš„ç‰©ä½“é®æŒ¡å’Œç‰¹å¾åŒºåˆ†ã€‚</p>
<hr />
<h4 id="detailed-summary_21">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è·¨æ¨¡æ€ç³»ç»Ÿåœ¨2Dè§†è§‰è¾“å…¥ä¸Šè®­ç»ƒåï¼Œå¤„ç†3Dåœºæ™¯æ—¶ä¼šé¢ä¸´ç»´åº¦è½¬æ¢çš„æŒ‘æˆ˜ï¼Œè€Œåœºæ™¯å†…ç›¸æœºè™½ç„¶èƒ½å¼¥åˆè¿™ä¸€ç»´åº¦å·®è·ï¼Œä½†éœ€è¦å­¦ä¹ æ§åˆ¶æ¨¡å—ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç‰©ä½“é®æŒ¡å’Œç‰¹å¾åŒºåˆ†æ–¹é¢å­˜åœ¨å±€é™ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é€šè¿‡åŸºäºé—æ†¾æœ€å°åŒ–çš„æ— å¯¼æ•°ä¼˜åŒ–æ¥æ”¹è¿›å¤šå…ƒäº’ä¿¡æ¯ä¼°è®¡ï¼Œç»“åˆè¡¨è¾¾æ€§åº¦é‡å’ŒåŸºäºä»·å€¼çš„ä¼˜åŒ–ï¼Œè¾…åŠ©æ§åˆ¶åœºæ™¯å†…ç›¸æœºç›´æ¥ä»è§†è§‰è¯­è¨€æ¨¡å‹çš„å™ªå£°è¾“å‡ºä¸­å­¦ä¹ ï¼Œä½¿ç°æˆçš„2Dè·¨æ¨¡æ€ç³»ç»Ÿèƒ½å¤Ÿåœ¨çº¿é€‚åº”3Dåœºæ™¯ã€‚</p>
<p><strong>Result:</strong> æ‰€æå‡ºçš„ç®¡é“åœ¨å¤šç‰©ä½“3Dåœºæ™¯çš„è·¨æ¨¡æ€ä»»åŠ¡ä¸­æé«˜äº†æ€§èƒ½ï¼Œæ— éœ€ä¾èµ–é¢„è®­ç»ƒæˆ–å¾®è°ƒï¼Œæœ‰æ•ˆå¤„ç†äº†ç‰©ä½“é®æŒ¡å¹¶æ”¹å–„äº†ç‰¹å¾åŒºåˆ†èƒ½åŠ›ï¼Œå±•ç¤ºäº†åœ¨å¤æ‚3Dç¯å¢ƒä¸­è·¨æ¨¡æ€ç³»ç»Ÿçš„é€‚åº”æ€§æ”¹è¿›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†é€šè¿‡æ”¹è¿›äº’ä¿¡æ¯ä¼°è®¡å’Œä¼˜åŒ–ç­–ç•¥ï¼Œå¯ä»¥ä½¿ç°æˆçš„2Dè·¨æ¨¡æ€ç³»ç»Ÿæœ‰æ•ˆé€‚åº”3Dåœºæ™¯ï¼Œä¸ºè·¨ç»´åº¦è§†è§‰å¤„ç†æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œé¿å…äº†æ˜‚è´µçš„é¢„è®­ç»ƒæˆ–å¾®è°ƒéœ€æ±‚ï¼Œå…·æœ‰å®é™…éƒ¨ç½²ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_21">ğŸ“„ Abstract</h4>
<p>Cross-modal systems trained on 2D visual inputs are presented with a dimensional shift when processing 3D scenes. An in-scene camera bridges the dimensionality gap but requires learning a control module. We introduce a new method that improves multivariate mutual information estimates by regret minimisation with derivative-free optimisation. Our algorithm enables off-the-shelf cross-modal systems trained on 2D visual inputs to adapt online to object occlusions and differentiate features. The pairing of expressive measures and value-based optimisation assists control of an in-scene camera to learn directly from the noisy outputs of vision-language models. The resulting pipeline improves performance in cross-modal tasks on multi-object 3D scenes without resorting to pretraining or finetuning.</p>
<h3 id="23-darkeqa-benchmarking-vision-language-models-for-embodied-question-answering-in-low-light-indoor-environments">[23] <a href="https://arxiv.org/abs/2512.24985">DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments</a></h3>
<p><em>Yohan Park, Hyunwoo Ha, Wonjun Jo, Tae-Hyun Oh</em></p>
<h4 id="tldr_22">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†DarkEQAåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä½å…‰ç…§æ¡ä»¶ä¸‹çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œæ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨é»‘æš—ç¯å¢ƒä¸­çš„å±€é™æ€§ï¼Œå¹¶å»ºç«‹äº†ç‰©ç†çœŸå®çš„è§†è§‰é€€åŒ–æ¨¡æ‹Ÿæ¡†æ¶ã€‚</p>
<hr />
<h4 id="detailed-summary_22">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦åœ¨ç†æƒ³å…‰ç…§æ¡ä»¶ä¸‹è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œè€Œå¿½ç•¥äº†å®é™…åº”ç”¨ä¸­å¿…éœ€çš„24/7å…¨å¤©å€™æ“ä½œéœ€æ±‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤œé—´æˆ–é»‘æš—ç¯å¢ƒä¸­çš„ä½å…‰ç…§æ¡ä»¶ï¼Œè¿™ä¸€æ ¸å¿ƒéœ€æ±‚å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†DarkEQAå¼€æºåŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡æ§åˆ¶è§†è§‰é€€åŒ–æ¥è¯„ä¼°ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒè§†è§’ä¸‹çš„é—®ç­”èƒ½åŠ›ï¼Œå…¶å…³é”®è®¾è®¡ç‰¹ç‚¹æ˜¯ç‰©ç†çœŸå®æ€§ï¼šåœ¨RAWçº¿æ€§ç©ºé—´æ¨¡æ‹ŸåŸºäºç‰©ç†çš„ç…§æ˜è¡°å‡å’Œä¼ æ„Ÿå™¨å™ªå£°ï¼Œéšåé‡‡ç”¨ISPå¯å‘çš„æ¸²æŸ“æµç¨‹ã€‚</p>
<p><strong>Result:</strong> é€šè¿‡è¯„ä¼°å¤šç§æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹å’Œä½å…‰ç…§å›¾åƒå¢å¼ºæ¨¡å‹ï¼Œç³»ç»Ÿæ­ç¤ºäº†è¿™äº›æ¨¡å‹åœ¨æŒ‘æˆ˜æ€§è§†è§‰æ¡ä»¶ä¸‹çš„å±€é™æ€§ï¼ŒDarkEQAèƒ½å¤Ÿéš”ç¦»æ„ŸçŸ¥ç“¶é¢ˆå¹¶å®ç°å¯å½’å› çš„é²æ£’æ€§åˆ†æã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä½å…‰ç…§æ¡ä»¶ä¸‹çš„æ€§èƒ½è¯„ä¼°é‡è¦æ€§ï¼ŒDarkEQAåŸºå‡†ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ ‡å‡†åŒ–æµ‹è¯•å¹³å°ï¼Œæ­ç¤ºäº†ç°æœ‰æ–¹æ³•åœ¨é»‘æš—ç¯å¢ƒä¸­çš„ä¸è¶³ï¼Œå¹¶ä¿ƒè¿›äº†æ›´å…·é²æ£’æ€§çš„å…·èº«æ™ºèƒ½ç³»ç»Ÿå‘å±•ã€‚</p>
<hr />
<h4 id="abstract_22">ğŸ“„ Abstract</h4>
<p>Vision Language Models (VLMs) are increasingly adopted as central reasoning modules for embodied agents. Existing benchmarks evaluate their capabilities under ideal, well-lit conditions, yet robust 24/7 operation demands performance under a wide range of visual degradations, including low-light conditions at night or in dark environments--a core necessity that has been largely overlooked. To address this underexplored challenge, we present DarkEQA, an open-source benchmark for evaluating EQA-relevant perceptual primitives under multi-level low-light conditions. DarkEQA isolates the perception bottleneck by evaluating question answering from egocentric observations under controlled degradations, enabling attributable robustness analysis. A key design feature of DarkEQA is its physical fidelity: visual degradations are modeled in linear RAW space, simulating physics-based illumination drop and sensor noise followed by an ISP-inspired rendering pipeline. We demonstrate the utility of DarkEQA by evaluating a wide range of state-of-the-art VLMs and Low-Light Image Enhancement (LLIE) models. Our analysis systematically reveals VLMs' limitations when operating under these challenging visual conditions. Our code and benchmark dataset will be released upon acceptance.</p>
<h3 id="24-uniact-unified-motion-generation-and-action-streaming-for-humanoid-robots">[24] <a href="https://arxiv.org/abs/2512.24321">UniAct: Unified Motion Generation and Action Streaming for Humanoid Robots</a></h3>
<p><em>Nan Jiang, Zimo He, Wanhe Yu, Lexi Pang, Yunhao Li, Hongjie Li, Jieming Cui, Yuhan Li, Yizhou Wang, Yixin Zhu, Siyuan Huang</em></p>
<h4 id="tldr_23">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†UniActï¼Œä¸€ç§ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆå¾®è°ƒçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸å› æœæµå¼å¤„ç†ç®¡é“ï¼Œå®ç°äº†äººå½¢æœºå™¨äººä»¥ä½äº500æ¯«ç§’çš„å»¶è¿Ÿæ‰§è¡Œå¤šæ¨¡æ€æŒ‡ä»¤ï¼Œåœ¨é›¶æ ·æœ¬è·Ÿè¸ªä¸å®Œç¾å‚è€ƒè¿åŠ¨æ–¹é¢å–å¾—äº†19%çš„æˆåŠŸç‡æå‡ã€‚</p>
<hr />
<h4 id="detailed-summary_23">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> äººå½¢æœºå™¨äººé¢†åŸŸé•¿æœŸé¢ä¸´çš„ä¸€ä¸ªæŒ‘æˆ˜æ˜¯å®ç°èƒ½å¤Ÿéµå¾ªå¤šæ ·åŒ–å¤šæ¨¡æ€æŒ‡ä»¤çš„é€šç”¨æ™ºèƒ½ä½“ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å°†è¯­è¨€ã€éŸ³ä¹å’Œè½¨è¿¹ç­‰å¼‚æ„æŒ‡ä»¤è½¬åŒ–ä¸ºç¨³å®šã€å®æ—¶çš„å…¨èº«åŠ¨ä½œæ–¹é¢å­˜åœ¨æ˜¾è‘—ç“¶é¢ˆï¼Œé«˜å±‚æ¬¡çš„æ„ŸçŸ¥ä¸å…¨èº«æ‰§è¡Œä¹‹é—´çš„é¸¿æ²Ÿé™åˆ¶äº†ç³»ç»Ÿçš„çµæ´»æ€§ã€‚</p>
<p><strong>Method:</strong> UniActé‡‡ç”¨ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæ•´åˆäº†å¾®è°ƒçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸å› æœæµå¼å¤„ç†ç®¡é“ï¼Œé€šè¿‡FSQå…±äº«ç¦»æ•£ç æœ¬ç»Ÿä¸€å¤šæ¨¡æ€è¾“å…¥ï¼Œç¡®ä¿è·¨æ¨¡æ€å¯¹é½çš„åŒæ—¶å°†è¿åŠ¨çº¦æŸåœ¨ç‰©ç†å¯è¡Œçš„æµå½¢ä¸Šï¼Œå®ç°äº†å¤šæ¨¡æ€æŒ‡ä»¤åˆ°åŠ¨ä½œçš„å®æ—¶è½¬æ¢ã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•åœ¨UniMoCapåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä½äº500æ¯«ç§’çš„å»¶è¿Ÿï¼Œåœ¨é›¶æ ·æœ¬è·Ÿè¸ªä¸å®Œç¾å‚è€ƒè¿åŠ¨æ–¹é¢å–å¾—äº†19%çš„æˆåŠŸç‡æå‡ï¼Œå¹¶åœ¨å¤šæ ·åŒ–çš„çœŸå®åœºæ™¯ä¸­å±•ç¤ºäº†é²æ£’çš„æ³›åŒ–èƒ½åŠ›ï¼ŒéªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ ‡å¿—ç€å‘å“åº”å¼é€šç”¨äººå½¢åŠ©æ‰‹è¿ˆå‡ºäº†å…³é”®ä¸€æ­¥ï¼Œé€šè¿‡ç»Ÿä¸€çš„æ„ŸçŸ¥ä¸æ§åˆ¶å®ç°äº†æ— ç¼äº¤äº’ï¼Œä¸ºè§£å†³å¤šæ¨¡æ€æŒ‡ä»¤åˆ°å…¨èº«åŠ¨ä½œçš„è½¬æ¢ç“¶é¢ˆæä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆï¼Œä¸ºäººå½¢æœºå™¨äººçš„å®é™…åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_23">ğŸ“„ Abstract</h4>
<p>A long-standing objective in humanoid robotics is the realization of versatile agents capable of following diverse multimodal instructions with human-level flexibility. Despite advances in humanoid control, bridging high-level multimodal perception with whole-body execution remains a significant bottleneck. Existing methods often struggle to translate heterogeneous instructions -- such as language, music, and trajectories -- into stable, real-time actions. Here we show that UniAct, a two-stage framework integrating a fine-tuned MLLM with a causal streaming pipeline, enables humanoid robots to execute multimodal instructions with sub-500 ms latency. By unifying inputs through a shared discrete codebook via FSQ, UniAct ensures cross-modal alignment while constraining motions to a physically grounded manifold. This approach yields a 19% improvement in the success rate of zero-shot tracking of imperfect reference motions. We validate UniAct on UniMoCap, our 20-hour humanoid motion benchmark, demonstrating robust generalization across diverse real-world scenarios. Our results mark a critical step toward responsive, general-purpose humanoid assistants capable of seamless interaction through unified perception and control.</p>
<h3 id="25-robust-egocentric-referring-video-object-segmentation-via-dual-modal-causal-intervention">[25] <a href="https://arxiv.org/abs/2512.24323">Robust Egocentric Referring Video Object Segmentation via Dual-Modal Causal Intervention</a></h3>
<p><em>Haijing Liu, Zhiyuan Song, Hefeng Wu, Tao Pu, Keze Wang, Liang Lin</em></p>
<h4 id="tldr_24">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†CERESï¼Œä¸€ç§ç”¨äºç¬¬ä¸€äººç§°è§†é¢‘ä¸­å‚ç…§è§†é¢‘å¯¹è±¡åˆ†å‰²çš„å› æœæ¨ç†æ¡†æ¶ï¼Œé€šè¿‡åŒæ¨¡æ€å› æœå¹²é¢„è§£å†³è®­ç»ƒæ•°æ®åå·®å’Œç¬¬ä¸€äººç§°è§†è§’çš„è§†è§‰æ··æ·†é—®é¢˜ï¼Œåœ¨Ego-RVOSåŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_24">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç¬¬ä¸€äººç§°å‚ç…§è§†é¢‘å¯¹è±¡åˆ†å‰²ä»»åŠ¡é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šè®­ç»ƒæ•°æ®ä¸­å­˜åœ¨çš„å¯¹è±¡-åŠ¨ä½œé…å¯¹åå·®å¯¼è‡´æ¨¡å‹å­¦ä¹ è™šå‡ç›¸å…³æ€§ï¼Œä»¥åŠç¬¬ä¸€äººç§°è§†è§’å›ºæœ‰çš„è§†è§‰æ··æ·†å› ç´ å¦‚å¿«é€Ÿè¿åŠ¨å’Œé¢‘ç¹é®æŒ¡ï¼Œè¿™äº›é™åˆ¶äº†ç°æœ‰æ–¹æ³•çš„é²æ£’æ€§ã€‚</p>
<p><strong>Method:</strong> CERESæ˜¯ä¸€ä¸ªæ’ä»¶å¼å› æœæ¡†æ¶ï¼Œé‡‡ç”¨åŒæ¨¡æ€å› æœå¹²é¢„ç­–ç•¥ï¼šåº”ç”¨åé—¨è°ƒæ•´åŸåˆ™æ¥æŠµæ¶ˆä»æ•°æ®é›†ç»Ÿè®¡ä¸­å­¦ä¹ çš„è¯­è¨€è¡¨ç¤ºåå·®ï¼Œå¹¶åˆ©ç”¨å‰é—¨è°ƒæ•´æ¦‚å¿µï¼Œé€šè¿‡å› æœåŸåˆ™æŒ‡å¯¼å°†è¯­ä¹‰è§†è§‰ç‰¹å¾ä¸å‡ ä½•æ·±åº¦ä¿¡æ¯æ™ºèƒ½é›†æˆï¼Œä»¥è§£å†³è§†è§‰æ··æ·†é—®é¢˜ã€‚</p>
<p><strong>Result:</strong> åœ¨Ego-RVOSåŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCERESå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å› æœæ¨ç†æ¡†æ¶åœ¨æå‡æ¨¡å‹é²æ£’æ€§å’Œåˆ†å‰²å‡†ç¡®æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†å› æœæ¨ç†åœ¨æ„å»ºæ›´å¯é çš„ç¬¬ä¸€äººç§°è§†é¢‘ç†è§£æ¨¡å‹æ–¹é¢çš„æ½œåŠ›ï¼Œé€šè¿‡è§£å†³æ•°æ®åå·®å’Œè§†è§’æ··æ·†é—®é¢˜ï¼Œä¸ºæ›´å¹¿æ³›çš„è‡ªæˆ‘ä¸­å¿ƒè§†è§‰ä»»åŠ¡æä¾›äº†æ–°çš„æ–¹æ³•è®ºæ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_24">ğŸ“„ Abstract</h4>
<p>Egocentric Referring Video Object Segmentation (Ego-RVOS) aims to segment the specific object actively involved in a human action, as described by a language query, within first-person videos. This task is critical for understanding egocentric human behavior. However, achieving such segmentation robustly is challenging due to ambiguities inherent in egocentric videos and biases present in training data. Consequently, existing methods often struggle, learning spurious correlations from skewed object-action pairings in datasets and fundamental visual confounding factors of the egocentric perspective, such as rapid motion and frequent occlusions. To address these limitations, we introduce Causal Ego-REferring Segmentation (CERES), a plug-in causal framework that adapts strong, pre-trained RVOS backbones to the egocentric domain. CERES implements dual-modal causal intervention: applying backdoor adjustment principles to counteract language representation biases learned from dataset statistics, and leveraging front-door adjustment concepts to address visual confounding by intelligently integrating semantic visual features with geometric depth information guided by causal principles, creating representations more robust to egocentric distortions. Extensive experiments demonstrate that CERES achieves state-of-the-art performance on Ego-RVOS benchmarks, highlighting the potential of applying causal reasoning to build more reliable models for broader egocentric video understanding.</p>
<h3 id="26-sensenova-mars-empowering-multimodal-agentic-reasoning-and-search-via-reinforcement-learning">[26] <a href="https://arxiv.org/abs/2512.24330">SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning</a></h3>
<p><em>Yong Xien Chng, Tao Hu, Wenwen Tong, Xueheng Li, Jiandong Chen, Haojia Yu, Jiefan Lu, Hewei Guo, Hanming Deng, Chengjun Xie, Gao Huang, Dahua Lin, Lewei Lu</em></p>
<h4 id="tldr_25">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºSenseNova-MARSï¼Œä¸€ç§é€šè¿‡å¼ºåŒ–å­¦ä¹ èµ‹èƒ½è§†è§‰è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€ä»£ç†æ¨ç†ä¸æœç´¢æ¡†æ¶ï¼Œå®ç°äº†è§†è§‰æ¨ç†ä¸å·¥å…·ä½¿ç”¨çš„äº¤ç»‡èƒ½åŠ›ï¼Œå¹¶åœ¨æœç´¢å¯¼å‘åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_25">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä»£ç†æ¨ç†æ–¹é¢ä¸»è¦å±€é™äºæ–‡æœ¬å¯¼å‘çš„æ€ç»´é“¾æˆ–å­¤ç«‹å·¥å…·è°ƒç”¨ï¼Œç¼ºä¹äººç±»èˆ¬æ— ç¼äº¤ç»‡åŠ¨æ€å·¥å…·æ“ä½œä¸è¿ç»­æ¨ç†çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦åè°ƒå¤–éƒ¨å·¥å…·ï¼ˆå¦‚æœç´¢å’Œå›¾åƒè£å‰ªï¼‰çš„çŸ¥è¯†å¯†é›†å‹å’Œè§†è§‰å¤æ‚åœºæ™¯ä¸­ã€‚</p>
<p><strong>Method:</strong> æå‡ºSenseNova-MARSå¤šæ¨¡æ€ä»£ç†æ¨ç†ä¸æœç´¢æ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ èµ‹èƒ½è§†è§‰è¯­è¨€æ¨¡å‹äº¤ç»‡è§†è§‰æ¨ç†ä¸å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼›å…·ä½“æ•´åˆå›¾åƒæœç´¢ã€æ–‡æœ¬æœç´¢å’Œå›¾åƒè£å‰ªå·¥å…·å¤„ç†ç»†ç²’åº¦å’ŒçŸ¥è¯†å¯†é›†å‹è§†è§‰ç†è§£ä»»åŠ¡ï¼›åœ¨å¼ºåŒ–å­¦ä¹ é˜¶æ®µæå‡ºæ‰¹é‡å½’ä¸€åŒ–ç»„åºåˆ—ç­–ç•¥ä¼˜åŒ–ç®—æ³•ä»¥æé«˜è®­ç»ƒç¨³å®šæ€§å¹¶å¢å¼ºæ¨¡å‹å·¥å…·è°ƒç”¨å’Œæ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> SenseNova-MARSåœ¨å¼€æºæœç´¢å’Œç»†ç²’åº¦å›¾åƒç†è§£åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼›åœ¨æœç´¢å¯¼å‘åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSenseNova-MARS-8Båœ¨MMSearchä¸Šå¾—åˆ†67.84ï¼Œåœ¨HR-MMSearchä¸Šå¾—åˆ†41.64ï¼Œè¶…è¶ŠGemini-3-Flashå’ŒGPT-5ç­‰ä¸“æœ‰æ¨¡å‹ï¼›åŒæ—¶å¼•å…¥HR-MMSearchåŸºå‡†ï¼Œè¿™æ˜¯é¦–ä¸ªç”±é«˜åˆ†è¾¨ç‡å›¾åƒç»„æˆçš„æœç´¢å¯¼å‘åŸºå‡†ï¼ŒåŒ…å«çŸ¥è¯†å¯†é›†å‹å’Œæœç´¢é©±åŠ¨é—®é¢˜ã€‚</p>
<p><strong>Conclusion:</strong> SenseNova-MARSä»£è¡¨äº†å‘ä»£ç†è§†è§‰è¯­è¨€æ¨¡å‹å‘å±•çš„æœ‰å¸Œæœ›ä¸€æ­¥ï¼Œé€šè¿‡æä¾›æœ‰æ•ˆä¸”é²æ£’çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼›è¯¥ç ”ç©¶å°†ä¿ƒè¿›è¯¥é¢†åŸŸè¿›ä¸€æ­¥æ¢ç´¢ï¼Œä½œè€…å°†å‘å¸ƒæ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†ä»¥æ”¯æŒåç»­ç ”ç©¶ã€‚</p>
<hr />
<h4 id="abstract_25">ğŸ“„ Abstract</h4>
<p>While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model's ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.</p>
<h3 id="27-spatial-aware-vision-language-model-for-autonomous-driving">[27] <a href="https://arxiv.org/abs/2512.24331">Spatial-aware Vision Language Model for Autonomous Driving</a></h3>
<p><em>Weijie Wei, Zhipeng Luo, Ling Feng, Venice Erin Liong</em></p>
<h4 id="tldr_26">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºLVLDriveæ¡†æ¶ï¼Œé€šè¿‡å°†LiDARç‚¹äº‘ä½œä¸ºé¢å¤–è¾“å…¥æ¨¡æ€æ¥å¢å¼ºç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä»¥è§£å†³åŸºäº2Då›¾åƒçš„è‡ªåŠ¨é©¾é©¶æ–¹æ³•åœ¨ä¸‰ç»´åº¦é‡ç©ºé—´æ¨ç†æ–¹é¢çš„å±€é™æ€§ï¼Œä»è€Œæå‡è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_26">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨é©¾é©¶æ–¹æ³•ä¸»è¦ä¾èµ–2Då›¾åƒçº¿ç´¢è¿›è¡Œå¤æ‚åœºæ™¯ç†è§£å’Œå†³ç­–ï¼Œè¿™å¯¼è‡´äº†åœ¨ç²¾ç¡®åº¦é‡ç©ºé—´æ¨ç†å’Œå‡ ä½•æ¨æ–­æ–¹é¢çš„ä¸¥é‡ç“¶é¢ˆï¼Œå½±å“äº†é©¾é©¶ç­–ç•¥çš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚ç°æœ‰å›¾åƒæ–¹æ³•éš¾ä»¥è¿›è¡Œå‡†ç¡®çš„ä¸‰ç»´ç©ºé—´æ¨ç†ï¼Œå› æ­¤éœ€è¦å¼•å…¥æ˜¾å¼çš„ä¸‰ç»´åº¦é‡æ•°æ®æ¥æ„å»ºå¯ä¿¡èµ–çš„VLMè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºLVLDriveæ¡†æ¶ï¼Œé€šè¿‡å°†LiDARç‚¹äº‘ä½œä¸ºé¢å¤–è¾“å…¥æ¨¡æ€æ¥å¢å¼ºç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹çš„ä¸‰ç»´åº¦é‡ç©ºé—´ç†è§£èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³ä¸‰ç»´æ•°æ®å¯¹é¢„è®­ç»ƒVLMçš„ç¾éš¾æ€§å¹²æ‰°é—®é¢˜ï¼Œè®¾è®¡äº†æ¸è¿›èåˆQ-Formeræ¨¡å—ï¼Œé€æ­¥æ³¨å…¥LiDARç‰¹å¾ä»¥ç¡®ä¿VLMç°æœ‰çŸ¥è¯†åº“çš„ç¨³å®šæ€§å’Œä¿ç•™ã€‚æ­¤å¤–ï¼Œå¼€å‘äº†ç©ºé—´æ„ŸçŸ¥é—®ç­”æ•°æ®é›†æ¥æ˜¾å¼æ•™å¯¼æ¨¡å‹é«˜çº§ä¸‰ç»´æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªè‡ªåŠ¨é©¾é©¶åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLVLDriveåœ¨åœºæ™¯ç†è§£ã€åº¦é‡ç©ºé—´æ„ŸçŸ¥å’Œå¯é é©¾é©¶å†³ç­–æ–¹é¢å‡ä¼˜äºçº¯è§†è§‰æ–¹æ³•ã€‚è¯¥æ¡†æ¶æ˜¾è‘—æå‡äº†ä¸‰ç»´ç©ºé—´æ¨ç†èƒ½åŠ›ï¼ŒéªŒè¯äº†æ¸è¿›èåˆç­–ç•¥åœ¨ä¿æŒVLMçŸ¥è¯†ç¨³å®šæ€§çš„åŒæ—¶æœ‰æ•ˆæ•´åˆä¸‰ç»´ä¿¡æ¯çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†æ˜¾å¼ä¸‰ç»´åº¦é‡æ•°æ®å¯¹äºæ„å»ºå¯ä¿¡èµ–çš„VLMè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å¿…è¦æ€§ï¼Œè¯æ˜äº†LiDAR-Vision-Languageå¤šæ¨¡æ€èåˆçš„æœ‰æ•ˆæ€§ã€‚æ¸è¿›èåˆç­–ç•¥ä¸ºæ•´åˆå¼‚æ„ä¼ æ„Ÿå™¨æ•°æ®åˆ°é¢„è®­ç»ƒæ¨¡å‹ä¸­æä¾›äº†å¯è¡Œæ–¹æ¡ˆï¼Œä¸ºæœªæ¥æ›´å®‰å…¨çš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå¼€å‘æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_26">ğŸ“„ Abstract</h4>
<p>While Vision-Language Models (VLMs) show significant promise for end-to-end autonomous driving by leveraging the common sense embedded in language models, their reliance on 2D image cues for complex scene understanding and decision-making presents a critical bottleneck for safety and reliability. Current image-based methods struggle with accurate metric spatial reasoning and geometric inference, leading to unreliable driving policies. To bridge this gap, we propose LVLDrive (LiDAR-Vision-Language), a novel framework specifically designed to upgrade existing VLMs with robust 3D metric spatial understanding for autonomous driving by incoperating LiDAR point cloud as an extra input modality. A key challenge lies in mitigating the catastrophic disturbance introduced by disparate 3D data to the pre-trained VLMs. To this end, we introduce a Gradual Fusion Q-Former that incrementally injects LiDAR features, ensuring the stability and preservation of the VLM's existing knowledge base. Furthermore, we develop a spatial-aware question-answering (SA-QA) dataset to explicitly teach the model advanced 3D perception and reasoning capabilities. Extensive experiments on driving benchmarks demonstrate that LVLDrive achieves superior performance compared to vision-only counterparts across scene understanding, metric spatial perception, and reliable driving decision-making. Our work highlights the necessity of explicit 3D metric data for building trustworthy VLM-based autonomous systems.</p>
<h3 id="28-forging-spatial-intelligence-a-roadmap-of-multi-modal-data-pre-training-for-autonomous-systems">[28] <a href="https://arxiv.org/abs/2512.24385">Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems</a></h3>
<p><em>Song Wang, Lingdong Kong, Xiaolu Liu, Hao Shi, Wentong Li, Jianke Zhu, Steven C. H. Hoi</em></p>
<h4 id="tldr_27">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€é¢„è®­ç»ƒæ¡†æ¶å’Œç»Ÿä¸€åˆ†ç±»ä½“ç³»ï¼Œæ—¨åœ¨è§£å†³è‡ªä¸»ç³»ç»Ÿä¸­å¤šæ¨¡æ€ä¼ æ„Ÿå™¨æ•°æ®èåˆçš„æŒ‘æˆ˜ï¼Œä¸ºå®ç°ç©ºé—´æ™ºèƒ½æä¾›äº†ç³»ç»Ÿæ€§æ–¹æ³•ã€‚ç ”ç©¶è¯†åˆ«äº†å…³é”®æŠ€æœ¯ç“¶é¢ˆå¹¶æå‡ºäº†å®ç°é€šç”¨å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„è·¯çº¿å›¾ã€‚</p>
<hr />
<h4 id="detailed-summary_27">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è‡ªåŠ¨é©¾é©¶è½¦è¾†å’Œæ— äººæœºç­‰è‡ªä¸»ç³»ç»Ÿçš„å¿«é€Ÿå‘å±•ï¼Œè¿«åˆ‡éœ€è¦ä»å¤šæ¨¡æ€è½¦è½½ä¼ æ„Ÿå™¨æ•°æ®ä¸­æ„å»ºçœŸæ­£çš„ç©ºé—´æ™ºèƒ½ã€‚å°½ç®¡åŸºç¡€æ¨¡å‹åœ¨å•æ¨¡æ€åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å¦‚ä½•æ•´åˆæ‘„åƒå¤´ã€æ¿€å…‰é›·è¾¾ç­‰ä¸åŒä¼ æ„Ÿå™¨çš„èƒ½åŠ›ä»¥å½¢æˆç»Ÿä¸€ç†è§£ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œç°æœ‰ç ”ç©¶ç¼ºä¹ç³»ç»Ÿæ€§çš„å¤šæ¨¡æ€é¢„è®­ç»ƒæ¡†æ¶å’Œç»Ÿä¸€åˆ†ç±»ä½“ç³»ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€é¢„è®­ç»ƒæ¡†æ¶ï¼Œç³»ç»Ÿåˆ†æäº†åŸºç¡€ä¼ æ„Ÿå™¨ç‰¹æ€§ä¸å­¦ä¹ ç­–ç•¥ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œè¯„ä¼°äº†å¹³å°ç‰¹å®šæ•°æ®é›†åœ¨æ¨åŠ¨æŠ€æœ¯è¿›æ­¥ä¸­çš„ä½œç”¨ã€‚æ ¸å¿ƒè´¡çŒ®æ˜¯æ„å»ºäº†ä¸€ä¸ªç»Ÿä¸€çš„é¢„è®­ç»ƒèŒƒå¼åˆ†ç±»ä½“ç³»ï¼Œæ¶µç›–ä»å•æ¨¡æ€åŸºçº¿åˆ°å­¦ä¹ æ•´ä½“è¡¨ç¤ºçš„å¤æ‚ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äº3Dç›®æ ‡æ£€æµ‹å’Œè¯­ä¹‰å æ®é¢„æµ‹ç­‰é«˜çº§ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¢è®¨äº†æ–‡æœ¬è¾“å…¥å’Œå æ®è¡¨ç¤ºçš„æ•´åˆï¼Œä»¥ä¿ƒè¿›å¼€æ”¾ä¸–ç•Œæ„ŸçŸ¥å’Œè§„åˆ’ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶é€šè¿‡ç³»ç»Ÿåˆ†æè¯†åˆ«äº†æ¨åŠ¨å¤šæ¨¡æ€ç©ºé—´æ™ºèƒ½å‘å±•çš„æ ¸å¿ƒæŠ€æœ¯é›†åˆï¼Œå»ºç«‹äº†ä»å•æ¨¡æ€åˆ°ç»Ÿä¸€æ¡†æ¶çš„å®Œæ•´åˆ†ç±»ä½“ç³»ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæ”¯æŒ3Dç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰å æ®é¢„æµ‹ç­‰é«˜çº§æ„ŸçŸ¥ä»»åŠ¡ï¼Œå¹¶é€šè¿‡æ•´åˆæ–‡æœ¬è¾“å…¥å®ç°å¼€æ”¾ä¸–ç•Œæ„ŸçŸ¥èƒ½åŠ›ã€‚ç ”ç©¶è¿˜è¯„ä¼°äº†ä¸åŒå¹³å°ç‰¹å®šæ•°æ®é›†å¯¹æŠ€æœ¯å‘å±•çš„ä¿ƒè¿›ä½œç”¨ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„å‘å±•æä¾›äº†ç³»ç»Ÿæ€§æ¡†æ¶å’Œåˆ†ç±»ä½“ç³»ï¼Œè¯†åˆ«äº†è®¡ç®—æ•ˆç‡å’Œæ¨¡å‹å¯æ‰©å±•æ€§ç­‰å…³é”®ç“¶é¢ˆã€‚ç ”ç©¶æå‡ºäº†å®ç°é€šç”¨å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„è·¯çº¿å›¾ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿå®ç°ç¨³å¥çš„ç©ºé—´æ™ºèƒ½ï¼Œæ”¯æŒè‡ªåŠ¨é©¾é©¶ç­‰å®é™…åº”ç”¨éƒ¨ç½²ã€‚è¯¥å·¥ä½œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ˜ç¡®çš„æŠ€æœ¯æ–¹å‘å’Œè¯„ä¼°åŸºå‡†ã€‚</p>
<hr />
<h4 id="abstract_27">ğŸ“„ Abstract</h4>
<p>The rapid advancement of autonomous systems, including self-driving vehicles and drones, has intensified the need to forge true Spatial Intelligence from multi-modal onboard sensor data. While foundation models excel in single-modal contexts, integrating their capabilities across diverse sensors like cameras and LiDAR to create a unified understanding remains a formidable challenge. This paper presents a comprehensive framework for multi-modal pre-training, identifying the core set of techniques driving progress toward this goal. We dissect the interplay between foundational sensor characteristics and learning strategies, evaluating the role of platform-specific datasets in enabling these advancements. Our central contribution is the formulation of a unified taxonomy for pre-training paradigms: ranging from single-modality baselines to sophisticated unified frameworks that learn holistic representations for advanced tasks like 3D object detection and semantic occupancy prediction. Furthermore, we investigate the integration of textual inputs and occupancy representations to facilitate open-world perception and planning. Finally, we identify critical bottlenecks, such as computational efficiency and model scalability, and propose a roadmap toward general-purpose multi-modal foundation models capable of achieving robust Spatial Intelligence for real-world deployment.</p>
<h3 id="29-hierarchical-vector-quantized-latents-for-perceptual-low-resolution-video-compression">[29] <a href="https://arxiv.org/abs/2512.24547">Hierarchical Vector-Quantized Latents for Perceptual Low-Resolution Video Compression</a></h3>
<p><em>Manikanta Kotthapalli, Banafsheh Rekabdar</em></p>
<h4 id="tldr_28">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šå°ºåº¦å‘é‡é‡åŒ–å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆMS-VQ-VAEï¼‰ï¼Œç”¨äºç”Ÿæˆä½åˆ†è¾¨ç‡è§†é¢‘çš„ç´§å‡‘é«˜ä¿çœŸæ½œåœ¨è¡¨ç¤ºï¼Œé€‚ç”¨äºå¸¦å®½æ•æ„Ÿåœºæ™¯ä¸‹çš„é«˜æ•ˆå­˜å‚¨å’Œä¼ è¾“ã€‚è¯¥æ¨¡å‹æ‰©å±•äº†VQ-VAE-2æ¡†æ¶åˆ°æ—¶ç©ºåŸŸï¼Œé‡‡ç”¨è½»é‡çº§è®¾è®¡ï¼ˆçº¦1850ä¸‡å‚æ•°ï¼‰ï¼Œåœ¨UCF101æ•°æ®é›†ä¸Šå®ç°äº†25.96 dB PSNRå’Œ0.8375 SSIMçš„ä¼˜å¼‚æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_28">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è§†é¢‘æµé‡çš„æŒ‡æ•°å¢é•¿å¯¹å¸¦å®½å’Œå­˜å‚¨åŸºç¡€è®¾æ–½æå‡ºäº†æ›´é«˜è¦æ±‚ï¼Œç‰¹åˆ«æ˜¯å†…å®¹åˆ†å‘ç½‘ç»œå’Œè¾¹ç¼˜è®¾å¤‡ã€‚ä¼ ç»Ÿè§†é¢‘ç¼–è§£ç å™¨å¦‚H.264å’ŒHEVCè™½ç„¶å‹ç¼©ç‡é«˜ï¼Œä½†ä¸»è¦é’ˆå¯¹åƒç´ åŸŸé‡å»ºè®¾è®¡ï¼Œç¼ºä¹å¯¹æœºå™¨å­¦ä¹ ä¸­å¿ƒåŒ–æ½œåœ¨è¡¨ç¤ºçš„åŸç”Ÿæ”¯æŒï¼Œé™åˆ¶äº†å…¶ä¸æ·±åº¦å­¦ä¹ æµç¨‹çš„é›†æˆã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šå°ºåº¦å‘é‡é‡åŒ–å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆMS-VQ-VAEï¼‰ï¼Œå°†VQ-VAE-2æ¡†æ¶æ‰©å±•åˆ°æ—¶ç©ºè®¾ç½®ï¼Œé‡‡ç”¨ä¸¤çº§åˆ†å±‚æ½œåœ¨ç»“æ„ï¼Œä½¿ç”¨3Dæ®‹å·®å·ç§¯æ„å»ºã€‚æ¨¡å‹è½»é‡çº§è®¾è®¡ï¼ˆçº¦1850ä¸‡å‚æ•°ï¼‰ï¼Œé’ˆå¯¹64Ã—64åˆ†è¾¨ç‡è§†é¢‘ç‰‡æ®µä¼˜åŒ–ï¼Œé€‚åˆè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ã€‚ä¸ºæé«˜æ„ŸçŸ¥é‡å»ºè´¨é‡ï¼Œå¼•å…¥äº†åŸºäºé¢„è®­ç»ƒVGG16ç½‘ç»œçš„æ„ŸçŸ¥æŸå¤±å‡½æ•°ã€‚</p>
<p><strong>Result:</strong> åœ¨UCF101æ•°æ®é›†ä¸Šä½¿ç”¨2ç§’è§†é¢‘ç‰‡æ®µï¼ˆ32å¸§ï¼Œ16 FPSï¼‰è¿›è¡Œè®­ç»ƒï¼Œæµ‹è¯•é›†ä¸Šè¾¾åˆ°25.96 dB PSNRå’Œ0.8375 SSIMã€‚éªŒè¯é›†ä¸Šç›¸æ¯”å•å°ºåº¦åŸºçº¿æ¨¡å‹æå‡äº†1.41 dB PSNRå’Œ0.0248 SSIMï¼Œè¯æ˜äº†å¤šå°ºåº¦æ¶æ„çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ¡†æ¶ç‰¹åˆ«é€‚ç”¨äºå¸¦å®½æ•æ„Ÿåœºæ™¯ä¸‹çš„å¯æ‰©å±•è§†é¢‘å‹ç¼©ï¼ŒåŒ…æ‹¬å®æ—¶æµåª’ä½“ã€ç§»åŠ¨è§†é¢‘åˆ†æå’ŒCDNçº§å­˜å‚¨ä¼˜åŒ–ã€‚æ¨¡å‹è½»é‡çº§è®¾è®¡ä½¿å…¶é€‚åˆåœ¨è®¡ç®—å’Œå†…å­˜èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ï¼Œä¸ºæ·±åº¦å­¦ä¹ æµç¨‹ä¸­çš„è§†é¢‘è¡¨ç¤ºæä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_28">ğŸ“„ Abstract</h4>
<p>The exponential growth of video traffic has placed increasing demands on bandwidth and storage infrastructure, particularly for content delivery networks (CDNs) and edge devices. While traditional video codecs like H.264 and HEVC achieve high compression ratios, they are designed primarily for pixel-domain reconstruction and lack native support for machine learning-centric latent representations, limiting their integration into deep learning pipelines. In this work, we present a Multi-Scale Vector Quantized Variational Autoencoder (MS-VQ-VAE) designed to generate compact, high-fidelity latent representations of low-resolution video, suitable for efficient storage, transmission, and client-side decoding. Our architecture extends the VQ-VAE-2 framework to a spatiotemporal setting, introducing a two-level hierarchical latent structure built with 3D residual convolutions. The model is lightweight (approximately 18.5M parameters) and optimized for 64x64 resolution video clips, making it appropriate for deployment on edge devices with constrained compute and memory resources. To improve perceptual reconstruction quality, we incorporate a perceptual loss derived from a pre-trained VGG16 network. Trained on the UCF101 dataset using 2-second video clips (32 frames at 16 FPS), on the test set we achieve 25.96 dB PSNR and 0.8375 SSIM. On validation, our model improves over the single-scale baseline by 1.41 dB PSNR and 0.0248 SSIM. The proposed framework is well-suited for scalable video compression in bandwidth-sensitive scenarios, including real-time streaming, mobile video analytics, and CDN-level storage optimization.</p>
<h3 id="30-phygdpo-physics-aware-groupwise-direct-preference-optimization-for-physically-consistent-text-to-video-generation">[30] <a href="https://arxiv.org/abs/2512.24551">PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation</a></h3>
<p><em>Yuanhao Cai, Kunpeng Li, Menglin Jia, Jialiang Wang, Junzhe Sun, Feng Liang, Weifeng Chen, Felix Juefei-Xu, Chu Wang, Ali Thabet, Xiaoliang Dai, Xuan Ju, Alan Yuille, Ji Hou</em></p>
<h4 id="tldr_29">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„ç‰©ç†æ„ŸçŸ¥åå¥½ä¼˜åŒ–æ¡†æ¶PhyGDPOï¼Œé€šè¿‡æ„å»ºå¤§è§„æ¨¡ç‰©ç†å¢å¼ºæ•°æ®é›†PhyVidGen-135Kå’Œè®¾è®¡åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„ç‰©ç†å¼•å¯¼å¥–åŠ±æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆè§†é¢‘çš„ç‰©ç†ä¸€è‡´æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_29">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ–¹æ³•è™½ç„¶åœ¨è§†è§‰è´¨é‡ä¸Šå–å¾—è¿›å±•ï¼Œä½†ç”Ÿæˆå†…å®¹éš¾ä»¥éµå¾ªç‰©ç†å®šå¾‹ï¼Œç°æœ‰åŸºäºå›¾å½¢å­¦æˆ–æç¤ºæ‰©å±•çš„æ–¹æ³•éš¾ä»¥è¶…è¶Šç®€å•æ¨¡æ‹Ÿç¯å¢ƒæˆ–å­¦ä¹ éšå¼ç‰©ç†æ¨ç†ï¼ŒåŒæ—¶ç¼ºä¹åŒ…å«ä¸°å¯Œç‰©ç†äº¤äº’ç°è±¡çš„è®­ç»ƒæ•°æ®ä¹Ÿæ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡é¦–å…ˆæå‡ºç‰©ç†å¢å¼ºè§†é¢‘æ•°æ®æ„å»ºæµç¨‹PhyAugPipeï¼Œåˆ©ç”¨å…·æœ‰æ€ç»´é“¾æ¨ç†èƒ½åŠ›çš„è§†è§‰è¯­è¨€æ¨¡å‹æ”¶é›†å¤§è§„æ¨¡è®­ç»ƒæ•°æ®é›†PhyVidGen-135Kï¼›ç„¶åæ„å»ºåŸºäºç»„åˆ«Plackett-Luceæ¦‚ç‡æ¨¡å‹çš„ç‰©ç†æ„ŸçŸ¥ç»„åˆ«ç›´æ¥åå¥½ä¼˜åŒ–æ¡†æ¶PhyGDPOï¼Œå…¶ä¸­è®¾è®¡äº†ç‰©ç†å¼•å¯¼å¥–åŠ±æ–¹æ¡ˆå°†VLMç‰©ç†å¥–åŠ±åµŒå…¥ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¹¶æå‡ºLoRA-Switch Referenceæ–¹æ¡ˆæ¶ˆé™¤å†…å­˜å¯†é›†å‹å‚è€ƒå¤åˆ¶ä»¥å®ç°é«˜æ•ˆè®­ç»ƒã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨PhyGenBenchå’ŒVideoPhy2åŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„å¼€æºæ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨æå‡ç”Ÿæˆè§†é¢‘ç‰©ç†ä¸€è‡´æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œç›¸å…³ä»£ç ã€æ¨¡å‹å’Œæ•°æ®å°†é€šè¿‡GitHubå¼€æºå‘å¸ƒã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä¸­çš„ç‰©ç†ä¸€è‡´æ€§æŒ‘æˆ˜æä¾›äº†ç³»ç»Ÿè§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡æ•°æ®æ„å»ºã€ä¼˜åŒ–æ¡†æ¶å’Œå¥–åŠ±è®¾è®¡çš„ååŒåˆ›æ–°ï¼Œä¸ºç‰©ç†æ„ŸçŸ¥å†…å®¹ç”Ÿæˆå¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œå…¶å¼€æºç­–ç•¥å°†ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶å’Œå‘å±•ã€‚</p>
<hr />
<h4 id="abstract_29">ğŸ“„ Abstract</h4>
<p>Recent advances in text-to-video (T2V) generation have achieved good visual quality, yet synthesizing videos that faithfully follow physical laws remains an open challenge. Existing methods mainly based on graphics or prompt extension struggle to generalize beyond simple simulated environments or learn implicit physical reasoning. The scarcity of training data with rich physics interactions and phenomena is also a problem. In this paper, we first introduce a Physics-Augmented video data construction Pipeline, PhyAugPipe, that leverages a vision-language model (VLM) with chain-of-thought reasoning to collect a large-scale training dataset, PhyVidGen-135K. Then we formulate a principled Physics-aware Groupwise Direct Preference Optimization, PhyGDPO, framework that builds upon the groupwise Plackett-Luce probabilistic model to capture holistic preferences beyond pairwise comparisons. In PhyGDPO, we design a Physics-Guided Rewarding (PGR) scheme that embeds VLM-based physics rewards to steer optimization toward physical consistency. We also propose a LoRA-Switch Reference (LoRA-SR) scheme that eliminates memory-heavy reference duplication for efficient training. Experiments show that our method significantly outperforms state-of-the-art open-source methods on PhyGenBench and VideoPhy2. Please check our project page at https://caiyuanhao1998.github.io/project/PhyGDPO for more video results. Our code, models, and data will be released at https://github.com/caiyuanhao1998/Open-PhyGDPO</p>
<h3 id="31-rgbt-ground-benchmark-visual-grounding-beyond-rgb-in-complex-real-world-scenarios">[31] <a href="https://arxiv.org/abs/2512.24561">RGBT-Ground Benchmark: Visual Grounding Beyond RGB in Complex Real-World Scenarios</a></h3>
<p><em>Tianyi Zhao, Jiawen Xi, Linhui Xiao, Junnan Li, Xue Yang, Maoxun Yuan, Xingxing Wei</em></p>
<h4 id="tldr_30">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†RGBT-Groundï¼Œé¦–ä¸ªé¢å‘å¤æ‚çœŸå®åœºæ™¯çš„å¤§è§„æ¨¡è§†è§‰å®šä½åŸºå‡†ï¼ŒåŒ…å«ç©ºé—´å¯¹é½çš„RGBå’Œçƒ­çº¢å¤–å›¾åƒå¯¹åŠé«˜è´¨é‡æ ‡æ³¨ï¼›åŒæ—¶æå‡ºäº†ç»Ÿä¸€çš„è§†è§‰å®šä½æ¡†æ¶RGBT-VGNetï¼Œé€šè¿‡èåˆäº’è¡¥è§†è§‰æ¨¡æ€å®ç°é²æ£’å®šä½ã€‚</p>
<hr />
<h4 id="detailed-summary_30">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†è§‰å®šä½åŸºå‡†å¤§å¤šåŸºäºCOCOç­‰æ¸…æ´ç¯å¢ƒæ•°æ®é›†æ„å»ºï¼Œåœºæ™¯å¤šæ ·æ€§æœ‰é™ï¼Œæ— æ³•åæ˜ çœŸå®ä¸–ç•Œä¸­å…‰ç…§ã€å¤©æ°”ç­‰å¤æ‚æ¡ä»¶çš„å˜åŒ–ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›è¯„ä¼°ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æ„å»ºäº†RGBT-GroundåŸºå‡†ï¼ŒåŒ…å«ç©ºé—´å¯¹é½çš„RGBå’Œçƒ­çº¢å¤–å›¾åƒå¯¹ã€é«˜è´¨é‡æŒ‡ä»£è¡¨è¾¾å¼ã€å¯¹è±¡è¾¹ç•Œæ¡†åŠåœºæ™¯ã€ç¯å¢ƒå’Œå¯¹è±¡çº§åˆ«çš„ç»†ç²’åº¦æ ‡æ³¨ï¼›æå‡ºäº†ç»Ÿä¸€çš„è§†è§‰å®šä½æ¡†æ¶ï¼Œæ”¯æŒå•æ¨¡æ€å’Œå¤šæ¨¡æ€è§†è§‰è¾“å…¥ï¼Œå¹¶è®¾è®¡äº†RGBT-VGNetåŸºçº¿æ¨¡å‹ï¼Œæœ‰æ•ˆèåˆäº’è¡¥è§†è§‰æ¨¡æ€ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„RGBT-VGNetåœ¨RGBT-GroundåŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•çš„é€‚é…ç‰ˆæœ¬ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤œé—´å’Œè¿œè·ç¦»åœºæ™¯ä¸­è¡¨ç°çªå‡ºï¼ŒéªŒè¯äº†å¤šæ¨¡æ€èåˆåœ¨å¤æ‚çœŸå®åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºå¤æ‚çœŸå®åœºæ™¯ä¸‹çš„é²æ£’è§†è§‰å®šä½æä¾›äº†é¦–ä¸ªå¤šæ¨¡æ€åŸºå‡†å’Œæœ‰æ•ˆåŸºçº¿ï¼Œä¿ƒè¿›äº†è§†è§‰è¯­è¨€ç†è§£åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­çš„å‘å±•ï¼›å¤šæ¨¡æ€èåˆç­–ç•¥èƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹åœ¨æŒ‘æˆ˜æ€§æ¡ä»¶ä¸‹çš„æ€§èƒ½ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†é‡è¦èµ„æºå’Œæ–¹æ³•æŒ‡å¯¼ã€‚</p>
<hr />
<h4 id="abstract_30">ğŸ“„ Abstract</h4>
<p>Visual Grounding (VG) aims to localize specific objects in an image according to natural language expressions, serving as a fundamental task in vision-language understanding. However, existing VG benchmarks are mostly derived from datasets collected under clean environments, such as COCO, where scene diversity is limited. Consequently, they fail to reflect the complexity of real-world conditions, such as changes in illumination, weather, etc., that are critical to evaluating model robustness and generalization in safety-critical applications. To address these limitations, we present RGBT-Ground, the first large-scale visual grounding benchmark built for complex real-world scenarios. It consists of spatially aligned RGB and Thermal infrared (TIR) image pairs with high-quality referring expressions, corresponding object bounding boxes, and fine-grained annotations at the scene, environment, and object levels. This benchmark enables comprehensive evaluation and facilitates the study of robust grounding under diverse and challenging conditions. Furthermore, we establish a unified visual grounding framework that supports both uni-modal (RGB or TIR) and multi-modal (RGB-TIR) visual inputs. Based on it, we propose RGBT-VGNet, a simple yet effective baseline for fusing complementary visual modalities to achieve robust grounding. We conduct extensive adaptations to the existing methods on RGBT-Ground. Experimental results show that our proposed RGBT-VGNet significantly outperforms these adapted methods, particularly in nighttime and long-distance scenarios. All resources will be publicly released to promote future research on robust visual grounding in complex real-world environments.</p>
<h3 id="32-improving-few-shot-change-detection-visual-question-answering-via-decision-ambiguity-guided-reinforcement-fine-tuning">[32] <a href="https://arxiv.org/abs/2512.24591">Improving Few-Shot Change Detection Visual Question Answering via Decision-Ambiguity-guided Reinforcement Fine-Tuning</a></h3>
<p><em>Fuyu Dong, Ke Li, Di Wang, Nan Luo, Yiming Zhang, Kaiyu Li, Jianfei Yang, Quan Wang</em></p>
<h4 id="tldr_31">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºDARFTæ¡†æ¶ï¼Œä¸€ç§å†³ç­–æ¨¡ç³Šæ€§å¼•å¯¼çš„å¼ºåŒ–å¾®è°ƒæ–¹æ³•ï¼Œç”¨äºæå‡å˜åŒ–æ£€æµ‹è§†è§‰é—®ç­”æ¨¡å‹çš„åˆ¤åˆ«èƒ½åŠ›å’Œé²æ£’æ€§ï¼Œé€šè¿‡æ˜¾å¼ä¼˜åŒ–å†³ç­–æ¨¡ç³Šæ ·æœ¬æ¥æŠ‘åˆ¶å¼ºå¹²æ‰°é¡¹å¹¶é”åŒ–å†³ç­–è¾¹ç•Œã€‚</p>
<hr />
<h4 id="detailed-summary_31">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å˜åŒ–æ£€æµ‹è§†è§‰é—®ç­”ä¸­ï¼Œç°æœ‰åŸºäºç›‘ç£å¾®è°ƒçš„æ–¹æ³•å­˜åœ¨å¤§é‡å†³ç­–æ¨¡ç³Šæ€§å¤±è´¥æ¡ˆä¾‹ï¼Œæ¨¡å‹å¯¹æ­£ç¡®ç­”æ¡ˆä¸å¼ºå¹²æ‰°é¡¹åˆ†é…ç›¸ä¼¼ç½®ä¿¡åº¦ï¼Œè€Œéæ˜æ˜¾é”™è¯¯é¢„æµ‹ï¼Œè¿™ç§å†³ç­–æ¨¡ç³Šæ ·æœ¬é™åˆ¶äº†æ¨¡å‹çš„åˆ¤åˆ«èƒ½åŠ›å’Œé²æ£’æ€§ã€‚</p>
<p><strong>Method:</strong> æå‡ºDARFTæ¡†æ¶ï¼Œé¦–å…ˆä½¿ç”¨ç›‘ç£å¾®è°ƒè®­ç»ƒçš„å‚è€ƒç­–ç•¥æŒ–æ˜å†³ç­–æ¨¡ç³Šæ ·æœ¬ï¼Œç„¶ååœ¨æŒ–æ˜çš„å­é›†ä¸Šåº”ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œé€šè¿‡å¤šæ ·æœ¬è§£ç å’Œç»„å†…ç›¸å¯¹ä¼˜åŠ¿æ¥æŠ‘åˆ¶å¼ºå¹²æ‰°é¡¹å¹¶é”åŒ–å†³ç­–è¾¹ç•Œï¼Œæ— éœ€é¢å¤–ç›‘ç£ä¿¡å·ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDARFTåœ¨ç›‘ç£å¾®è°ƒåŸºçº¿ä¸Šå–å¾—ä¸€è‡´æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸‹è¡¨ç°æ˜¾è‘—ï¼ŒéªŒè¯äº†æ˜¾å¼ä¼˜åŒ–å†³ç­–æ¨¡ç³Šæ ·æœ¬å¯¹æå‡æ¨¡å‹åˆ¤åˆ«èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> å†³ç­–æ¨¡ç³Šæ€§æ˜¯å˜åŒ–æ£€æµ‹è§†è§‰é—®ç­”æ¨¡å‹å¤±è´¥çš„é‡è¦æ ¹æºï¼Œæ˜¾å¼ä¼˜åŒ–è¿™ç±»æ ·æœ¬èƒ½æœ‰æ•ˆæå‡æ¨¡å‹é²æ£’æ€§ï¼Œç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ä¸ºå¤„ç†å†³ç­–æ¨¡ç³Šæ€§æä¾›äº†æœ‰æ•ˆæ¡†æ¶ï¼Œå¯¹å°‘æ ·æœ¬åœºæ™¯å°¤å…¶æœ‰ç›Šã€‚</p>
<hr />
<h4 id="abstract_31">ğŸ“„ Abstract</h4>
<p>Change detection visual question answering (CDVQA) requires answering text queries by reasoning about semantic changes in bi-temporal remote sensing images. A straightforward approach is to boost CDVQA performance with generic vision-language models via supervised fine-tuning (SFT). Despite recent progress, we observe that a significant portion of failures do not stem from clearly incorrect predictions, but from decision ambiguity, where the model assigns similar confidence to the correct answer and strong distractors. To formalize this challenge, we define Decision-Ambiguous Samples (DAS) as instances with a small probability margin between the ground-truth answer and the most competitive alternative. We argue that explicitly optimizing DAS is crucial for improving the discriminability and robustness of CDVQA models. To this end, we propose DARFT, a Decision-Ambiguity-guided Reinforcement Fine-Tuning framework that first mines DAS using an SFT-trained reference policy and then applies group-relative policy optimization on the mined subset. By leveraging multi-sample decoding and intra-group relative advantages, DARFT suppresses strong distractors and sharpens decision boundaries without additional supervision. Extensive experiments demonstrate consistent gains over SFT baselines, particularly under few-shot settings.</p>
<h3 id="33-slicelens-fine-grained-and-grounded-error-slice-discovery-for-multi-instance-vision-tasks">[33] <a href="https://arxiv.org/abs/2512.24592">SliceLens: Fine-Grained and Grounded Error Slice Discovery for Multi-Instance Vision Tasks</a></h3>
<p><em>Wei Zhang, Chaoqun Wang, Zixuan Guan, Sam Kao, Pengfei Zhao, Peng Wu, Sifeng He</em></p>
<h4 id="tldr_32">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºSliceLensï¼Œä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„å‡è®¾é©±åŠ¨æ¡†æ¶ï¼Œç”¨äºåœ¨å¤šå®ä¾‹è§†è§‰ä»»åŠ¡ä¸­è¯†åˆ«ç»†ç²’åº¦é”™è¯¯åˆ‡ç‰‡ï¼Œå¹¶å¼•å…¥é¦–ä¸ªé’ˆå¯¹ç»†ç²’åº¦é”™è¯¯åˆ‡ç‰‡å‘ç°çš„åŸºå‡†FeSDï¼Œæ˜¾è‘—æå‡äº†é”™è¯¯åˆ‡ç‰‡çš„å‘ç°ç²¾åº¦å’Œå¯è§£é‡Šæ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_32">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰é”™è¯¯åˆ‡ç‰‡å‘ç°æ–¹æ³•ä¸»è¦é’ˆå¯¹å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œéš¾ä»¥é€‚ç”¨äºæ£€æµ‹ã€åˆ†å‰²å’Œå§¿æ€ä¼°è®¡ç­‰å¤šå®ä¾‹è§†è§‰ä»»åŠ¡ï¼Œä¸”ç°æœ‰åŸºå‡†é€šå¸¸é’ˆå¯¹ç‰¹å®šç®—æ³•æˆ–åå‘å›¾åƒåˆ†ç±»ï¼Œç¼ºä¹åæ˜ çœŸå®æ¨¡å‹å¤±è´¥çš„ç»†ç²’åº¦è¯„ä¼°ã€‚ç°å®åœºæ™¯ä¸­çš„é”™è¯¯åˆ‡ç‰‡å¸¸æ¶‰åŠå¤æ‚è§†è§‰å…³ç³»çš„è§’è½æ¡ˆä¾‹ï¼Œç°æœ‰ç¼ºä¹ç»†ç²’åº¦æ¨ç†çš„å®ä¾‹çº§æ–¹æ³•éš¾ä»¥æä¾›æœ‰æ„ä¹‰çš„æ´å¯Ÿã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºSliceLensæ¡†æ¶ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹é€šè¿‡åŸºäºè§†è§‰çš„æ¨ç†ç”Ÿæˆå’ŒéªŒè¯å¤šæ ·åŒ–å¤±è´¥å‡è®¾ï¼Œå®ç°ç»†ç²’åº¦ä¸”å¯è§£é‡Šçš„é”™è¯¯åˆ‡ç‰‡å¯é è¯†åˆ«ã€‚åŒæ—¶å¼•å…¥FeSDåŸºå‡†ï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨ä¸ºè¯„ä¼°å®ä¾‹çº§è§†è§‰ä»»åŠ¡ä¸­ç»†ç²’åº¦é”™è¯¯åˆ‡ç‰‡å‘ç°è€Œè®¾è®¡çš„åŸºå‡†ï¼ŒåŒ…å«ä¸“å®¶æ ‡æ³¨å¹¶ç²¾å¿ƒç»†åŒ–çš„çœŸå®åˆ‡ç‰‡ï¼Œç²¾ç¡®åœ°å®šä½åˆ°å±€éƒ¨é”™è¯¯åŒºåŸŸã€‚</p>
<p><strong>Result:</strong> åœ¨ç°æœ‰åŸºå‡†å’ŒFeSDä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSliceLenså®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨FeSDä¸Šå°†Precision@10æé«˜äº†0.42ï¼ˆ0.73 vs. 0.31ï¼‰ï¼Œå¹¶è¯†åˆ«å‡ºå¯è§£é‡Šçš„åˆ‡ç‰‡ï¼Œä¿ƒè¿›äº†å¯æ“ä½œçš„æ¨¡å‹æ”¹è¿›ï¼Œè¿™åœ¨æ¨¡å‹ä¿®å¤å®éªŒä¸­å¾—åˆ°äº†éªŒè¯ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºè§£å†³å¤šå®ä¾‹è§†è§‰ä»»åŠ¡ä¸­çš„é”™è¯¯åˆ‡ç‰‡å‘ç°é—®é¢˜æä¾›äº†æœ‰æ•ˆçš„æ¡†æ¶å’ŒåŸºå‡†ï¼Œé€šè¿‡ç»“åˆå¤§è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†é”™è¯¯åˆ‡ç‰‡çš„å‘ç°ç²¾åº¦å’Œå¯è§£é‡Šæ€§ï¼Œä¸ºæ¨¡å‹è¯„ä¼°å’Œä¿®å¤æä¾›äº†å®ç”¨å·¥å…·ã€‚FeSDåŸºå‡†çš„å»ºç«‹ä¸ºæœªæ¥ç»†ç²’åº¦é”™è¯¯åˆ‡ç‰‡ç ”ç©¶æä¾›äº†æ ‡å‡†åŒ–è¯„ä¼°å¹³å°ã€‚</p>
<hr />
<h4 id="abstract_32">ğŸ“„ Abstract</h4>
<p>Systematic failures of computer vision models on subsets with coherent visual patterns, known as error slices, pose a critical challenge for robust model evaluation. Existing slice discovery methods are primarily developed for image classification, limiting their applicability to multi-instance tasks such as detection, segmentation, and pose estimation. In real-world scenarios, error slices often arise from corner cases involving complex visual relationships, where existing instance-level approaches lacking fine-grained reasoning struggle to yield meaningful insights. Moreover, current benchmarks are typically tailored to specific algorithms or biased toward image classification, with artificial ground truth that fails to reflect real model failures. To address these limitations, we propose SliceLens, a hypothesis-driven framework that leverages LLMs and VLMs to generate and verify diverse failure hypotheses through grounded visual reasoning, enabling reliable identification of fine-grained and interpretable error slices. We further introduce FeSD (Fine-grained Slice Discovery), the first benchmark specifically designed for evaluating fine-grained error slice discovery across instance-level vision tasks, featuring expert-annotated and carefully refined ground-truth slices with precise grounding to local error regions. Extensive experiments on both existing benchmarks and FeSD demonstrate that SliceLens achieves state-of-the-art performance, improving Precision@10 by 0.42 (0.73 vs. 0.31) on FeSD, and identifies interpretable slices that facilitate actionable model improvements, as validated through model repair experiments.</p>
<h3 id="34-monirefer-a-real-world-large-scale-multi-modal-dataset-based-on-roadside-infrastructure-for-3d-visual-grounding">[34] <a href="https://arxiv.org/abs/2512.24605">MoniRefer: A Real-world Large-scale Multi-modal Dataset based on Roadside Infrastructure for 3D Visual Grounding</a></h3>
<p><em>Panquan Yang, Junfei Huang, Zongzhangbao Yin, Yingsong Hu, Anni Xu, Xinyi Luo, Xueqi Sun, Hai Wu, Sheng Ao, Zhaoxing Zhu, Chenglu Wen, Cheng Wang</em></p>
<h4 id="tldr_33">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†é¢å‘å®¤å¤–ç›‘æ§åœºæ™¯çš„3Dè§†è§‰å®šä½æ–°ä»»åŠ¡ï¼Œå¹¶æ„å»ºäº†é¦–ä¸ªå¤§è§„æ¨¡çœŸå®ä¸–ç•Œå¤šæ¨¡æ€æ•°æ®é›†MoniReferï¼ŒåŒæ—¶å¼€å‘äº†ç«¯åˆ°ç«¯æ–¹æ³•Moni3DVGï¼Œå®ç°äº†åŸºç¡€è®¾æ–½çº§åˆ«çš„äº¤é€šåœºæ™¯ç†è§£ã€‚</p>
<hr />
<h4 id="detailed-summary_33">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰3Dè§†è§‰å®šä½ç ”ç©¶ä¸»è¦é›†ä¸­äºå®¤å†…å’Œå®¤å¤–é©¾é©¶åœºæ™¯ï¼Œè€Œç”±äºç¼ºä¹ç”±è·¯è¾¹åŸºç¡€è®¾æ–½ä¼ æ„Ÿå™¨é‡‡é›†çš„é…å¯¹ç‚¹äº‘-æ–‡æœ¬æ•°æ®ï¼Œå®¤å¤–ç›‘æ§åœºæ™¯çš„3Dè§†è§‰å®šä½ä»»åŠ¡å°šæœªå¾—åˆ°æ¢ç´¢ï¼Œè¿™é™åˆ¶äº†åŸºç¡€è®¾æ–½çº§åˆ«å¯¹å¤æ‚äº¤é€šç¯å¢ƒçš„ç†è§£èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ç«¯åˆ°ç«¯æ–¹æ³•Moni3DVGï¼Œè¯¥æ–¹æ³•ç»¼åˆåˆ©ç”¨å›¾åƒæä¾›çš„ä¸°å¯Œå¤–è§‚ä¿¡æ¯ä»¥åŠç‚¹äº‘æä¾›çš„å‡ ä½•å’Œå…‰å­¦ä¿¡æ¯è¿›è¡Œå¤šæ¨¡æ€ç‰¹å¾å­¦ä¹ ï¼Œå®ç°äº†3Dç‰©ä½“å®šä½ï¼›åŒæ—¶æ„å»ºäº†é¦–ä¸ªçœŸå®ä¸–ç•Œå¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†MoniReferï¼ŒåŒ…å«çº¦136,018ä¸ªç‰©ä½“å’Œ411,128ä¸ªè‡ªç„¶è¯­è¨€è¡¨è¾¾ã€‚</p>
<p><strong>Result:</strong> åœ¨æå‡ºçš„åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒå’Œæ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§å’Œæœ‰æ•ˆæ€§ï¼›æ•°æ®é›†åŒ…å«æ¥è‡ªçœŸå®ä¸–ç•Œå¤æ‚äº¤é€šäº¤å‰å£çš„å¤šä¸ªåœºæ™¯æ•°æ®ï¼Œæ‰€æœ‰è¯­è¨€æè¿°å’Œ3Dæ ‡ç­¾å‡ç»è¿‡äººå·¥éªŒè¯ä»¥ç¡®ä¿è´¨é‡å’Œå‡†ç¡®æ€§ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶å¡«è¡¥äº†å®¤å¤–ç›‘æ§åœºæ™¯3Dè§†è§‰å®šä½çš„ç ”ç©¶ç©ºç™½ï¼Œä¸ºåŸºç¡€è®¾æ–½çº§åˆ«çš„äº¤é€šåœºæ™¯ç†è§£æä¾›äº†æ–°çš„è§†è§’ï¼›æå‡ºçš„æ•°æ®é›†å’Œæ–¹æ³•ä¸ºæœªæ¥ç›¸å…³ç ”ç©¶å¥ å®šäº†åŸºç¡€ï¼Œæ¨åŠ¨äº†è¶…è¶Šè‡ªæˆ‘è½¦è¾†è§†è§’çš„äº¤é€šç¯å¢ƒç†è§£èƒ½åŠ›å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_33">ğŸ“„ Abstract</h4>
<p>3D visual grounding aims to localize the object in 3D point cloud scenes that semantically corresponds to given natural language sentences. It is very critical for roadside infrastructure system to interpret natural languages and localize relevant target objects in complex traffic environments. However, most existing datasets and approaches for 3D visual grounding focus on the indoor and outdoor driving scenes, outdoor monitoring scenarios remain unexplored due to scarcity of paired point cloud-text data captured by roadside infrastructure sensors. In this paper, we introduce a novel task of 3D Visual Grounding for Outdoor Monitoring Scenarios, which enables infrastructure-level understanding of traffic scenes beyond the ego-vehicle perspective. To support this task, we construct MoniRefer, the first real-world large-scale multi-modal dataset for roadside-level 3D visual grounding. The dataset consists of about 136,018 objects with 411,128 natural language expressions collected from multiple complex traffic intersections in the real-world environments. To ensure the quality and accuracy of the dataset, we manually verified all linguistic descriptions and 3D labels for objects. Additionally, we also propose a new end-to-end method, named Moni3DVG, which utilizes the rich appearance information provided by images and geometry and optical information from point cloud for multi-modal feature learning and 3D object localization. Extensive experiments and ablation studies on the proposed benchmarks demonstrate the superiority and effectiveness of our method. Our dataset and code will be released.</p>
<h3 id="35-echofoley-event-centric-hierarchical-control-for-video-grounded-creative-sound-generation">[35] <a href="https://arxiv.org/abs/2512.24731">EchoFoley: Event-Centric Hierarchical Control for Video Grounded Creative Sound Generation</a></h3>
<p><em>Bingxuan Li, Yiming Cui, Yicheng He, Yiwei Wang, Shu Zhang, Longyin Wen, Yulei Niu</em></p>
<h4 id="tldr_34">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†EchoFoleyä»»åŠ¡ï¼Œé’ˆå¯¹è§†é¢‘æ–‡æœ¬åˆ°éŸ³é¢‘ç”Ÿæˆä¸­çš„è§†è§‰ä¸»å¯¼ã€ç»†ç²’åº¦æ§åˆ¶ç¼ºå¤±å’ŒæŒ‡ä»¤ç†è§£ä¸è¶³ä¸‰å¤§é—®é¢˜ï¼Œé€šè¿‡ç¬¦å·åŒ–å£°éŸ³äº‹ä»¶è¡¨ç¤ºå’Œæ…¢-å¿«æ€ç»´ç­–ç•¥çš„ä»£ç†ç”Ÿæˆæ¡†æ¶ï¼Œå®ç°äº†è§†é¢‘åœºæ™¯ä¸­äº‹ä»¶çº§å±€éƒ¨æ§åˆ¶å’Œå±‚æ¬¡è¯­ä¹‰æ§åˆ¶çš„éŸ³æ•ˆç”Ÿæˆã€‚</p>
<hr />
<h4 id="detailed-summary_34">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†é¢‘æ–‡æœ¬åˆ°éŸ³é¢‘ç”Ÿæˆé¢ä¸´ä¸‰ä¸ªå…³é”®é™åˆ¶ï¼šè§†è§‰ä¸æ–‡æœ¬æ¡ä»¶ä¸å¹³è¡¡å¯¼è‡´çš„è§†è§‰ä¸»å¯¼é—®é¢˜ï¼›ç¼ºä¹ç»†ç²’åº¦å¯æ§ç”Ÿæˆçš„å…·ä½“å®šä¹‰ï¼›ç°æœ‰æ•°æ®é›†ä¾èµ–ç®€çŸ­åˆ†ç±»æ ‡ç­¾å¯¼è‡´çš„æŒ‡ä»¤ç†è§£å’Œè·Ÿéšèƒ½åŠ›å¼±ã€‚è¿™äº›é™åˆ¶é˜»ç¢äº†è§†é¢‘åœºæ™¯ä¸­ç²¾ç¡®å¯æ§çš„éŸ³æ•ˆç”Ÿæˆã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å¼•å…¥äº†EchoFoleyä»»åŠ¡ï¼Œé‡‡ç”¨ç¬¦å·åŒ–å£°éŸ³äº‹ä»¶è¡¨ç¤ºæ¥æŒ‡å®šè§†é¢‘æˆ–æŒ‡ä»¤ä¸­æ¯ä¸ªå£°éŸ³äº§ç”Ÿçš„æ—¶é—´ã€å†…å®¹å’Œæ–¹å¼ï¼Œæ”¯æŒå£°éŸ³ç”Ÿæˆã€æ’å…¥å’Œç¼–è¾‘ç­‰ç»†ç²’åº¦æ§åˆ¶ã€‚åŸºäºæ­¤æ„å»ºäº†åŒ…å«6000å¤šä¸ªè§†é¢‘-æŒ‡ä»¤-æ ‡æ³¨ä¸‰å…ƒç»„çš„å¤§è§„æ¨¡ä¸“å®¶ç­–åˆ’åŸºå‡†EchoFoley-6kï¼Œå¹¶æå‡ºäº†é‡‡ç”¨æ…¢-å¿«æ€ç»´ç­–ç•¥çš„ä»¥å£°éŸ³äº‹ä»¶ä¸ºä¸­å¿ƒçš„ä»£ç†ç”Ÿæˆæ¡†æ¶EchoVidiaã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼ŒEchoVidiaåœ¨å¯æ§æ€§æ–¹é¢è¶…è¶Šäº†æœ€è¿‘çš„VT2Aæ¨¡å‹40.7%ï¼Œåœ¨æ„ŸçŸ¥è´¨é‡æ–¹é¢æå‡äº†12.5%ã€‚æ„å»ºçš„EchoFoley-6kåŸºå‡†ä¸ºè§†é¢‘éŸ³æ•ˆç”Ÿæˆæä¾›äº†å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„è®­ç»ƒå’Œè¯„ä¼°æ•°æ®ï¼Œæ”¯æŒäº‹ä»¶çº§å±€éƒ¨æ§åˆ¶å’Œå±‚æ¬¡è¯­ä¹‰æ§åˆ¶ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡ç¬¦å·åŒ–å£°éŸ³äº‹ä»¶è¡¨ç¤ºå’Œä»£ç†ç”Ÿæˆæ¡†æ¶ï¼Œè§£å†³äº†è§†é¢‘éŸ³æ•ˆç”Ÿæˆä¸­çš„ç»†ç²’åº¦æ§åˆ¶é—®é¢˜ï¼Œä¸ºå¤šæ¨¡æ€å™äº‹ä¸­çš„éŸ³æ•ˆè®¾è®¡æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚EchoFoleyä»»åŠ¡å’ŒåŸºå‡†çš„å»ºç«‹ä¸ºæœªæ¥å¯æ§éŸ³æ•ˆç”Ÿæˆç ”ç©¶å¥ å®šäº†åŸºç¡€ï¼Œæ…¢-å¿«æ€ç»´ç­–ç•¥çš„å¼•å…¥æå‡äº†æ¨¡å‹å¯¹å¤æ‚æŒ‡ä»¤çš„ç†è§£å’Œæ‰§è¡Œèƒ½åŠ›ã€‚</p>
<hr />
<h4 id="abstract_34">ğŸ“„ Abstract</h4>
<p>Sound effects build an essential layer of multimodal storytelling, shaping the emotional atmosphere and the narrative semantics of videos. Despite recent advancement in video-text-to-audio (VT2A), the current formulation faces three key limitations: First, an imbalance between visual and textual conditioning that leads to visual dominance; Second, the absence of a concrete definition for fine-grained controllable generation; Third, weak instruction understanding and following, as existing datasets rely on brief categorical tags. To address these limitations, we introduce EchoFoley, a new task designed for video-grounded sound generation with both event level local control and hierarchical semantic control. Our symbolic representation for sounding events specifies when, what, and how each sound is produced within a video or instruction, enabling fine-grained controls like sound generation, insertion, and editing. To support this task, we construct EchoFoley-6k, a large-scale, expert-curated benchmark containing over 6,000 video-instruction-annotation triplets. Building upon this foundation, we propose EchoVidia a sounding-event-centric agentic generation framework with slow-fast thinking strategy. Experiments show that EchoVidia surpasses recent VT2A models by 40.7% in controllability and 12.5% in perceptual quality.</p>
<h3 id="36-unic-lift-unified-3d-instance-segmentation-via-contrastive-learning">[36] <a href="https://arxiv.org/abs/2512.24763">UniC-Lift: Unified 3D Instance Segmentation via Contrastive Learning</a></h3>
<p><em>Ankit Dhiman, Srinath R, Jaswanth Reddy, Lokesh R Boregowda, Venkatesh Babu Radhakrishnan</em></p>
<h4 id="tldr_35">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äº3Dé«˜æ–¯æº…å°„åœºæ™¯å®ä¾‹åˆ†å‰²çš„ç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„ç‰¹å¾åµŒå…¥å’Œåˆ›æ–°çš„"åµŒå…¥åˆ°æ ‡ç­¾"è§£ç è¿‡ç¨‹ï¼Œè§£å†³äº†å¤šè§†å›¾2Dæ ‡ç­¾ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå¹¶é‡‡ç”¨è¾¹ç•Œç¡¬æŒ–æ˜ç­–ç•¥æå‡åˆ†å‰²è´¨é‡ã€‚</p>
<hr />
<h4 id="detailed-summary_35">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ–¹æ³•åœ¨å¤šè§†å›¾2Då®ä¾‹åˆ†å‰²æ‰©å±•åˆ°3Dåœºæ™¯æ—¶é¢ä¸´è§†å›¾é—´æ ‡ç­¾ä¸ä¸€è‡´çš„å…³é”®æŒ‘æˆ˜ï¼Œå¯¼è‡´3Dé¢„æµ‹è´¨é‡ä¸ä½³ã€‚ç°æœ‰è§£å†³æ–¹æ¡ˆé€šå¸¸é‡‡ç”¨ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œè¦ä¹ˆä¾èµ–è¶…å‚æ•°æ•æ„Ÿçš„å¯¹æ¯”å­¦ä¹ èšç±»ï¼Œè¦ä¹ˆéœ€è¦é¢„å¤„ç†æ ‡ç­¾ä»¥ç¡®ä¿ä¸€è‡´æ€§ï¼Œè¿™äº›æ–¹æ³•å­˜åœ¨è®­ç»ƒæ•ˆç‡ä½å’Œæ€§èƒ½å—é™çš„é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºç»Ÿä¸€æ¡†æ¶å°†åˆ†å‰²æ­¥éª¤åˆå¹¶ï¼Œåœ¨3Dé«˜æ–¯åŸºå…ƒä¸­å¼•å…¥å¯å­¦ä¹ çš„ç‰¹å¾åµŒå…¥ï¼Œé€šè¿‡åˆ›æ–°çš„"åµŒå…¥åˆ°æ ‡ç­¾"è¿‡ç¨‹é«˜æ•ˆè§£ç ä¸ºå®ä¾‹æ ‡ç­¾ã€‚ä¸ºè§£å†³å¯¹è±¡è¾¹ç•Œä¼ªå½±é—®é¢˜ï¼Œé‡‡ç”¨è¾¹ç•Œç¡¬æŒ–æ˜ç­–ç•¥ï¼Œå¹¶åœ¨ç‰¹å¾åµŒå…¥ååº”ç”¨çº¿æ€§å±‚å†è®¡ç®—ä¸‰å…ƒç»„æŸå¤±ï¼Œä»¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•åœ¨ScanNetã€Replica3Då’ŒMessy-Roomsæ•°æ®é›†ä¸Šå‡è¶…è¶Šäº†åŸºçº¿æ–¹æ³•ï¼Œåœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚è¾¹ç•Œç¡¬æŒ–æ˜ç­–ç•¥ä¸çº¿æ€§å±‚ç¨³å®šåŒ–å¤„ç†æ˜¾è‘—æå‡äº†åˆ†å‰²è´¨é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹è±¡è¾¹ç•ŒåŒºåŸŸã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†ç»Ÿä¸€æ¡†æ¶åœ¨3Dåœºæ™¯å®ä¾‹åˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡ç«¯åˆ°ç«¯ä¼˜åŒ–å‡å°‘è®­ç»ƒæ—¶é—´å¹¶æå‡æ€§èƒ½ã€‚è¾¹ç•Œå¤„ç†ç­–ç•¥ä¸º3Dåˆ†å‰²ä¸­çš„å‡ ä½•ä¸€è‡´æ€§æŒ‘æˆ˜æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œä¸ºæœªæ¥3Dåœºæ™¯ç†è§£ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒæ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_35">ğŸ“„ Abstract</h4>
<p>3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF) have advanced novel-view synthesis. Recent methods extend multi-view 2D segmentation to 3D, enabling instance/semantic segmentation for better scene understanding. A key challenge is the inconsistency of 2D instance labels across views, leading to poor 3D predictions. Existing methods use a two-stage approach in which some rely on contrastive learning with hyperparameter-sensitive clustering, while others preprocess labels for consistency. We propose a unified framework that merges these steps, reducing training time and improving performance by introducing a learnable feature embedding for segmentation in Gaussian primitives. This embedding is then efficiently decoded into instance labels through a novel "Embedding-to-Label" process, effectively integrating the optimization. While this unified framework offers substantial benefits, we observed artifacts at the object boundaries. To address the object boundary issues, we propose hard-mining samples along these boundaries. However, directly applying hard mining to the feature embeddings proved unstable. Therefore, we apply a linear layer to the rasterized feature embeddings before calculating the triplet loss, which stabilizes training and significantly improves performance. Our method outperforms baselines qualitatively and quantitatively on the ScanNet, Replica3D, and Messy-Rooms datasets.</p>
<h3 id="37-vln-mme-diagnosing-mllms-as-language-guided-visual-navigation-agents">[37] <a href="https://arxiv.org/abs/2512.24851">VLN-MME: Diagnosing MLLMs as Language-guided Visual Navigation agents</a></h3>
<p><em>Xunyi Zhao, Gengze Zhou, Qi Wu</em></p>
<h4 id="tldr_36">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†VLN-MMEè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å…·èº«å¯¼èˆªä»»åŠ¡ä¸­çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œå¹¶å‘ç°å¢å¼ºæ¨ç†é“¾å’Œè‡ªåæ€æœºåˆ¶åè€Œå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œæ­ç¤ºäº†MLLMsåœ¨ä¸‰ç»´ç©ºé—´æ¨ç†æ–¹é¢çš„å±€é™æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_36">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶ä½œä¸ºå…·èº«æ™ºèƒ½ä½“åœ¨éœ€è¦å¤šè½®å¯¹è¯ç©ºé—´æ¨ç†å’Œåºåˆ—åŠ¨ä½œé¢„æµ‹çš„å¯¼èˆªä»»åŠ¡ä¸­çš„æ€§èƒ½ä»éœ€æ·±å…¥æ¢ç´¢ï¼Œå½“å‰ç¼ºä¹ç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶æ¥ç³»ç»Ÿè¯„ä¼°MLLMsåœ¨å…·èº«å¯¼èˆªä¸­çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†VLN-MMEè¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡å°†ä¼ ç»Ÿå¯¼èˆªæ•°æ®é›†è½¬åŒ–ä¸ºæ ‡å‡†åŒ–åŸºå‡†æ¥è¯„ä¼°MLLMsä½œä¸ºé›¶æ ·æœ¬æ™ºèƒ½ä½“çš„èƒ½åŠ›ï¼Œé‡‡ç”¨é«˜åº¦æ¨¡å—åŒ–å’Œå¯è®¿é—®çš„è®¾è®¡ç®€åŒ–è¯„ä¼°æµç¨‹ï¼Œæ”¯æŒå¯¹ä¸åŒMLLMæ¶æ„ã€æ™ºèƒ½ä½“è®¾è®¡å’Œå¯¼èˆªä»»åŠ¡è¿›è¡Œç»“æ„åŒ–æ¯”è¾ƒå’Œç»„ä»¶çº§æ¶ˆèå®éªŒã€‚</p>
<p><strong>Result:</strong> å®éªŒå‘ç°å¢å¼ºåŸºçº¿æ™ºèƒ½ä½“ä½¿ç”¨æ€ç»´é“¾æ¨ç†å’Œè‡ªåæ€æœºåˆ¶åè€Œå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œè¿™è¡¨æ˜MLLMsåœ¨å…·èº«å¯¼èˆªä»»åŠ¡ä¸­è¡¨ç°å‡ºè¾ƒå·®çš„æƒ…å¢ƒæ„ŸçŸ¥èƒ½åŠ›ï¼Œè™½ç„¶èƒ½å¤Ÿéµå¾ªæŒ‡ä»¤å¹¶ç»“æ„åŒ–è¾“å‡ºï¼Œä½†å…¶ä¸‰ç»´ç©ºé—´æ¨ç†çš„ä¿çœŸåº¦è¾ƒä½ã€‚</p>
<p><strong>Conclusion:</strong> VLN-MMEä¸ºç³»ç»Ÿè¯„ä¼°é€šç”¨MLLMsåœ¨å…·èº«å¯¼èˆªç¯å¢ƒä¸­çš„èƒ½åŠ›å¥ å®šäº†åŸºç¡€ï¼Œæ­ç¤ºäº†å…¶åœ¨åºåˆ—å†³ç­–èƒ½åŠ›æ–¹é¢çš„å±€é™æ€§ï¼Œè¿™äº›å‘ç°ä¸ºMLLMsä½œä¸ºå…·èº«æ™ºèƒ½ä½“çš„åè®­ç»ƒæä¾›äº†å…³é”®æŒ‡å¯¼ï¼Œè¡¨æ˜éœ€è¦æ”¹è¿›å…¶ç©ºé—´æ¨ç†å’Œæƒ…å¢ƒæ„ŸçŸ¥èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="abstract_36">ğŸ“„ Abstract</h4>
<p>Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across a wide range of vision-language tasks. However, their performance as embodied agents, which requires multi-round dialogue spatial reasoning and sequential action prediction, needs further exploration. Our work investigates this potential in the context of Vision-and-Language Navigation (VLN) by introducing a unified and extensible evaluation framework to probe MLLMs as zero-shot agents by bridging traditional navigation datasets into a standardized benchmark, named VLN-MME. We simplify the evaluation with a highly modular and accessible design. This flexibility streamlines experiments, enabling structured comparisons and component-level ablations across diverse MLLM architectures, agent designs, and navigation tasks. Crucially, enabled by our framework, we observe that enhancing our baseline agent with Chain-of-Thought (CoT) reasoning and self-reflection leads to an unexpected performance decrease. This suggests MLLMs exhibit poor context awareness in embodied navigation tasks; although they can follow instructions and structure their output, their 3D spatial reasoning fidelity is low. VLN-MME lays the groundwork for systematic evaluation of general-purpose MLLMs in embodied navigation settings and reveals limitations in their sequential decision-making capabilities. We believe these findings offer crucial guidance for MLLM post-training as embodied agents.</p>
<h3 id="38-finmmdocr-benchmarking-financial-multimodal-reasoning-with-scenario-awareness-document-understanding-and-multi-step-computation">[38] <a href="https://arxiv.org/abs/2512.24903">FinMMDocR: Benchmarking Financial Multimodal Reasoning with Scenario Awareness, Document Understanding, and Multi-Step Computation</a></h3>
<p><em>Zichen Tang, Haihong E, Rongjin Li, Jiacheng Liu, Linwei Jia, Zhuodi Hao, Zhongjun Yang, Yuanze Li, Haolin Tian, Xinyi Hu, Peizhi Zhao, Yuan Liu, Zhengyu Wang, Xianghe Wang, Yiling Huang, Xueyuan Lin, Ruofei Bai, Zijian Xie, Qian Huang, Ruining Cao, Haocheng Gao</em></p>
<h4 id="tldr_37">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†FinMMDocRï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„åŒè¯­å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œé‡‘èæ•°å€¼æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œè¯¥åŸºå‡†åœ¨åœºæ™¯æ„ŸçŸ¥ã€æ–‡æ¡£ç†è§£å’Œå¤šæ­¥è®¡ç®—ä¸‰ä¸ªæ–¹é¢æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰åŸºå‡†ã€‚</p>
<hr />
<h4 id="detailed-summary_37">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¤„ç†çœŸå®ä¸–ç•Œé‡‘èæ•°å€¼æ¨ç†ä»»åŠ¡æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚é‡‘èåœºæ™¯ã€é•¿æ–‡æ¡£ç†è§£å’Œå¤šæ­¥éª¤æ¨ç†æ–¹é¢ç¼ºä¹è¶³å¤Ÿçš„æŒ‘æˆ˜æ€§ï¼Œæ— æ³•å……åˆ†æµ‹è¯•æ¨¡å‹åœ¨å®é™…é‡‘èåº”ç”¨ä¸­çš„èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†FinMMDocRåŸºå‡†ï¼ŒåŒ…å«1200ä¸ªä¸“å®¶æ ‡æ³¨çš„é—®é¢˜ï¼Œå…¶ä¸­57.9%èå…¥äº†12ç§éšå«é‡‘èåœºæ™¯ï¼›æ”¶é›†äº†837ä»½ä¸­è‹±æ–‡é‡‘èæ–‡æ¡£ï¼Œæ¶µç›–9ç§ç±»å‹ï¼Œå¹³å‡é•¿åº¦50.8é¡µï¼›è®¾è®¡äº†å¹³å‡éœ€è¦11æ­¥æ¨ç†çš„é—®é¢˜ç»“æ„ï¼ŒåŒ…æ‹¬5.3æ­¥ä¿¡æ¯æå–å’Œ5.7æ­¥è®¡ç®—æ­¥éª¤ï¼Œå…¶ä¸­65.0%çš„é—®é¢˜éœ€è¦è·¨é¡µè¯æ®æ”¯æŒã€‚</p>
<p><strong>Result:</strong> åœ¨FinMMDocRåŸºå‡†ä¸Šï¼Œè¡¨ç°æœ€ä½³çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä»…è¾¾åˆ°58.0%çš„å‡†ç¡®ç‡ï¼Œä¸åŒæ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•åœ¨è¯¥ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½å·®å¼‚ï¼Œè¿™çªæ˜¾äº†å½“å‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚é‡‘èå¤šæ¨¡æ€æ¨ç†ä»»åŠ¡æ—¶çš„å±€é™æ€§ã€‚</p>
<p><strong>Conclusion:</strong> FinMMDocRåŸºå‡†æ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œé‡‘èæ•°å€¼æ¨ç†ä»»åŠ¡ä¸Šçš„æ˜¾è‘—ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†éšå«åœºæ™¯ã€é•¿æ–‡æ¡£ç†è§£å’Œå¤šæ­¥éª¤è®¡ç®—æ–¹é¢ï¼Œè¯¥åŸºå‡†æœ‰æœ›æ¨åŠ¨æ¨¡å‹æ”¹è¿›å’Œæ¨ç†å¢å¼ºæ–¹æ³•åœ¨å¤æ‚å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸Šçš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_37">ğŸ“„ Abstract</h4>
<p>We introduce FinMMDocR, a novel bilingual multimodal benchmark for evaluating multimodal large language models (MLLMs) on real-world financial numerical reasoning. Compared to existing benchmarks, our work delivers three major advancements. (1) Scenario Awareness: 57.9% of 1,200 expert-annotated problems incorporate 12 types of implicit financial scenarios (e.g., Portfolio Management), challenging models to perform expert-level reasoning based on assumptions; (2) Document Understanding: 837 Chinese/English documents spanning 9 types (e.g., Company Research) average 50.8 pages with rich visual elements, significantly surpassing existing benchmarks in both breadth and depth of financial documents; (3) Multi-Step Computation: Problems demand 11-step reasoning on average (5.3 extraction + 5.7 calculation steps), with 65.0% requiring cross-page evidence (2.4 pages average). The best-performing MLLM achieves only 58.0% accuracy, and different retrieval-augmented generation (RAG) methods show significant performance variations on this task. We expect FinMMDocR to drive improvements in MLLMs and reasoning-enhanced methods on complex multimodal reasoning tasks in real-world scenarios.</p>
<h3 id="39-viper-process-aware-evaluation-for-generative-video-reasoning">[39] <a href="https://arxiv.org/abs/2512.24952">VIPER: Process-aware Evaluation for Generative Video Reasoning</a></h3>
<p><em>Yifan Li, Yukai Gu, Yingqian Min, Zikang Liu, Yifan Du, Kun Zhou, Min Yang, Wayne Xin Zhao, Minghui Qiu</em></p>
<h4 id="tldr_38">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æå‡ºäº†VIPERåŸºå‡†å’Œè¿‡ç¨‹ä¸€è‡´æ€§è¯„ä¼°èŒƒå¼ï¼Œç”¨äºè§£å†³ç”Ÿæˆå¼è§†é¢‘æ¨ç†ä¸­ç°æœ‰è¯„ä¼°æ–¹æ³•å¯¼è‡´çš„"ç»“æœæ¬ºéª—"é—®é¢˜ï¼Œé€šè¿‡åˆ†å±‚è¯„ä¼°æ¡†æ¶é‡åŒ–æ¨¡å‹æ¨ç†è¿‡ç¨‹çš„å¯é æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_38">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹å±•ç°å‡ºé“¾å¼å¸§æ¨ç†èƒ½åŠ›ï¼Œä½†å½“å‰è¯„ä¼°æ¡†æ¶ä¸»è¦ä¾èµ–å•å¸§è¯„ä¼°ï¼Œå¯¼è‡´æ¨¡å‹å¯èƒ½é€šè¿‡é”™è¯¯è¿‡ç¨‹å¾—å‡ºæ­£ç¡®ç»“è®ºçš„"ç»“æœæ¬ºéª—"é—®é¢˜ï¼Œç¼ºä¹å¯¹æ¨ç†è¿‡ç¨‹æœ‰æ•ˆæ€§çš„ç³»ç»Ÿè¯„ä¼°ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºè¿‡ç¨‹æ„ŸçŸ¥è¯„ä¼°èŒƒå¼ï¼ŒåŒ…æ‹¬VIPERåŸºå‡†ï¼ˆæ¶µç›–æ—¶é—´ã€ç»“æ„ã€ç¬¦å·ã€ç©ºé—´ã€ç‰©ç†å’Œè§„åˆ’æ¨ç†çš„16ä¸ªä»»åŠ¡ï¼‰å’Œè¿‡ç¨‹-ç»“æœä¸€è‡´æ€§æŒ‡æ ‡POC@rï¼Œè¯¥æŒ‡æ ‡åˆ©ç”¨VLM-as-Judgeåˆ†å±‚è¯„ä¼°æ¡†æ¶åŒæ—¶è¯„ä¼°ä¸­é—´æ­¥éª¤æœ‰æ•ˆæ€§å’Œæœ€ç»ˆç»“æœã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜æœ€å…ˆè¿›çš„è§†é¢‘æ¨¡å‹ä»…è¾¾åˆ°çº¦20%çš„POC@1.0åˆ†æ•°ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ç»“æœæ¬ºéª—ç°è±¡ï¼›æµ‹è¯•æ—¶ç¼©æ”¾å’Œé‡‡æ ·é²æ£’æ€§åˆ†æè¿›ä¸€æ­¥æ­ç¤ºäº†å½“å‰è§†é¢‘ç”Ÿæˆä¸çœŸæ­£å¹¿ä¹‰è§†è§‰æ¨ç†ä¹‹é—´çš„å·¨å¤§å·®è·ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†è¿‡ç¨‹è¯„ä¼°åœ¨ç”Ÿæˆå¼è§†é¢‘æ¨ç†ä¸­çš„é‡è¦æ€§ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å¯é æ¨ç†èƒ½åŠ›æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºçš„åŸºå‡†å’Œè¯„ä¼°æ¡†æ¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†ç³»ç»Ÿæ€§è¯„ä¼°å·¥å…·ï¼Œæ¨åŠ¨å‘æ›´å¯é çš„è§†è§‰æ¨ç†ç³»ç»Ÿå‘å±•ã€‚</p>
<hr />
<h4 id="abstract_38">ğŸ“„ Abstract</h4>
<p>Recent breakthroughs in video generation have demonstrated an emerging capability termed Chain-of-Frames (CoF) reasoning, where models resolve complex tasks through the generation of continuous frames. While these models show promise for Generative Video Reasoning (GVR), existing evaluation frameworks often rely on single-frame assessments, which can lead to outcome-hacking, where a model reaches a correct conclusion through an erroneous process. To address this, we propose a process-aware evaluation paradigm. We introduce VIPER, a comprehensive benchmark spanning 16 tasks across temporal, structural, symbolic, spatial, physics, and planning reasoning. Furthermore, we propose Process-outcome Consistency (POC@r), a new metric that utilizes VLM-as-Judge with a hierarchical rubric to evaluate both the validity of the intermediate steps and the final result. Our experiments reveal that state-of-the-art video models achieve only about 20% POC@1.0 and exhibit a significant outcome-hacking. We further explore the impact of test-time scaling and sampling robustness, highlighting a substantial gap between current video generation and true generalized visual reasoning. Our benchmark will be publicly released.</p>
<h3 id="40-gamo-geometry-aware-multi-view-diffusion-outpainting-for-sparse-view-3d-reconstruction">[40] <a href="https://arxiv.org/abs/2512.25073">GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction</a></h3>
<p><em>Yi-Chuan Huang, Hao-Jen Chien, Chin-Yang Lin, Ying-Huan Chen, Yu-Lun Liu</em></p>
<h4 id="tldr_39">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºGaMOï¼ˆå‡ ä½•æ„ŸçŸ¥å¤šè§†è§’å¤–æ¨ï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¤šè§†è§’å¤–æ¨è€Œéç”Ÿæˆæ–°è§†è§’æ¥æ”¹è¿›ç¨€ç–è§†å›¾3Dé‡å»ºï¼Œåœ¨ä¿æŒå‡ ä½•ä¸€è‡´æ€§çš„åŒæ—¶æä¾›æ›´å¹¿çš„åœºæ™¯è¦†ç›–ï¼Œç›¸æ¯”ç°æœ‰æ–¹æ³•å®ç°äº†25å€åŠ é€Ÿå’Œæ›´ä¼˜çš„é‡å»ºè´¨é‡ã€‚</p>
<hr />
<h4 id="detailed-summary_39">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰åŸºäºæ‰©æ•£æ¨¡å‹çš„ç¨€ç–è§†å›¾3Dé‡å»ºæ–¹æ³•å­˜åœ¨ä¸‰ä¸ªå…³é”®å±€é™ï¼šå·²çŸ¥è§†è§’å¤–å›´è¦†ç›–ä¸è¶³ã€ç”Ÿæˆè§†å›¾é—´çš„å‡ ä½•ä¸ä¸€è‡´æ€§ä»¥åŠè®¡ç®—æˆæœ¬é«˜æ˜‚çš„æµç¨‹ã€‚è¿™äº›é™åˆ¶é˜»ç¢äº†ä»æœ‰é™è¾“å…¥è§†å›¾å®ç°é«˜è´¨é‡åœºæ™¯é‡å»ºçš„å®é™…åº”ç”¨ã€‚</p>
<p><strong>Method:</strong> GaMOæ¡†æ¶é€šè¿‡å¤šè§†è§’å¤–æ¨é‡æ–°å®šä¹‰ç¨€ç–è§†å›¾é‡å»ºé—®é¢˜ï¼Œä»ç°æœ‰ç›¸æœºä½å§¿æ‰©å±•è§†é‡è€Œéç”Ÿæˆæ–°è§†è§’ï¼Œä»è€Œå›ºæœ‰åœ°ä¿æŒå‡ ä½•ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¤šè§†è§’æ¡ä»¶å’Œå‡ ä½•æ„ŸçŸ¥å»å™ªç­–ç•¥ï¼Œä»¥é›¶æ ·æœ¬æ–¹å¼è¿è¡Œæ— éœ€é¢å¤–è®­ç»ƒã€‚</p>
<p><strong>Result:</strong> åœ¨Replicaå’ŒScanNet++æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒGaMOåœ¨3ã€6ã€9ä¸ªè¾“å…¥è§†å›¾ä¸‹å‡è¾¾åˆ°æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡ï¼Œåœ¨PSNRå’ŒLPIPSæŒ‡æ ‡ä¸Šè¶…è¶Šå…ˆå‰æ–¹æ³•ï¼ŒåŒæ—¶ç›¸æ¯”SOTAæ‰©æ•£æ–¹æ³•å®ç°25å€åŠ é€Ÿï¼Œå¤„ç†æ—¶é—´ä½äº10åˆ†é’Ÿã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡å¤šè§†è§’å¤–æ¨è€Œéæ–°è§†è§’ç”Ÿæˆæ¥æ”¹è¿›ç¨€ç–è§†å›¾é‡å»ºçš„æœ‰æ•ˆæ€§ï¼Œæä¾›äº†ä¸€ç§è®¡ç®—é«˜æ•ˆä¸”å‡ ä½•ä¸€è‡´çš„æ–¹æ³•ã€‚GaMOçš„æˆåŠŸè¡¨æ˜é‡æ–°å®šä¹‰é—®é¢˜è¡¨è¿°å¯ä»¥å…‹æœç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œä¸ºå®é™…åº”ç”¨ä¸­çš„å¿«é€Ÿé«˜è´¨é‡3Dé‡å»ºå¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_39">ğŸ“„ Abstract</h4>
<p>Recent advances in 3D reconstruction have achieved remarkable progress in high-quality scene capture from dense multi-view imagery, yet struggle when input views are limited. Various approaches, including regularization techniques, semantic priors, and geometric constraints, have been implemented to address this challenge. Latest diffusion-based methods have demonstrated substantial improvements by generating novel views from new camera poses to augment training data, surpassing earlier regularization and prior-based techniques. Despite this progress, we identify three critical limitations in these state-of-the-art approaches: inadequate coverage beyond known view peripheries, geometric inconsistencies across generated views, and computationally expensive pipelines. We introduce GaMO (Geometry-aware Multi-view Outpainter), a framework that reformulates sparse-view reconstruction through multi-view outpainting. Instead of generating new viewpoints, GaMO expands the field of view from existing camera poses, which inherently preserves geometric consistency while providing broader scene coverage. Our approach employs multi-view conditioning and geometry-aware denoising strategies in a zero-shot manner without training. Extensive experiments on Replica and ScanNet++ demonstrate state-of-the-art reconstruction quality across 3, 6, and 9 input views, outperforming prior methods in PSNR and LPIPS, while achieving a $25\times$ speedup over SOTA diffusion-based methods with processing time under 10 minutes. Project page: https://yichuanh.github.io/GaMO/</p>
<div id='cs.CL'></div>

<h1 id="cscl-back">cs.CL <a href="#toc">[Back]</a></h1>
<h3 id="41-break-out-the-silverware-semantic-understanding-of-stored-household-items">[41] <a href="https://arxiv.org/abs/2512.23739">Break Out the Silverware -- Semantic Understanding of Stored Household Items</a></h3>
<p><em>Michaela Levi-Richter, Reuth Mirsky, Oren Glickman</em></p>
<h4 id="tldr_40">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†å­˜å‚¨å®¶å±…ç‰©å“æŒ‘æˆ˜åŸºå‡†ï¼Œç”¨äºè¯„ä¼°æœåŠ¡æœºå™¨äººåœ¨ä¸å¯è§ç‰©å“å®šä½æ–¹é¢çš„è®¤çŸ¥èƒ½åŠ›ï¼Œå¹¶å¼€å‘äº†NOAMæ··åˆä»£ç†ç®¡é“ï¼Œè¯¥ç®¡é“ç»“åˆäº†ç»“æ„åŒ–åœºæ™¯ç†è§£ä¸å¤§è¯­è¨€æ¨¡å‹æ¨ç†ï¼Œæ˜¾è‘—æå‡äº†é¢„æµ‹å‡†ç¡®æ€§å¹¶æ¥è¿‘äººç±»æ°´å¹³ã€‚</p>
<hr />
<h4 id="detailed-summary_40">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡è®¡ç®—æœºè§†è§‰å’Œæœºæ¢°æ“ä½œæŠ€æœ¯å·²å–å¾—è¿›å±•ï¼Œä½†æœåŠ¡æœºå™¨äººä»ç¼ºä¹æ¨æ–­æ—¥å¸¸ç‰©å“å­˜å‚¨ä½ç½®çš„å¸¸è¯†æ¨ç†èƒ½åŠ›ï¼Œè¿™äº›ç‰©å“é€šå¸¸éšè—åœ¨æŠ½å±‰ã€æ©±æŸœæˆ–å£æ©±ä¸­ï¼Œæ— æ³•ç›´æ¥è§‚å¯Ÿã€‚ç°æœ‰ç³»ç»Ÿéš¾ä»¥å®Œæˆ"ç»™æˆ‘æ‹¿ä¸ªç›˜å­"è¿™ç±»ç®€å•æŒ‡ä»¤ï¼Œå› æ­¤éœ€è¦å¼€å‘èƒ½å¤Ÿç†è§£å®¶åº­ç©ºé—´ç»„ç»‡é€»è¾‘çš„è®¤çŸ¥èƒ½åŠ›åŸºå‡†ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†å­˜å‚¨å®¶å±…ç‰©å“æŒ‘æˆ˜åŸºå‡†ï¼ŒåŒ…å«ä¸¤ä¸ªæ•°æ®é›†ï¼š100ä¸ªç‰©å“-å›¾åƒå¯¹çš„çœŸå®ä¸–ç•Œè¯„ä¼°é›†å’Œ6500ä¸ªå¸¦å­˜å‚¨å¤šè¾¹å½¢æ ‡æ³¨çš„å…¬å¼€å¨æˆ¿å›¾åƒå¼€å‘é›†ã€‚ä¸ºè§£å†³è¯¥æŒ‘æˆ˜ï¼Œå¼€å‘äº†NOAMæ··åˆä»£ç†ç®¡é“ï¼Œè¯¥ç®¡é“å°†è§†è§‰è¾“å…¥è½¬æ¢ä¸ºç©ºé—´ä¸Šä¸‹æ–‡å’Œå¯è§å®¹å™¨çš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œç„¶åæç¤ºå¤§è¯­è¨€æ¨¡å‹æ¨æ–­æœ€å¯èƒ½çš„éšè—å­˜å‚¨ä½ç½®ï¼Œå®ç°äº†ç»“æ„åŒ–åœºæ™¯ç†è§£ä¸è¯­è¨€æ¨¡å‹æ¨ç†çš„ç»“åˆã€‚</p>
<p><strong>Result:</strong> NOAMåœ¨é¢„æµ‹å‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºéšæœºé€‰æ‹©ã€è§†è§‰è¯­è¨€ç®¡é“ã€é¢†å…ˆçš„å¤šæ¨¡æ€æ¨¡å‹ç­‰åŸºçº¿æ–¹æ³•ï¼Œå¹¶æ¥è¿‘äººç±»è¡¨ç°æ°´å¹³ã€‚è¯„ä¼°è¡¨æ˜è¯¥é›†æˆè§†è§‰è¯­è¨€ä»£ç†å±•ç°å‡ºæ–°å…´çš„å¸¸è¯†æ¨ç†èƒ½åŠ›ï¼Œä¸ºå®¶åº­ç¯å¢ƒä¸­çš„è®¤çŸ¥èƒ½åŠ›ä»£ç†éƒ¨ç½²æä¾›äº†æœ€ä½³å®è·µã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†ç»“åˆç»“æ„åŒ–åœºæ™¯ç†è§£ä¸å¤§è¯­è¨€æ¨¡å‹æ¨ç†åœ¨æœºå™¨äººå¸¸è¯†æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæœåŠ¡æœºå™¨äººåœ¨å®¶åº­ç¯å¢ƒä¸­çš„è®¤çŸ¥èƒ½åŠ›è¯„ä¼°å’Œéƒ¨ç½²æä¾›äº†æ ‡å‡†åŒ–åŸºå‡†ã€‚NOAMçš„æ¨¡å—åŒ–è®¾è®¡ä½¿å…¶èƒ½å¤Ÿé›†æˆåˆ°æ›´å¹¿æ³›çš„æœºå™¨äººç³»ç»Ÿä¸­ï¼Œæ¨åŠ¨äº†å®¶åº­æœåŠ¡æœºå™¨äººå‘æ›´æ™ºèƒ½ã€æ›´è‡ªä¸»çš„æ–¹å‘å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_40">ğŸ“„ Abstract</h4>
<p>``Bring me a plate.'' For domestic service robots, this simple command reveals a complex challenge: inferring where everyday items are stored, often out of sight in drawers, cabinets, or closets. Despite advances in vision and manipulation, robots still lack the commonsense reasoning needed to complete this task. We introduce the Stored Household Item Challenge, a benchmark task for evaluating service robots' cognitive capabilities: given a household scene and a queried item, predict its most likely storage location.
  Our benchmark includes two datasets: (1) a real-world evaluation set of 100 item-image pairs with human-annotated ground truth from participants' kitchens, and (2) a development set of 6,500 item-image pairs annotated with storage polygons over public kitchen images. These datasets support realistic modeling of household organization and enable comparative evaluation across agent architectures.
  To begin tackling this challenge, we introduce NOAM (Non-visible Object Allocation Model), a hybrid agent pipeline that combines structured scene understanding with large language model inference. NOAM converts visual input into natural language descriptions of spatial context and visible containers, then prompts a language model (e.g., GPT-4) to infer the most likely hidden storage location. This integrated vision-language agent exhibits emergent commonsense reasoning and is designed for modular deployment within broader robotic systems.
  We evaluate NOAM against baselines including random selection, vision-language pipelines (Grounding-DINO + SAM), leading multimodal models (e.g., Gemini, GPT-4o, Kosmos-2, LLaMA, Qwen), and human performance. NOAM significantly improves prediction accuracy and approaches human-level results, highlighting best practices for deploying cognitively capable agents in domestic environments.</p>
<h3 id="42-automated-analysis-of-sustainability-reports-using-large-language-models-for-the-extraction-and-prediction-of-eu-taxonomy-compliant-kpis">[42] <a href="https://arxiv.org/abs/2512.24289">Automated Analysis of Sustainability Reports: Using Large Language Models for the Extraction and Prediction of EU Taxonomy-Compliant KPIs</a></h3>
<p><em>Jonathan Schmoll, Adam Jatowt</em></p>
<h4 id="tldr_41">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶é’ˆå¯¹æ¬§ç›Ÿåˆ†ç±»æ³•åˆè§„æµç¨‹è‡ªåŠ¨åŒ–ç¼ºä¹å…¬å¼€åŸºå‡†æ•°æ®é›†çš„é—®é¢˜ï¼Œæ„å»ºäº†é¦–ä¸ªç»“æ„åŒ–æ•°æ®é›†å¹¶ç³»ç»Ÿè¯„ä¼°äº†å¤§è¯­è¨€æ¨¡å‹åœ¨åˆè§„å·¥ä½œæµä¸­çš„è¡¨ç°ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨å®šæ€§ä¸å®šé‡ä»»åŠ¡ä¸Šçš„æ˜¾è‘—æ€§èƒ½å·®è·ã€‚</p>
<hr />
<h4 id="detailed-summary_41">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ¬§ç›Ÿåˆ†ç±»æ³•åˆè§„æµç¨‹çš„æ‰‹åŠ¨æ“ä½œèµ„æºå¯†é›†ä¸”æ•ˆç‡ä½ä¸‹ï¼Œè€Œå¤§è¯­è¨€æ¨¡å‹è‡ªåŠ¨åŒ–ç ”ç©¶å› ç¼ºä¹å…¬å¼€åŸºå‡†æ•°æ®é›†è€Œå—é˜»ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½å¹¶é¦–æ¬¡ç³»ç»Ÿè¯„ä¼°LLMåœ¨æ ¸å¿ƒåˆè§„å·¥ä½œæµä¸­çš„èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æ„å»ºäº†åŒ…å«190ä»½å…¬å¸æŠ¥å‘Šçš„æ–°å‹ç»“æ„åŒ–æ•°æ®é›†ï¼ŒåŒ…å«çœŸå®ç»æµæ´»åŠ¨å’Œç»æµç»©æ•ˆæŒ‡æ ‡æ ‡æ³¨ï¼Œé‡‡ç”¨å¤šæ­¥éª¤æ™ºèƒ½ä½“æ¡†æ¶è¿›è¡Œå®šæ€§ä»»åŠ¡è¯„ä¼°ï¼Œå¹¶åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹æµ‹è¯•æ¨¡å‹å®šé‡é¢„æµ‹èƒ½åŠ›ï¼Œç³»ç»Ÿæ¯”è¾ƒäº†ä¸åŒè¾“å…¥æ ¼å¼ï¼ˆç®€æ´å…ƒæ•°æ®ä¸å®Œæ•´éç»“æ„åŒ–æŠ¥å‘Šï¼‰çš„æ€§èƒ½å·®å¼‚ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºå®šæ€§ä¸å®šé‡ä»»åŠ¡å­˜åœ¨æ˜æ˜¾æ€§èƒ½å·®è·ï¼šLLMåœ¨è¯†åˆ«ç»æµæ´»åŠ¨çš„å®šæ€§ä»»åŠ¡ä¸­è¡¨ç°ä¸­ç­‰ï¼Œå¤šæ­¥éª¤æ™ºèƒ½ä½“æ¡†æ¶ä»…ç•¥å¾®æå‡ç²¾åº¦ï¼›è€Œåœ¨é¢„æµ‹è´¢åŠ¡KPIçš„å®šé‡ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹å®Œå…¨å¤±è´¥ã€‚ç ”ç©¶è¿˜å‘ç°ç®€æ´å…ƒæ•°æ®å¾€å¾€ä¼˜äºå®Œæ•´éç»“æ„åŒ–æŠ¥å‘Šçš„çŸ›ç›¾ç°è±¡ï¼Œä¸”æ¨¡å‹ç½®ä¿¡åº¦åˆ†æ•°æ ¡å‡†æ•ˆæœä¸ä½³ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜å¤§è¯­è¨€æ¨¡å‹å°šæœªå‡†å¤‡å¥½å®ç°å®Œå…¨è‡ªåŠ¨åŒ–åˆè§„æµç¨‹ï¼Œä½†å¯ä½œä¸ºäººç±»ä¸“å®¶çš„æœ‰åŠ›è¾…åŠ©å·¥å…·ã€‚å…¬å¼€æ•°æ®é›†ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†åŸºå‡†ï¼ŒåŒæ—¶æ­ç¤ºäº†æ¨¡å‹åœ¨å¤æ‚å®šé‡æ¨ç†ä»»åŠ¡ä¸Šçš„æ ¹æœ¬å±€é™æ€§ä»¥åŠè¾“å…¥æ ¼å¼ä¼˜åŒ–çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_41">ğŸ“„ Abstract</h4>
<p>The manual, resource-intensive process of complying with the EU Taxonomy presents a significant challenge for companies. While Large Language Models (LLMs) offer a path to automation, research is hindered by a lack of public benchmark datasets. To address this gap, we introduce a novel, structured dataset from 190 corporate reports, containing ground-truth economic activities and quantitative Key Performance Indicators (KPIs). We use this dataset to conduct the first systematic evaluation of LLMs on the core compliance workflow. Our results reveal a clear performance gap between qualitative and quantitative tasks. LLMs show moderate success in the qualitative task of identifying economic activities, with a multi-step agentic framework modestly enhancing precision. Conversely, the models comprehensively fail at the quantitative task of predicting financial KPIs in a zero-shot setting. We also discover a paradox, where concise metadata often yields superior performance to full, unstructured reports, and find that model confidence scores are poorly calibrated. We conclude that while LLMs are not ready for full automation, they can serve as powerful assistive tools for human experts. Our dataset provides a public benchmark for future research.</p>
<h3 id="43-figure-it-out-improving-the-frontier-of-reasoning-with-active-visual-thinking">[43] <a href="https://arxiv.org/abs/2512.24297">Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking</a></h3>
<p><em>Meiqi Chen, Fandong Meng, Jie Zhou</em></p>
<h4 id="tldr_42">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†FIGRæ¡†æ¶ï¼Œé€šè¿‡ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ å°†ä¸»åŠ¨è§†è§‰æ€ç»´é›†æˆåˆ°å¤šè½®æ¨ç†ä¸­ï¼Œåˆ©ç”¨è§†è§‰è¡¨ç¤ºå¤–éƒ¨åŒ–ä¸­é—´ç»“æ„å‡è®¾ï¼Œåœ¨å¤æ‚æ•°å­¦æ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºçº¯æ–‡æœ¬é“¾å¼æ€ç»´åŸºçº¿ã€‚</p>
<hr />
<h4 id="detailed-summary_42">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤æ‚æ¨ç†é—®é¢˜é€šå¸¸æ¶‰åŠéšå¼çš„ç©ºé—´ã€å‡ ä½•å’Œç»“æ„å…³ç³»ï¼Œè¿™äº›å…³ç³»æ— æ³•åœ¨æ–‡æœ¬ä¸­æ˜¾å¼ç¼–ç ï¼Œè€Œçº¯æ–‡æœ¬æ¨ç†éš¾ä»¥è¡¨ç¤ºå¤æ‚åœºæ™¯ä¸­çš„å…¨å±€ç»“æ„çº¦æŸï¼Œå¯¼è‡´åœ¨éœ€è¦ç»“æ„åŒ–æ€ç»´çš„ä»»åŠ¡ä¸Šè¡¨ç°å—é™ã€‚</p>
<p><strong>Method:</strong> FIGRæ¡†æ¶é€šè¿‡ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ å°†ä¸»åŠ¨è§†è§‰æ€ç»´é›†æˆåˆ°å¤šè½®æ¨ç†è¿‡ç¨‹ä¸­ï¼Œåœ¨é—®é¢˜æ±‚è§£æ—¶åŠ¨æ€æ„å»ºè§†è§‰è¡¨ç¤ºæ¥å¤–éƒ¨åŒ–ä¸­é—´ç»“æ„å‡è®¾ï¼Œå¹¶è‡ªé€‚åº”åœ°è°ƒèŠ‚è§†è§‰æ¨ç†çš„è°ƒç”¨æ—¶æœºå’Œæ–¹å¼ï¼Œä»è€Œå®ç°å¯¹å…¨å±€ç»“æ„å±æ€§çš„ç¨³å®šè¿è´¯æ¨ç†ã€‚</p>
<p><strong>Result:</strong> åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒFIGRæ˜¾è‘—ä¼˜äºå¼ºå¤§çš„çº¯æ–‡æœ¬é“¾å¼æ€ç»´åŸºçº¿ï¼Œåœ¨AIME 2025ä¸Šæ¯”åŸºç¡€æ¨¡å‹æå‡13.12%ï¼Œåœ¨BeyondAIMEä¸Šæå‡11.00%ï¼Œè¯æ˜äº†è§†è§‰å¼•å¯¼å¤šæ¨¡æ€æ¨ç†åœ¨å¢å¼ºå¤æ‚æ¨ç†ç¨³å®šæ€§å’Œå¯é æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜å°†è§†è§‰è¡¨ç¤ºæ•´åˆåˆ°æ¨ç†è¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰æ–‡æœ¬éš¾ä»¥è¡¨è¾¾çš„å…¨å±€ç»“æ„çº¦æŸï¼Œä¸»åŠ¨è§†è§‰æ€ç»´æœºåˆ¶ä¸ºå¤æ‚æ¨ç†ä»»åŠ¡æä¾›äº†æ›´ç¨³å®šå¯é çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºå¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿçš„å‘å±•å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_42">ğŸ“„ Abstract</h4>
<p>Complex reasoning problems often involve implicit spatial, geometric, and structural relationships that are not explicitly encoded in text. While recent reasoning models have achieved strong performance across many domains, purely text-based reasoning struggles to represent global structural constraints in complex settings. In this paper, we introduce FIGR, which integrates active visual thinking into multi-turn reasoning via end-to-end reinforcement learning. FIGR externalizes intermediate structural hypotheses by constructing visual representations during problem solving. By adaptively regulating when and how visual reasoning should be invoked, FIGR enables more stable and coherent reasoning over global structural properties that are difficult to capture from text alone. Experiments on challenging mathematical reasoning benchmarks demonstrate that FIGR outperforms strong text-only chain-of-thought baselines. In particular, FIGR improves the base model by 13.12% on AIME 2025 and 11.00% on BeyondAIME, highlighting the effectiveness of figure-guided multimodal reasoning in enhancing the stability and reliability of complex reasoning.</p>
<h3 id="44-skim-aware-contrastive-learning-for-efficient-document-representation">[44] <a href="https://arxiv.org/abs/2512.24373">Skim-Aware Contrastive Learning for Efficient Document Representation</a></h3>
<p><em>Waheed Ahmed Abro, Zied Bouraoui</em></p>
<h4 id="tldr_43">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ çš„æ–°æ¡†æ¶ï¼Œç”¨äºå¢å¼ºé•¿æ–‡æ¡£è¡¨ç¤ºï¼Œè¯¥æ–¹æ³•å—äººç±»ç•¥è¯»ç­–ç•¥å¯å‘ï¼Œé€šè¿‡éšæœºæ©ç æ–‡æ¡£ç‰‡æ®µå¹¶åˆ©ç”¨è‡ªç„¶è¯­è¨€æ¨ç†å¯¹æ¯”ç›®æ ‡æ¥å¯¹é½ç›¸å…³éƒ¨åˆ†ï¼Œåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶æ˜¾è‘—æå‡äº†æ³•å¾‹å’Œç”Ÿç‰©åŒ»å­¦æ–‡æœ¬çš„è¡¨ç¤ºè´¨é‡ã€‚</p>
<hr />
<h4 id="detailed-summary_43">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡åŸºäºTransformerçš„æ¨¡å‹åœ¨è¯çº§å’Œå¥çº§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ³•å¾‹å’ŒåŒ»å­¦ç­‰é¢†åŸŸæœ‰æ•ˆè¡¨ç¤ºé•¿æ–‡æ¡£ä»ç„¶å›°éš¾ã€‚ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶è™½ç„¶èƒ½å¤„ç†æ›´é•¿è¾“å…¥ï¼Œä½†è®¡ç®—èµ„æºå¯†é›†ä¸”éš¾ä»¥æ•è·å®Œæ•´æ–‡æ¡£ä¸Šä¸‹æ–‡ï¼›åˆ†å±‚Transformeræ¨¡å‹æ•ˆç‡æ›´é«˜ï¼Œä½†æ— æ³•æ¸…æ™°è§£é‡Šæ–‡æ¡£ä¸åŒéƒ¨åˆ†ä¹‹é—´çš„å…³ç³»ã€‚äººç±»é€šå¸¸é€šè¿‡ç•¥è¯»æ–‡æœ¬ã€èšç„¦é‡è¦éƒ¨åˆ†æ¥ç†è§£æ•´ä½“å†…å®¹ï¼Œè¿™ä¸€ç­–ç•¥å¯å‘äº†æœ¬ç ”ç©¶ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œå—äººç±»ç•¥è¯»ç­–ç•¥å¯å‘ï¼Œé€šè¿‡éšæœºæ©ç æ–‡æ¡£ç‰‡æ®µå¹¶ä½¿ç”¨åŸºäºè‡ªç„¶è¯­è¨€æ¨ç†çš„å¯¹æ¯”ç›®æ ‡æ¥å¯¹é½ç›¸å…³éƒ¨åˆ†ï¼ŒåŒæ—¶ä½¿å…¶ä¸ä¸ç›¸å…³éƒ¨åˆ†ä¿æŒè·ç¦»ã€‚è¯¥æ–¹æ³•æ¨¡æ‹Ÿäººç±»ä¿¡æ¯åˆæˆè¿‡ç¨‹ï¼Œæ—¨åœ¨ç”Ÿæˆæ—¢ä¸°å¯Œåˆè®¡ç®—é«˜æ•ˆçš„æ–‡æ¡£è¡¨ç¤ºï¼Œç‰¹åˆ«é’ˆå¯¹æ³•å¾‹å’Œç”Ÿç‰©åŒ»å­¦ç­‰é¢†åŸŸçš„ä¸“ä¸šé•¿æ–‡æ¡£ã€‚</p>
<p><strong>Result:</strong> åœ¨æ³•å¾‹å’Œç”Ÿç‰©åŒ»å­¦æ–‡æœ¬ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢å‡å–å¾—æ˜¾è‘—æå‡ã€‚è¯¥æ–¹æ³•ä¸ä»…æé«˜äº†æ–‡æ¡£è¡¨ç¤ºçš„è´¨é‡ï¼Œè¿˜ä¿æŒäº†è®¡ç®—æ•ˆç‡ï¼Œç›¸æ¯”ä¼ ç»Ÿç¨€ç–æ³¨æ„åŠ›æœºåˆ¶å’Œåˆ†å±‚Transformeræ¨¡å‹è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†å—äººç±»è®¤çŸ¥ç­–ç•¥å¯å‘çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨é•¿æ–‡æ¡£è¡¨ç¤ºä¸­çš„æ½œåŠ›ï¼Œä¸ºå¤„ç†ä¸“ä¸šé¢†åŸŸé•¿æ–‡æœ¬æä¾›äº†æ–°çš„æœ‰æ•ˆé€”å¾„ã€‚é€šè¿‡æ¨¡æ‹Ÿäººç±»ç•¥è¯»å’Œä¿¡æ¯åˆæˆè¿‡ç¨‹ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„æ–‡æ¡£è¡¨ç¤ºæ—¢ä¸°å¯Œåˆé«˜æ•ˆï¼Œä¸ºæœªæ¥é•¿æ–‡æ¡£ç†è§£ç ”ç©¶æä¾›äº†é‡è¦å¯ç¤ºã€‚</p>
<hr />
<h4 id="abstract_43">ğŸ“„ Abstract</h4>
<p>Although transformer-based models have shown strong performance in word- and sentence-level tasks, effectively representing long documents, especially in fields like law and medicine, remains difficult. Sparse attention mechanisms can handle longer inputs, but are resource-intensive and often fail to capture full-document context. Hierarchical transformer models offer better efficiency but do not clearly explain how they relate different sections of a document. In contrast, humans often skim texts, focusing on important sections to understand the overall message. Drawing from this human strategy, we introduce a new self-supervised contrastive learning framework that enhances long document representation. Our method randomly masks a section of the document and uses a natural language inference (NLI)-based contrastive objective to align it with relevant parts while distancing it from unrelated ones. This mimics how humans synthesize information, resulting in representations that are both richer and more computationally efficient. Experiments on legal and biomedical texts confirm significant gains in both accuracy and efficiency.</p>
<h3 id="45-comparing-approaches-to-automatic-summarization-in-less-resourced-languages">[45] <a href="https://arxiv.org/abs/2512.24410">Comparing Approaches to Automatic Summarization in Less-Resourced Languages</a></h3>
<p><em>Chester Palen-Michel, Constantine Lignos</em></p>
<h4 id="tldr_44">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶ç³»ç»Ÿæ¯”è¾ƒäº†ä½èµ„æºè¯­è¨€è‡ªåŠ¨æ–‡æœ¬æ‘˜è¦çš„å¤šç§æ–¹æ³•ï¼Œå‘ç°å¤šè¯­è¨€å¾®è°ƒçš„mT5æ¨¡å‹åœ¨å¤§å¤šæ•°æŒ‡æ ‡ä¸Šä¼˜äºé›¶æ ·æœ¬LLMæ–¹æ³•ï¼Œå¹¶æ­ç¤ºäº†LLMä½œä¸ºè¯„ä¼°å™¨åœ¨ä½èµ„æºè¯­è¨€ä¸­å¯èƒ½ä¸å¯é çš„é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_44">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è‡ªåŠ¨æ–‡æœ¬æ‘˜è¦åœ¨è‹±è¯­ç­‰é«˜èµ„æºè¯­è¨€ä¸­å·²å–å¾—é«˜æ€§èƒ½ï¼Œä½†å¯¹ä½èµ„æºè¯­è¨€çš„æ‘˜è¦ç ”ç©¶å…³æ³¨è¾ƒå°‘ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨ç³»ç»Ÿæ¯”è¾ƒä½èµ„æºè¯­è¨€æ‘˜è¦çš„å„ç§æ–¹æ³•ï¼Œå¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æ¯”è¾ƒäº†å¤šç§æ–¹æ³•ï¼šåŒ…æ‹¬ä¸åŒè§„æ¨¡LLMçš„é›¶æ ·æœ¬æç¤ºã€å¸¦ä¸‰ç§æ•°æ®å¢å¼ºå’Œä¸å¸¦å¢å¼ºçš„mT5å¾®è°ƒã€å¤šè¯­è¨€è¿ç§»ï¼Œä»¥åŠLLMç¿»è¯‘ç®¡é“æ–¹æ³•ï¼ˆæºè¯­è¨€â†’è‹±è¯­â†’æ‘˜è¦â†’ç¿»è¯‘å›æºè¯­è¨€ï¼‰ã€‚</p>
<p><strong>Result:</strong> ä½¿ç”¨äº”ç§ä¸åŒæŒ‡æ ‡è¯„ä¼°å‘ç°ï¼šç›¸ä¼¼å‚æ•°è§„æ¨¡çš„LLMæ€§èƒ½å­˜åœ¨å·®å¼‚ï¼›å¤šè¯­è¨€å¾®è°ƒçš„mT5åŸºçº¿åœ¨å¤§å¤šæ•°æŒ‡æ ‡ä¸Šä¼˜äºåŒ…æ‹¬é›¶æ ·æœ¬LLMåœ¨å†…çš„å¤§å¤šæ•°æ–¹æ³•ï¼›LLMä½œä¸ºè¯„ä¼°å™¨åœ¨ä½èµ„æºè¯­è¨€ä¸­å¯èƒ½ä¸å¤ªå¯é ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜å¯¹äºä½èµ„æºè¯­è¨€æ‘˜è¦ä»»åŠ¡ï¼Œå¤šè¯­è¨€å¾®è°ƒæ–¹æ³•æ¯”é›¶æ ·æœ¬LLMæ–¹æ³•æ›´æœ‰æ•ˆï¼ŒåŒæ—¶æ­ç¤ºäº†å½“å‰è¯„ä¼°æ–¹æ³•åœ¨ä½èµ„æºè¯­è¨€ä¸­çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†é‡è¦æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_44">ğŸ“„ Abstract</h4>
<p>Automatic text summarization has achieved high performance in high-resourced languages like English, but comparatively less attention has been given to summarization in less-resourced languages. This work compares a variety of different approaches to summarization from zero-shot prompting of LLMs large and small to fine-tuning smaller models like mT5 with and without three data augmentation approaches and multilingual transfer. We also explore an LLM translation pipeline approach, translating from the source language to English, summarizing and translating back. Evaluating with five different metrics, we find that there is variation across LLMs in their performance across similar parameter sizes, that our multilingual fine-tuned mT5 baseline outperforms most other approaches including zero-shot LLM performance for most metrics, and that LLM as judge may be less reliable on less-resourced languages.</p>
<h3 id="46-safe-in-the-future-dangerous-in-the-past-dissecting-temporal-and-linguistic-vulnerabilities-in-llms">[46] <a href="https://arxiv.org/abs/2512.24556">Safe in the Future, Dangerous in the Past: Dissecting Temporal and Linguistic Vulnerabilities in LLMs</a></h3>
<p><em>Muhammad Abdullahi Said, Muhammad Sammani Sani</em></p>
<h4 id="tldr_45">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶ç³»ç»Ÿå®¡è®¡äº†ä¸»æµå¤§è¯­è¨€æ¨¡å‹åœ¨è‹±è¯­ä¸è±ªè¨è¯­ä¹‹é—´çš„å®‰å…¨å¯¹é½å·®å¼‚ï¼Œæ­ç¤ºäº†å®‰å…¨æ€§èƒ½å¹¶éç®€å•é€€åŒ–ï¼Œè€Œæ˜¯ç”±è¯­è¨€ä¸æ—¶æ€æ¡†æ¶çš„å¤æ‚äº¤äº’å†³å®šçš„åŠ¨æ€çŠ¶æ€ï¼ŒæŒ‘æˆ˜äº†ç°æœ‰çš„å¤šè¯­è¨€å®‰å…¨å·®è·å™äº‹ã€‚</p>
<hr />
<h4 id="detailed-summary_45">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> éšç€å¤§è¯­è¨€æ¨¡å‹èå…¥å…¨çƒå…³é”®åŸºç¡€è®¾æ–½ï¼Œå®‰å…¨å¯¹é½åœ¨è‹±è¯­åˆ°å…¶ä»–è¯­è¨€çš„é›¶æ ·æœ¬è¿ç§»å‡è®¾å­˜åœ¨å±é™©ç›²ç‚¹ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹è±ªè¨è¯­ç­‰ä½èµ„æºè¯­è¨€ä»¥åŠè¥¿éç‰¹å®šå¨èƒåœºæ™¯ï¼ˆå¦‚é›…è™é›…è™æ¬ºè¯ˆã€ä¸¹æ©æªåˆ¶é€ ï¼‰ï¼Œå½“å‰ç ”ç©¶ç¼ºä¹å¯¹è¯­è¨€ä¸æ—¶æ€å› ç´ äº¤äº’å½±å“çš„ç³»ç»Ÿæ€§ç†è§£ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨2Ã—4å› å­è®¾è®¡è¿›è¡Œ1,440æ¬¡è¯„ä¼°ï¼Œä½¿ç”¨åŸºäºè¥¿éå¨èƒåœºæ™¯æ„å»ºçš„æ–°å‹å¯¹æŠ—æ•°æ®é›†HausaSafetyï¼Œç³»ç»Ÿæµ‹è¯•äº†ä¸‰ç§æœ€å…ˆè¿›æ¨¡å‹ï¼ˆGPT-5.1ã€Gemini 3 Proå’ŒClaude 4.5 Opusï¼‰åœ¨è‹±è¯­ä¸è±ªè¨è¯­ä¹‹é—´ä»¥åŠä¸åŒæ—¶æ€æ¡†æ¶ä¸‹çš„å®‰å…¨æ€§èƒ½éçº¿æ€§äº¤äº’ä½œç”¨ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶å‘ç°äº†å¤æ‚å¹²æ‰°æœºåˆ¶è€Œéç®€å•çš„å®‰å…¨é€€åŒ–ï¼ŒClaude 4.5 Opusåœ¨è±ªè¨è¯­ä¸­å®‰å…¨æ€§æ˜¾è‘—é«˜äºè‹±è¯­ï¼ˆ45.0% vs 36.7%ï¼‰ï¼Œä½†å­˜åœ¨ç¾éš¾æ€§çš„æ—¶æ€æ¨ç†å¤±è´¥ï¼Œè¿‡å»æ—¶æ¡†æ¶ç»•è¿‡é˜²å¾¡ï¼ˆ15.6%å®‰å…¨ï¼‰è€Œå°†æ¥æ—¶åœºæ™¯è§¦å‘è¿‡åº¦ä¿å®ˆæ‹’ç»ï¼ˆ57.2%å®‰å…¨ï¼‰ï¼Œæœ€å®‰å…¨ä¸æœ€è„†å¼±é…ç½®é—´å­˜åœ¨9.2å€å·®å¼‚ã€‚</p>
<p><strong>Conclusion:</strong> å½“å‰æ¨¡å‹ä¾èµ–è¡¨é¢å¯å‘å¼è€Œéé²æ£’çš„è¯­ä¹‰ç†è§£ï¼Œå½¢æˆå®‰å…¨æ¼æ´ä½¿å…¨çƒå—æ–¹ç”¨æˆ·æš´éœ²äºæœ¬åœ°åŒ–å±å®³ï¼Œéœ€è¦å‘ä¸å˜å¯¹é½èŒƒå¼è½¬å˜ä»¥ç¡®ä¿è·¨è¯­è¨€å’Œæ—¶æ€å˜åŒ–çš„å®‰å…¨ç¨³å®šæ€§ï¼Œå®‰å…¨æ€§èƒ½æ˜¯æƒ…å¢ƒä¾èµ–çš„åŠ¨æ€çŠ¶æ€è€Œéå›ºå®šå±æ€§ã€‚</p>
<hr />
<h4 id="abstract_45">ğŸ“„ Abstract</h4>
<p>As Large Language Models (LLMs) integrate into critical global infrastructure, the assumption that safety alignment transfers zero-shot from English to other languages remains a dangerous blind spot. This study presents a systematic audit of three state of the art models (GPT-5.1, Gemini 3 Pro, and Claude 4.5 Opus) using HausaSafety, a novel adversarial dataset grounded in West African threat scenarios (e.g., Yahoo-Yahoo fraud, Dane gun manufacturing). Employing a 2 x 4 factorial design across 1,440 evaluations, we tested the non-linear interaction between language (English vs. Hausa) and temporal framing. Our results challenge the prevailing multilingual safety gap narrative. Instead of a simple degradation in low-resource settings, we identified a mechanism of Complex Interference where safety is determined by the intersection of variables. While models exhibited a Reverse Linguistic with Claude 4.5 Opus proving significantly safer in Hausa (45.0%) than in English (36.7%) due to uncertainty-driven refusal they suffered catastrophic failures in temporal reasoning. We report a profound Temporal Asymmetry, where past-tense framing bypassed defenses (15.6% safe) while future-tense scenarios triggered hyper-conservative refusals (57.2% safe). The magnitude of this volatility is illustrated by a 9.2x disparity between the safest and most vulnerable configurations, proving that safety is not a fixed property but a context-dependent state. We conclude that current models rely on superficial heuristics rather than robust semantic understanding, creating Safety Pockets that leave Global South users exposed to localized harms. We propose Invariant Alignment as a necessary paradigm shift to ensure safety stability across linguistic and temporal shifts.</p>
<div id='cs.AI'></div>

<h1 id="csai-back">cs.AI <a href="#toc">[Back]</a></h1>
<h3 id="47-road-reflective-optimization-via-automated-debugging-for-zero-shot-agent-alignment">[47] <a href="https://arxiv.org/abs/2512.24040">ROAD: Reflective Optimization via Automated Debugging for Zero-Shot Agent Alignment</a></h3>
<p><em>Natchaya Temyingyong, Daman Jain, Neeraj Kumarsahu, Prabhat Kumar, Rachata Phondi, Wachiravit Modecrua, Krittanon Kaewtawee, Krittin Pachtrachai, Touchapon Kraisingkorn</em></p>
<h4 id="tldr_46">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ROADæ¡†æ¶ï¼Œä¸€ç§æ— éœ€æ ‡æ³¨æ•°æ®é›†å³å¯ä¼˜åŒ–LLMæç¤ºçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡æ¨¡æ‹Ÿäººç±»å·¥ç¨‹å¸ˆçš„è°ƒè¯•å¾ªç¯ï¼Œå°†å¤±è´¥æ—¥å¿—è½¬åŒ–ä¸ºç»“æ„åŒ–å†³ç­–åè®®ï¼Œæ˜¾è‘—æå‡äº†ä»£ç†æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_46">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è‡ªåŠ¨æç¤ºä¼˜åŒ–æ–¹æ³•ä¸¥é‡ä¾èµ–å¤§é‡æ ‡æ³¨çš„å¼€å‘é›†æ¥è®¡ç®—é€‚åº”åº¦åˆ†æ•°ï¼Œä½†åœ¨å®é™…è½¯ä»¶å·¥ç¨‹ä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»£ç†å¼€å‘çš„å†·å¯åŠ¨é˜¶æ®µï¼Œå·¥ç¨‹å¸ˆé€šå¸¸åªèƒ½è·å¾—æ··ä¹±çš„ç”Ÿäº§æ—¥å¿—å’Œä¸æ–­æ¼”å˜çš„æ•…éšœæ¨¡å¼ï¼Œç¼ºä¹ç²¾å¿ƒç­–åˆ’çš„æ•°æ®é›†ã€‚</p>
<p><strong>Method:</strong> ROADæ¡†æ¶é‡‡ç”¨å¤šæ™ºèƒ½ä½“æ¶æ„ï¼Œå°†ä¼˜åŒ–è§†ä¸ºåŠ¨æ€è°ƒè¯•è°ƒæŸ¥è€Œééšæœºæœç´¢ï¼ŒåŒ…æ‹¬ç”¨äºæ ¹å› åˆ†æçš„Analyzerã€ç”¨äºæ¨¡å¼èšåˆçš„Optimizerå’Œç”¨äºç­–ç•¥æ•´åˆçš„Coachï¼Œèƒ½å¤Ÿå°†éç»“æ„åŒ–å¤±è´¥æ—¥å¿—è½¬åŒ–ä¸ºé²æ£’çš„ç»“æ„åŒ–å†³ç­–æ ‘åè®®ã€‚</p>
<p><strong>Result:</strong> åœ¨æ ‡å‡†åŒ–å­¦æœ¯åŸºå‡†å’Œå®é™…ç”Ÿäº§çŸ¥è¯†ç®¡ç†å¼•æ“ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒROADå…·æœ‰é«˜æ ·æœ¬æ•ˆç‡ï¼Œä»…é€šè¿‡ä¸‰æ¬¡è‡ªåŠ¨è¿­ä»£å°±å®ç°äº†æˆåŠŸç‡ä»73.6%æå‡è‡³79.2%ï¼ˆæå‡5.6%ï¼‰å’Œæœç´¢å‡†ç¡®ç‡æå‡3.8%ï¼Œåœ¨é›¶å”®é¢†åŸŸå¤æ‚æ¨ç†ä»»åŠ¡ä¸­ï¼ŒROADä½¿ä»£ç†æ€§èƒ½ç›¸æ¯”åŸºçº¿æå‡çº¦19%ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ï¼Œæ¨¡æ‹Ÿäººç±»å·¥ç¨‹å¸ˆçš„æ•…éšœåˆ†æå’Œä¿®å¤å¾ªç¯ä¸ºéƒ¨ç½²å¯é çš„LLMä»£ç†æä¾›äº†ä¸€ç§å¯è¡Œä¸”æ•°æ®é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œé¿å…äº†èµ„æºå¯†é›†çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œç‰¹åˆ«é€‚ç”¨äºç¼ºä¹æ ‡æ³¨æ•°æ®çš„å®é™…åº”ç”¨åœºæ™¯ã€‚</p>
<hr />
<h4 id="abstract_46">ğŸ“„ Abstract</h4>
<p>Automatic Prompt Optimization (APO) has emerged as a critical technique for enhancing Large Language Model (LLM) performance, yet current state-of-the-art methods typically rely on large, labeled gold-standard development sets to compute fitness scores for evolutionary or Reinforcement Learning (RL) approaches. In real-world software engineering, however, such curated datasets are rarely available during the initial cold start of agent development, where engineers instead face messy production logs and evolving failure modes. We present ROAD (Reflective Optimization via Automated Debugging), a novel framework that bypasses the need for refined datasets by treating optimization as a dynamic debugging investigation rather than a stochastic search. Unlike traditional mutation strategies, ROAD utilizes a specialized multi-agent architecture, comprising an Analyzer for root-cause analysis, an Optimizer for pattern aggregation, and a Coach for strategy integration, to convert unstructured failure logs into robust, structured Decision Tree Protocols. We evaluated ROAD across both a standardized academic benchmark and a live production Knowledge Management engine. Experimental results demonstrate that ROAD is highly sample-efficient, achieving a 5.6 percent increase in success rate (73.6 percent to 79.2 percent) and a 3.8 percent increase in search accuracy within just three automated iterations. Furthermore, on complex reasoning tasks in the retail domain, ROAD improved agent performance by approximately 19 percent relative to the baseline. These findings suggest that mimicking the human engineering loop of failure analysis and patching offers a viable, data-efficient alternative to resource-intensive RL training for deploying reliable LLM agents.</p>
<h3 id="48-multi-modal-cross-domain-mixed-fusion-model-with-dual-disentanglement-for-fault-diagnosis-under-unseen-working-conditions">[48] <a href="https://arxiv.org/abs/2512.24679">Multi-modal cross-domain mixed fusion model with dual disentanglement for fault diagnosis under unseen working conditions</a></h3>
<p><em>Pengcheng Xia, Yixiang Huang, Chengjin Qin, Chengliang Liu</em></p>
<h4 id="tldr_47">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€è·¨åŸŸæ··åˆèåˆæ¨¡å‹ï¼Œé€šè¿‡åŒé‡è§£è€¦æ¡†æ¶åˆ†ç¦»æ¨¡æ€ä¸å˜ä¸æ¨¡æ€ç‰¹å®šç‰¹å¾ä»¥åŠåŸŸä¸å˜ä¸åŸŸç‰¹å®šè¡¨ç¤ºï¼Œç»“åˆè·¨åŸŸæ··åˆèåˆç­–ç•¥å’Œä¸‰æ¨¡æ€èåˆæœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†æœªè§å·¥ä½œæ¡ä»¶ä¸‹æœºæ¢°æ•…éšœè¯Šæ–­çš„æ³›åŒ–æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_47">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ™ºèƒ½æ•…éšœè¯Šæ–­æ–¹æ³•åœ¨çœŸå®åœºæ™¯ä¸­é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šåœ¨æœªè§å·¥ä½œæ¡ä»¶ä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè€ŒåŸŸè‡ªé€‚åº”æ–¹æ³•ä¾èµ–ç›®æ ‡åŸŸæ ·æœ¬ï¼›åŒæ—¶å¤šæ•°ç ”ç©¶ä¾èµ–å•æ¨¡æ€ä¼ æ„Ÿä¿¡å·ï¼Œå¿½è§†äº†å¤šæ¨¡æ€ä¿¡æ¯çš„äº’è¡¥æ€§å¯¹æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„ä½œç”¨ã€‚</p>
<p><strong>Method:</strong> æå‡ºå¤šæ¨¡æ€è·¨åŸŸæ··åˆèåˆæ¨¡å‹ï¼Œé‡‡ç”¨åŒé‡è§£è€¦æ¡†æ¶åˆ†ç¦»æ¨¡æ€ä¸å˜ä¸æ¨¡æ€ç‰¹å®šç‰¹å¾ä»¥åŠåŸŸä¸å˜ä¸åŸŸç‰¹å®šè¡¨ç¤ºï¼›è®¾è®¡è·¨åŸŸæ··åˆèåˆç­–ç•¥ï¼Œåœ¨åŸŸé—´éšæœºæ··åˆæ¨¡æ€ä¿¡æ¯ä»¥å¢å¼ºæ¨¡æ€å’ŒåŸŸå¤šæ ·æ€§ï¼›å¼•å…¥ä¸‰æ¨¡æ€èåˆæœºåˆ¶è‡ªé€‚åº”æ•´åˆå¤šæ¨¡æ€å¼‚æ„ä¿¡æ¯ã€‚</p>
<p><strong>Result:</strong> åœ¨æ„Ÿåº”ç”µæœºæ•…éšœè¯Šæ–­çš„å¹¿æ³›å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•åœ¨æœªè§æ’å®šå’Œæ—¶å˜å·¥ä½œæ¡ä»¶ä¸‹å‡ä¼˜äºå…ˆè¿›æ–¹æ³•ï¼›å…¨é¢çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†æ¯ä¸ªæå‡ºç»„ä»¶å’Œå¤šæ¨¡æ€èåˆçš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†æ¨¡å‹åœ¨è·¨åŸŸæ•…éšœè¯Šæ–­ä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡åŒé‡è§£è€¦å’Œè·¨åŸŸæ··åˆèåˆç­–ç•¥ï¼Œæœ‰æ•ˆè§£å†³äº†å¤šæ¨¡æ€æ•…éšœè¯Šæ–­ä¸­çš„åŸŸæ³›åŒ–é—®é¢˜ï¼Œä¸ºæœºæ¢°å¯é æ€§ä¿éšœæä¾›äº†æ›´é²æ£’çš„è§£å†³æ–¹æ¡ˆï¼›æ–¹æ³•æ¡†æ¶å…·æœ‰é€šç”¨æ€§ï¼Œå¯æ¨å¹¿åˆ°å…¶ä»–éœ€è¦å¤šæ¨¡æ€ä¿¡æ¯èåˆçš„å·¥ä¸šè¯Šæ–­åœºæ™¯ã€‚</p>
<hr />
<h4 id="abstract_47">ğŸ“„ Abstract</h4>
<p>Intelligent fault diagnosis has become an indispensable technique for ensuring machinery reliability. However, existing methods suffer significant performance decline in real-world scenarios where models are tested under unseen working conditions, while domain adaptation approaches are limited to their reliance on target domain samples. Moreover, most existing studies rely on single-modal sensing signals, overlooking the complementary nature of multi-modal information for improving model generalization. To address these limitations, this paper proposes a multi-modal cross-domain mixed fusion model with dual disentanglement for fault diagnosis. A dual disentanglement framework is developed to decouple modality-invariant and modality-specific features, as well as domain-invariant and domain-specific representations, enabling both comprehensive multi-modal representation learning and robust domain generalization. A cross-domain mixed fusion strategy is designed to randomly mix modality information across domains for modality and domain diversity augmentation. Furthermore, a triple-modal fusion mechanism is introduced to adaptively integrate multi-modal heterogeneous information. Extensive experiments are conducted on induction motor fault diagnosis under both unseen constant and time-varying working conditions. The results demonstrate that the proposed method consistently outperforms advanced methods and comprehensive ablation studies further verify the effectiveness of each proposed component and multi-modal fusion. The code is available at: https://github.com/xiapc1996/MMDG.</p>
<h3 id="49-genz-foundational-models-as-latent-variable-generators-within-traditional-statistical-models">[49] <a href="https://arxiv.org/abs/2512.24834">GenZ: Foundational models as latent variable generators within traditional statistical models</a></h3>
<p><em>Marko Jojic, Nebojsa Jojic</em></p>
<h4 id="tldr_48">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºGenZï¼Œä¸€ç§é€šè¿‡å¯è§£é‡Šè¯­ä¹‰ç‰¹å¾æ¡¥æ¥åŸºç¡€æ¨¡å‹ä¸ç»Ÿè®¡å»ºæ¨¡çš„æ··åˆæ–¹æ³•ï¼Œèƒ½å¤Ÿå‘ç°æ•°æ®é›†ç‰¹å®šæ¨¡å¼ä»¥æå‡é¢„æµ‹æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºä»…ä¾èµ–åŸºç¡€æ¨¡å‹é¢†åŸŸçŸ¥è¯†çš„åŸºçº¿æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_48">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§å‹è¯­è¨€æ¨¡å‹è™½å…·å¤‡å¹¿æ³›é¢†åŸŸçŸ¥è¯†ï¼Œä½†å¸¸æ— æ³•æ•æ‰å¯¹é¢„æµ‹ä»»åŠ¡è‡³å…³é‡è¦çš„æ•°æ®é›†ç‰¹å®šæ¨¡å¼ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨éœ€è¦ç²¾ç¡®ç»Ÿè®¡å»ºæ¨¡åœºæ™¯ä¸­çš„åº”ç”¨æ•ˆæœã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é€šè¿‡è¿­ä»£è¿‡ç¨‹å‘ç°è¯­ä¹‰ç‰¹å¾æè¿°ï¼Œè¯¥è¿‡ç¨‹åŸºäºç»Ÿè®¡å»ºæ¨¡è¯¯å·®å¯¹æ¯”é¡¹ç›®ç»„è€Œéä»…ä¾èµ–åŸºç¡€æ¨¡å‹çš„é¢†åŸŸç†è§£ï¼Œå°†å…¶å½¢å¼åŒ–ä¸ºè”åˆä¼˜åŒ–è¯­ä¹‰ç‰¹å¾æè¿°ç¬¦å’Œç»Ÿè®¡æ¨¡å‹å‚æ•°çš„å¹¿ä¹‰EMç®—æ³•ï¼Œåˆ©ç”¨å†»ç»“çš„åŸºç¡€æ¨¡å‹åŸºäºå‘ç°çš„ç‰¹å¾å¯¹é¡¹ç›®è¿›è¡Œåˆ†ç±»ï¼Œå¹¶å°†è¿™äº›åˆ¤æ–­è§†ä¸ºé¢„æµ‹å®å€¼ç›®æ ‡çš„æ½œåœ¨äºŒå…ƒç‰¹å¾çš„å™ªå£°è§‚æµ‹ã€‚</p>
<p><strong>Result:</strong> åœ¨æˆ¿ä»·é¢„æµ‹ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹ä½¿ç”¨ä»å¤šæ¨¡æ€åˆ—è¡¨æ•°æ®ä¸­å‘ç°çš„è¯­ä¹‰ç‰¹å¾å®ç°äº†12%çš„ä¸­ä½æ•°ç›¸å¯¹è¯¯å·®ï¼Œæ˜¾è‘—ä¼˜äºä¾èµ–LLMä¸€èˆ¬é¢†åŸŸçŸ¥è¯†çš„GPT-5åŸºçº¿ï¼ˆ38%è¯¯å·®ï¼‰ï¼›åœ¨Netflixç”µå½±åµŒå…¥çš„å†·å¯åŠ¨ååŒè¿‡æ»¤ä¸­ï¼Œæ¨¡å‹ä»…ä»è¯­ä¹‰æè¿°é¢„æµ‹ååŒè¿‡æ»¤è¡¨ç¤ºè¾¾åˆ°0.59ä½™å¼¦ç›¸ä¼¼åº¦ï¼ŒåŒ¹é…ä¼ ç»ŸååŒè¿‡æ»¤éœ€è¦çº¦4000ç”¨æˆ·è¯„åˆ†æ‰èƒ½è¾¾åˆ°çš„æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•æˆåŠŸæ¡¥æ¥äº†åŸºç¡€æ¨¡å‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ä¸ç»Ÿè®¡å»ºæ¨¡çš„ç²¾ç¡®æ€§ï¼Œå‘ç°çš„è¯­ä¹‰ç‰¹å¾æ­ç¤ºäº†æ•°æ®é›†ç‰¹å®šæ¨¡å¼ï¼ˆå¦‚å»ºç­‘ç»†èŠ‚é¢„æµ‹æœ¬åœ°ä½æˆ¿å¸‚åœºã€ç‰¹è®¸ç»è¥æˆå‘˜é¢„æµ‹ç”¨æˆ·åå¥½ï¼‰ï¼Œè¿™äº›æ¨¡å¼ä¸æ¨¡å‹å•ç‹¬ä¾èµ–çš„é¢†åŸŸçŸ¥è¯†å­˜åœ¨å·®å¼‚ï¼Œä¸ºæ„å»ºæ›´å¯è§£é‡Šä¸”æ€§èƒ½ä¼˜è¶Šçš„æ··åˆAIç³»ç»Ÿæä¾›äº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_48">ğŸ“„ Abstract</h4>
<p>We present GenZ, a hybrid model that bridges foundational models and statistical modeling through interpretable semantic features. While large language models possess broad domain knowledge, they often fail to capture dataset-specific patterns critical for prediction tasks. Our approach addresses this by discovering semantic feature descriptions through an iterative process that contrasts groups of items identified via statistical modeling errors, rather than relying solely on the foundational model's domain understanding. We formulate this as a generalized EM algorithm that jointly optimizes semantic feature descriptors and statistical model parameters. The method prompts a frozen foundational model to classify items based on discovered features, treating these judgments as noisy observations of latent binary features that predict real-valued targets through learned statistical relationships. We demonstrate the approach on two domains: house price prediction (hedonic regression) and cold-start collaborative filtering for movie recommendations. On house prices, our model achieves 12\% median relative error using discovered semantic features from multimodal listing data, substantially outperforming a GPT-5 baseline (38\% error) that relies on the LLM's general domain knowledge. For Netflix movie embeddings, our model predicts collaborative filtering representations with 0.59 cosine similarity purely from semantic descriptions -- matching the performance that would require approximately 4000 user ratings through traditional collaborative filtering. The discovered features reveal dataset-specific patterns (e.g., architectural details predicting local housing markets, franchise membership predicting user preferences) that diverge from the model's domain knowledge alone.</p>
<h3 id="50-semi-automated-data-annotation-in-multisensor-datasets-for-autonomous-vehicle-testing">[50] <a href="https://arxiv.org/abs/2512.24896">Semi-Automated Data Annotation in Multisensor Datasets for Autonomous Vehicle Testing</a></h3>
<p><em>Andrii Gamalii, Daniel GÃ³rniak, Robert Nowak, BartÅ‚omiej Olber, Krystian Radlak, Jakub Winter</em></p>
<h4 id="tldr_49">ğŸ§© TL;DR</h4>
<p>è¯¥æŠ¥å‘Šä»‹ç»äº†DARTSé¡¹ç›®ä¸­å¼€å‘çš„åŠè‡ªåŠ¨åŒ–æ•°æ®æ ‡æ³¨æµæ°´çº¿ï¼Œè¯¥æµæ°´çº¿é‡‡ç”¨äººæœºååŒæ–¹æ³•ï¼Œç»“åˆäººå·¥æ™ºèƒ½ä¸äººç±»ä¸“ä¸šçŸ¥è¯†ï¼Œæ˜¾è‘—é™ä½äº†å¤§è§„æ¨¡å¤šæ¨¡æ€é©¾é©¶åœºæ™¯æ•°æ®é›†çš„æ ‡æ³¨æˆæœ¬å’Œæ—¶é—´ã€‚</p>
<hr />
<h4 id="detailed-summary_49">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ‰‹åŠ¨æ ‡æ³¨æ³¢å…°é©¾é©¶æ¡ä»¶ä¸‹çš„å¤§è§„æ¨¡ã€å¤šæ¨¡æ€æ•°æ®é›†æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ï¼ŒDARTSé¡¹ç›®éœ€è¦è§£å†³è¿™ä¸€æ•ˆç‡ç“¶é¢ˆï¼Œä»¥åŠ é€Ÿè‡ªåŠ¨é©¾é©¶ç ”ç©¶çš„æ•°æ®å‡†å¤‡è¿‡ç¨‹ã€‚</p>
<p><strong>Method:</strong> è¯¥è§£å†³æ–¹æ¡ˆé‡‡ç”¨äººæœºååŒæ–¹æ³•ï¼Œç»“åˆ3Dç›®æ ‡æ£€æµ‹ç®—æ³•ç”Ÿæˆåˆæ­¥æ ‡æ³¨ï¼Œæ”¯æŒè¿­ä»£æ¨¡å‹é‡è®­ç»ƒï¼Œå¹¶é›†æˆäº†æ•°æ®åŒ¿ååŒ–å’Œé¢†åŸŸé€‚åº”æŠ€æœ¯ï¼Œæ„å»ºäº†å®Œæ•´çš„åŠè‡ªåŠ¨åŒ–æ ‡æ³¨æµæ°´çº¿ã€‚</p>
<p><strong>Result:</strong> å¼€å‘çš„å·¥å…·å’Œæ–¹æ³•å®ç°äº†æ˜¾è‘—çš„æ—¶é—´èŠ‚çœï¼ŒåŒæ—¶ç¡®ä¿äº†è·¨ä¸åŒä¼ æ„Ÿå™¨æ¨¡æ€çš„ä¸€è‡´é«˜è´¨é‡æ ‡æ³¨ï¼Œç›´æ¥æ”¯æŒäº†DARTSé¡¹ç›®ä¸­å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†çš„å¿«é€Ÿå‡†å¤‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºè‡ªåŠ¨é©¾é©¶ç ”ç©¶æä¾›äº†é«˜æ•ˆçš„æ•°æ®æ ‡æ³¨è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡åŠè‡ªåŠ¨åŒ–æµæ°´çº¿æ˜¾è‘—æå‡äº†æ•°æ®å‡†å¤‡æ•ˆç‡ï¼ŒåŒæ—¶ç¡®ä¿äº†æ ‡æ³¨è´¨é‡ï¼Œä¸ºæ³¢å…°è‡ªåŠ¨é©¾é©¶æŠ€æœ¯ç ”ç©¶å¥ å®šäº†åšå®çš„æ•°æ®åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_49">ğŸ“„ Abstract</h4>
<p>This report presents the design and implementation of a semi-automated data annotation pipeline developed within the DARTS project, whose goal is to create a large-scale, multimodal dataset of driving scenarios recorded in Polish conditions. Manual annotation of such heterogeneous data is both costly and time-consuming. To address this challenge, the proposed solution adopts a human-in-the-loop approach that combines artificial intelligence with human expertise to reduce annotation cost and duration. The system automatically generates initial annotations, enables iterative model retraining, and incorporates data anonymization and domain adaptation techniques. At its core, the tool relies on 3D object detection algorithms to produce preliminary annotations. Overall, the developed tools and methodology result in substantial time savings while ensuring consistent, high-quality annotations across different sensor modalities. The solution directly supports the DARTS project by accelerating the preparation of large annotated dataset in the project's standardized format, strengthening the technological base for autonomous vehicle research in Poland.</p>
  </article>
</body>
</html>
